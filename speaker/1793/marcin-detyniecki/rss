<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>pyvideo.org: Videos of Marcin Detyniecki</title><link>http://www.pyvideo.org/speaker/1793/marcin-detyniecki/rss</link><description></description><atom:link href="http://www.pyvideo.org/speaker/1793/marcin-detyniecki/rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Tue, 14 Apr 2015 00:00:00 -0500</lastBuildDate><ttl>500</ttl><item><title>Why and how to explain machine learning predictions</title><link>http://www.pyvideo.org/video/3524/why-and-how-to-explain-machine-learning-predictio</link><description>&lt;p&gt;Description&lt;/p&gt;
Unfortunately, the predictive models that are most powerful are usually the least interpretable. However in some cases, for example fraud detection, end users need an understandable explanation of a particular prediction, at observation level, and not only at population level (e.g. : features importance). During this talk we will present different approaches to tackle this issue, both for random forests and gradient boosting trees. We will also demonstrate an implementation based on scikit-learn.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bora Eang,Christophe Bourguignat,Marcin Detyniecki</dc:creator><pubDate>Tue, 14 Apr 2015 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/3524/why-and-how-to-explain-machine-learning-predictio</guid><enclosure url="https://www.youtube.com/watch?v=TB3axpIpCiQ" length="None" type="video/flv"></enclosure><media:thumbnail url="https://i.ytimg.com/vi/TB3axpIpCiQ/hqdefault.jpg"></media:thumbnail></item></channel></rss>