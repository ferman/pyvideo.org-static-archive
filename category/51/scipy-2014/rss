<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>pyvideo.org: Videos of SciPy 2014</title><link>http://www.pyvideo.org/category/51/scipy-2014/rss</link><description></description><atom:link href="http://www.pyvideo.org/category/51/scipy-2014/rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Mon, 14 Jul 2014 00:00:00 -0500</lastBuildDate><ttl>500</ttl><item><title>Time Series Analysis for Network Security</title><link>http://www.pyvideo.org/video/2864/time-series-analysis-network-security</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Endgame seeks to develop products that allow customers to gain visibility into their networks and discover anomalies. I will describe how Endgame brings together various Python packages (scipy, pandas, statsmodels, kairos, etc...) in order to collect, record, and then analyze time series that are collected from network security data feeds.
&lt;p&gt;Description&lt;/p&gt;
In this talk, I will describe how Endgame has brought together many different Python tools in order to solve the problem of detecting outliers in network security data.

The first step in this process is collecting and storing the metrics that will form a time series. Here, I will describe how Endgame plugs into the flow of network data and then stores that data. (Python packages: elasticsearch, pyspark, kairos)

The next step is applying a Fourier transform in order to classify time series that exhibit daily and weekly patterns. This information is especially useful in deciding how to characterize a time series's past behavior and thus judge how unusual new data is. (Python package: numpy)

Finally, exponentially weighted moving averages and standard deviations are calculated in different ways depending on how the time series was classified. For example, if strong daily patterns are present, the data is stacked by daily time bin and moving averages are calculated within each time bin. Corrections for weekend and weekday behavior are also applied if necessary. Autoregressive moving average models are also used and the performance of each algorithm is gauged and compared (Python packages: pandas, scikits.statsmodels).

The final result of this process is a list of outliers and their severity. Further algorithms will judge what outliers are serious enough to present to users.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Phil Roth</dc:creator><guid>http://www.pyvideo.org/video/2864/time-series-analysis-network-security</guid><enclosure url="http://www.youtube.com/watch?v=ZSM-tmbBZ5E" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i.ytimg.com/vi/ZSM-tmbBZ5E/hqdefault.jpg"></media:thumbnail></item><item><title>Astropy in 2014: What's new, where we are headed</title><link>http://www.pyvideo.org/video/2778/astropy-in-2014-whats-new-where-we-are-headed</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We report on the progress made on the Astropy Project in the past year highlighting the new capabilities added as well as the near-term development plans.
&lt;p&gt;Description&lt;/p&gt;
Astropy continues to see significant growth in available software, developers, and users. A new major release (V0.3) was made in the past year as well as many minor releases. V0.4 is scheduled for release by the time of conference and will include support for the VO SAMP protocol. We will report on the progress made in building and enhancing the core libraries in the Astropy Project, including a new model and fitting framework, enhanced units, quantities, and table functionality, a VO cone search tool, and a new convolution subpackage. We'll review the current set of tools available, highlighting in particular the new capabilities present. We will also give an overview of current activities and development plans for the core and affiliated packages, as well as adding new resources/tutorials for learning how to use astropy.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Perry Greenfield</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2778/astropy-in-2014-whats-new-where-we-are-headed</guid><enclosure url="http://www.youtube.com/watch?v=R12BY23xczI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/R12BY23xczI/hqdefault.jpg"></media:thumbnail></item><item><title>Cartopy</title><link>http://www.pyvideo.org/video/2782/cartopy</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Cartopy is a Python package which builds on Proj.4 to define coordinate reference systems for the transformation and visualisation of geospatial data. It has a simple matplotlib interface for publication quality visualisation. This talk will outline some of cartopy's functionality and demonstrate some practical applications within the realm of scientific presentation of geospatial data.
&lt;p&gt;Description&lt;/p&gt;
The practice of representing geospatial data upon a flat surface is known as
cartography, and the topological implications of projecting fundamentally 3D
data onto a 2 dimensional surface has been the challenge of map-makers since
time immemorial. Geospatial visualisation software is often implemented without
consideration for the 3rd dimension and this commonly results in problems
around the dateline or at the poles. For small areas these problems are often
not apparent and mostly surmountable, but at a global scale, such as when
visualising output from GCMs (General circulation models), the underlying
representation must be addressed head-on in order to visualise the data
"impact free".

Cartopy is a Python package which builds on top of
Proj.4 to define coordinate reference systems for the transformation and
visualisation of geospatial data. As well as the fundamental transformations
there is also a matplotlib interface allowing easy generation of maps with the
same publication quiality expected of matplotlib. Cartopy employs several
techniques to handle geospatial data correctly, including true spherical
interpolation for raster data, and Shapely geometry interpolate-and-cut
transformations for geospatial vector data.

This talk will outline some
of the capabilities of cartopy, and continue onto its practical application
within the realm of scientific presentation of geospatial data.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Richard Hattersley</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2782/cartopy</guid><enclosure url="http://www.youtube.com/watch?v=Ax75kA4_kRo" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Ax75kA4_kRo/hqdefault.jpg"></media:thumbnail></item><item><title>Deploying Python Tools to GIS Users</title><link>http://www.pyvideo.org/video/2775/deploying-python-tools-to-gis-users</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The geospatial community has coalesced around Python, both in the commercial and open source spaces. In this talk, I'll show how Python tools can be shared with users of ArcGIS, a commercial GIS system which uses Python as its primary development environment. By constructing small Python wrappers, code can be shared in graphical tools which enable non-programmers to use what you've built.
&lt;p&gt;Description&lt;/p&gt;
Geospatial data is frequently manipulated directly using Python tools, commonly
built on top of powerful libraries such as GDAL, GEOS and NetCDF. Delivering
model results to end users in many instances requires providing tools in
familiar graphical environments, such as desktop GIS systems, which can permit
users without programming knowledge to integrate models and results into their
existing scientific workflows. This talk discusses how to construct simple
wrappers around existing Python programs to enable their use by ArcGIS, a
commonly used commercial GIS.

Two separate approaches will be
illustrated: creating Python toolboxes, or collections of tools embeddable in
workflows, and creating customized Python graphical add-ins, which can control
the graphical environment provided within ArcGIS. Building contextual help,
interactive widgets, and leveraging `numpy` for direct data integration will be
discussed. While ArcGIS exposes much of its functionality via the `ArcPy`
package, this talk instead focuses on integrating code from other environments,
and doesn't presume existing ArcGIS expertise.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Shaun Walbridge</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2775/deploying-python-tools-to-gis-users</guid><enclosure url="http://www.youtube.com/watch?v=_rMNUAtLnPw" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/_rMNUAtLnPw/hqdefault.jpg"></media:thumbnail></item><item><title>Enhancements to Ginga: an Astronomical Image Viewer and Toolkit</title><link>http://www.pyvideo.org/video/2777/enhancements-to-ginga-an-astronomical-image-view</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We describe recent developments in the Ginga package, a open-source astronomical image viewer and toolkit written in python and hosted on Github.  The package was introduced to the scientific python community at SciPy 2013 and has received a number of enhancements since then based on user feedback.  The talk includes an image mosaicing demo of a wide-field camera exposure with 116 4Kx2K CCDs.
&lt;p&gt;Description&lt;/p&gt;
Ginga is an open-source astronomical image viewer and toolkit written in python and [hosted on Github](https://github.com/ejeschke/ginga).  It uses and inter-operates with several key scientific python packages: numpy, scipy, astropy and matplotlib.

In this talk/poster we describe and illustrate recent enhancements to the package since the introductory talk at SciPy 2013, including:


* modular/pluggable interfaces for world coordinate systems, image file I/O and star and image catalogs
* support for rendering into matplotlib figures
* support for image mosaicing
* support for image overlays
* customizable user-interface bindings
* improved documentation
* self contained Mac OS X packages

During the talk we will demonstrate the mosaicing plugin that is being used with several instruments at Subaru Telescope in Hawaii, including the new Hyper Suprime-Cam wide-field camera with 116 separate 4Kx2K CCDs.

The talk/poster may be of interest to anyone developing code in python needing to display scientific image (CCD or CMOS) data and astronomers interested in python-based quick look and analysis tools.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eric Jeschke</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2777/enhancements-to-ginga-an-astronomical-image-view</guid><enclosure url="http://www.youtube.com/watch?v=wa_0SuVckGQ" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/wa_0SuVckGQ/hqdefault.jpg"></media:thumbnail></item><item><title>Epipy: Visualization of Emerging Zoonoses Through Temporal Networks</title><link>http://www.pyvideo.org/video/2767/epipy-visualization-of-emerging-zoonoses-through</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We introduce two new plots for visualizing infectious disease outbreaks. Case tree plots depict the emergence and growth of clusters of zoonotic disease over time. Checkerboard plots also represent temporal case clusters, but do not construct transmission trees. These plots visualize outbreak dynamics  and allow for analyses like case fatality risk stratified by generation.
&lt;p&gt;Description&lt;/p&gt;
We present two new visualizations, [case tree plots](https://github.com/cmrivers/epipy/blob/master/figs/example_casetree.png)
and [checkerboard](https://github.com/cmrivers/epipy/blob/master/figs/test_checkerboard.png)
plots, for visualizing emerging zoonoses.

Zoonoses represent an estimated 58% of all human infectious diseases, and 73%
of emerging infectious diseases. Recent examples of zoonotic outbreaks include
H1N1, SARS and Middle East Respiratory Syndrome, which have caused thousands of
deaths combined. The current toolkit for visualizing data from these emerging
diseases is limited.

Case tree and checkerboard plots were developed to address that gap.
The visualizations are best suited for diseases like SARS for which there are a
limited number of cases, with data available on human to human transmission.
They a) allow for easy estimation of epidemiological parameters like basic
reproduction number b) indicate the frequency of introductory events, e.g.
spillovers in the case of zoonoses c) represent patterns of case attributes
like patient sex both by generation and over time.

Case tree plots depict the emergence and growth of clusters of disease over
time. Each case is represented by a colored node. Nodes that share an
epidemiological link are connected by an edge. The color of the node varies
based on the node attribute; it could represent patient sex, health status
(e.g. alive, dead), or any other categorical attribute. Node placement along
the x-axis corresponds with the date of illness onset for the case.

A second visualization, the checkerboard plot, was developed to complement case
tree plots. They can be used in conjunction with case tree plots, or in
situations where representing a hypothetical network structure is
inappropriate.

The plots are available in the open source package epipy, which is available on
[github](https://github.com/cmrivers/epipy). Detailed documentation and
examples are also [available](cmrivers.github.io/epipy). In addition to these
visualizations, epipy includes functions for common epidemiology calculations
like odds ratio and relative risk.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Caitlin Rivers</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2767/epipy-visualization-of-emerging-zoonoses-through</guid><enclosure url="http://www.youtube.com/watch?v=JDtIDCVrxgk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/JDtIDCVrxgk/hqdefault.jpg"></media:thumbnail></item><item><title>HoloPy: Holograpy and Light Scattering in Python</title><link>http://www.pyvideo.org/video/2765/holopy-holograpy-and-light-scattering-in-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Digital holography microscopy is a powerful tool for fast 3D imaging of soft matter systems. However, making measurements from holograms requires special computation. HoloPy is a set of tools for reconstructing and fitting to holograms. It also includes tools for computing light scattering, setting up inverse problems, and working with images and metadata.
&lt;p&gt;Description&lt;/p&gt;
Digital holographic microscopy is fast and powerful tool for 3D imaging.
Holography captures information about a 3D scene onto a 2D camera using
interference. This means that the speed of holographic imaging is limited only
by camera speed, making holography an ideal tool for studying fast processes in
soft matter systems. However, making use of this encoded information requires
significant computational post processing. We have developed and released
[HoloPy](http://manoharan.seas.harvard.edu/holopy/), a python based tool for
doing these calculations. 

The traditional method for extracting
information from holograms is to optically reconstruct by shining light through
a hologram to obtain an image of the recorded scene. HoloPy implements the
digital equivalent of this, numerical reconstruction, in the form of light
propagation by convolution. This is a fast technique based on fast Fourier
transforms, which effectively allows refocusing a holographic image after it is
taken. 

For systems where a detailed scattering model is available, Lee
and coworkers showed that it is possible to make more precise measurements by
fitting a scattering model to a recorded hologram
[[1](http://physics.nyu.edu/grierlab/index12c/)]. We have extended this
technique to clusters of spheres
[[2](http://arxiv.org/pdf/1202.1600)][[3](http://people.seas.harvard.edu/~vnm/pdf/Perry-Faraday_Discussions-2012.pdf)]
and to non-spherical particles [[4](http://arxiv.org/pdf/1310.4517)]. HoloPy
implements all of these fitting techniques such that they can be used with a
few lines of python code. HoloPy also exposes an interface to all of its
scattering models compute light scattering of microscopic particles or clusters
of particles for other purposes. 

HoloPy is open source (GPLv3) and is
hosted on [launchpad](https://launchpad.net/holopy). HoloPy uses Numpy for most
of its manipulations, though it calls out to Fortran and
[C](http://code.google.com/p/a-dda) codes to compute light scattering. HoloPy
also includes matplotlib and mayavi based tools for visualizing holograms and
particles. 

[1] Lee et.al., Optics Express, Vol. 15, Issue 26, pp.
18275-18282 (2007)

[2] Fung et. al., JQSRT, Vol 113, Issue 18, pp.
2482-2489 (2012)

[3] Perry et. al., Faraday Discussions, Vol 159, pp.
211-234 (2012)

[4] Wang et. al. JQSRT, (2014)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tom Dimiduk</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2765/holopy-holograpy-and-light-scattering-in-python</guid><enclosure url="http://www.youtube.com/watch?v=uW6bMEcFX4A" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/uW6bMEcFX4A/hqdefault.jpg"></media:thumbnail></item><item><title>How Interactive Visualization Led to Insights in Digital Holographic Microscopy</title><link>http://www.pyvideo.org/video/2766/how-interactive-visualization-led-to-insights-in</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Digital holographic microscopy is a fast 3D imaging technique.  A camera records a time series of light scattering patterns as standard 2D images and then post-processing routines extract 3D information.  By creating a GPU-accelerated GUI on top of the Holopy package, we noticed unexpected discrepancies between the different models used during post-processing.
&lt;p&gt;Description&lt;/p&gt;
Digital holographic microscopy is a fast 3D imaging technique ideally suited to
studies of micron-sized objects that diffuse through random walks via Brownian
motion [[1]](http://dx.doi.org/10.1364/OE.15.018275).  Microspheres fit this
category and are widely used in biological assays and as ideal test subjects
for experiments in statistical mechanics.  Microspheres suspended in water move
too quickly to monitor with confocal microscopy.  With digital holographic
microscopy, 2D images encoding 3D volumes can be recorded at thousands of
frames per second
[[2]](http://www.nature.com/nmat/journal/v11/n2/abs/nmat3190.html).  The
computationally challenging part of digital holographic microscopy is
extracting the 3D information during post-processing.

The open source
[Holopy](https://launchpad.net/holopy) package which relies heavily on SciPy
and NumPy is used to recover the 3D information via one of two techniques:
reconstruction by numerical back-propagation of electromagnetic fields or
modeling forward light scattering with Mie theory.  The parameter space
describing the imaged volume is multidimensional.  Even for simple micron-sized
spheres, a hologram depends on each sphere's radius and index of refraction in
addition to its 3D position.  By supplementing Holopy with a [GPU-accelerated
GUI](https://github.com/RebeccaWPerry/holography-gpu) using PyQt4, we enabled
users to interactively adjust the system parameters and see a modeled digital
hologram change in response.

Simply adding the capability of
interactively manipulating holograms in a GUI led us to notice unexpected
discrepancies between the two modeling techniques and failures of both,
suggesting further experiments.  We observed that the numerical light
propagation technique only accurately characterizes the light within a cone
stretching from the extent of the image back towards the object.  Neither model
accurately characterizes the light upstream of the object toward the light
source.  The GUI was a natural format to interact with the theory and gain
insight because it showed us the models in an analogous format to how we see
the data on the microscope.  Other scientific projects may benefit from tools
that allow experimentalists to interact with theory in the same way they
interact with their experiments.

[1] Lee et.al., Optics Express, Vol.
15, Issue 26, pp. 18275-18282 (2007) doi: 10.1364/OE.15.018275.

[2] Kaz
et.al., Nature Materials, Vol. 11, pp. 138\u2013142 (2012)
doi:10.1038/nmat3190.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rebecca Perry</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2766/how-interactive-visualization-led-to-insights-in</guid><enclosure url="http://www.youtube.com/watch?v=-IjIpX8QsAM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/-IjIpX8QsAM/hqdefault.jpg"></media:thumbnail></item><item><title>How to Choose a Good Colour Map</title><link>http://www.pyvideo.org/video/2768/how-to-choose-a-good-colour-map</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Representing data through colours is a very common approach to conveying important information to an audience. We suggest some best practices scientists should consider when deciding how they should present their results.
&lt;p&gt;Description&lt;/p&gt;
Representing data through colours is a very common approach to conveying important information to an audience.  This is done throughout all fields in the scientific community and stakes a claim in the commercial and marketing realm as well.  Colour maps and contour maps are the preferred way for scientists to visualise three-dimensional data in two dimensions.  Research has shown that the choice of colourmap is crucial since the human brain interpolates hue poorly.  We suggest some best practices scientists should consider when deciding how they should present their results.  Specifically, we look at some examples of colourmaps that can easily be misinterpreted, making reference to an in-depth supportive study, and suggest alternative approaches to improve them.  We conclude by listing some open source tools that aid making good colourmap choices.  Kristen Thyng's talk on perception of colourmaps in matplotlib is an excellent follow-on from this.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damon McDougall</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2768/how-to-choose-a-good-colour-map</guid><enclosure url="http://www.youtube.com/watch?v=Alnc9E1RnD8" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Alnc9E1RnD8/hqdefault.jpg"></media:thumbnail></item><item><title>Light-weight real-time event detection with Python</title><link>http://www.pyvideo.org/video/2781/light-weight-real-time-event-detection-with-pytho</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Real-time feeds of user activity from various apps such as Twitter, Foursquare, and others are becoming increasingly available. These 'digital footprints' provide new means to understand how individuals utilize the places and spaces of urban environments. We present a light-weight framework for real-time event detection in a city based on existing SciPy libraries and real-time Twitter streams.
&lt;p&gt;Description&lt;/p&gt;
In this paper, we utilize real-time 'social information sources' to automatically detect important events at the urban scale. The goal is to provide city planners and others with information on *what* is going on, and *when* and *where* it is happening. Traditionally, this type of analysis would require a large investment in heavy-duty computing infrastructure, however, we suggest that a focus on real-time analytics in a lightweight streaming framework is the most logical step forward.

Using online Latent Semantic Analysis (LSA) from the [`gensim`][gensim] Python package, we extract 'topics' from tweets in an online training fashion. To maintain real-time relevance, the topic model is continually updated, and depending on parameterization, can 'forget' past topics. Based on a set of learned topics, a grid of spatially located tweets for each identified topic is generated using standard `numpy` and `scipy.spatial` functionality. Using an efficient streaming algorithm for approximating 2D kernel density estimation (KDE), locations with the highest density of tweets on a particular topic are located. Locations are semantically labeled using the learned topics, based on the assumption that events can be directly tied to a particularly popular topic at a particular location.

To facilitate real time visualization of results, we utilize the [`pico`][pico] Python/Javascript library as a real-time bridge between server-side Python analysis and client-side Javascript visualization. This enables fast, responsive interactivity of computationally intensive tasks. Additionally, since `pico` allows streaming data from Python to Javascript, updates to the web-interface are sent and consumed as needed, such that only significant changes in an event's status, or the introduction of a new event, will cause updates to the visualizations. Finally, because all models, data structures, and outputs on the server side are pickle-able Python objects, this entire framework is small enough to be deployed on almost any server with Python installed.

[pico]: https://github.com/fergalwalsh/pico
[gensim]: https://github.com/piskvorky/gensim/
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2781/light-weight-real-time-event-detection-with-pytho</guid><enclosure url="http://www.youtube.com/watch?v=iT_QmeJsBhE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/iT_QmeJsBhE/hqdefault.jpg"></media:thumbnail></item><item><title>Multi Purpose Particle Tracking</title><link>http://www.pyvideo.org/video/2764/multi-purpose-particle-tracking</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In many scientific contexts it is necessary to identify and track features in video. Several labs with separate projects and priorities collaborated to develop a common, novice-accessible package of standard algorithms. The package manages optional high-performance components, such as numba, and interactive tools to tackle challenging data, while prioritizing testing and easy adoption by novices.
&lt;p&gt;Description&lt;/p&gt;
Tracking the motion of many particles is an established technique [[Crocker, J.C., Grier, D.G.](http://dx.doi.org/10.1006/jcis.1996.0217)], but many physicists, biologists, and chemical engineers still (make their undergraduates) do it by hand. [Trackpy](https://github.com/soft-matter/trackpy), is a flexible, high-performance implementation of these algorithms in Python using the scientific stack -- including pandas, numba, the IPython notebook, and mpld3 -- which scales well to track, filter, and analyze tens of thousands of feature trajectories.  It was developed collaboratively by research groups at U. Chicago, U.  Penn, Johns Hopkins, and others.

Researchers with very different requirements for performance and precision collaborate on the same package. Some original "magic" manages high-performance components, including numba, using them if they are available and beneficial; however, the package is still fully functional without these features.   Accessibility to new programmers is a high priority.

Biological data and video with significant background variation can confound standard feature identification algorithms, and manual curation is unavoidable. Here, the high-performance group operations in pandas and the cutting-edge notebook ecosystem, in particular the interactive IPython tools and mpld3, enable detailed examination and discrimination.

The infrastructure developed for this project can be applied to other work. Large video data sets can be processed frame by frame, out of core. Image sequences and video are managed through an abstract class that treats all formats alike through a handy, idiomatic interface in a companion project dubbed [PIMS](https://github.com/soft-matter/pims).

A suite of over 150 unit tests with automated continuous integration testing has ensured stability and accuracy during the collaborative process. In our experience, this is an unusual but worthwhile level of testing for a niche codebase from an academic lab.

In general, we have lessons to share from developing shared tools for researchers with separate priorities and varied levels of programming skill and interest.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel B. Allan</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2764/multi-purpose-particle-tracking</guid><enclosure url="http://www.youtube.com/watch?v=MxK7Fe4xfXM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/MxK7Fe4xfXM/hqdefault.jpg"></media:thumbnail></item><item><title>Perceptions of Matplotlib Colormaps</title><link>http://www.pyvideo.org/video/2769/perceptions-of-matplotlib-colormaps</link><description>&lt;p&gt;Abstract&lt;/p&gt;
On several issues related to the perception of colormaps...
&lt;p&gt;Description&lt;/p&gt;
The choice of colormap in a scientific figure significantly affects the way the presented information is perceived by the viewer. This follows on Damon McDougall's talk on how to choose a colormap for an application by delving deeper into several important issues and how well many of the available Matplotlib colormaps stand up against the concerns. For example, it is known that the human brain is better able to interpret changes in magnitude of the luminance and saturation of colors in colormaps instead of the hue. Also, some research has shown that logarithmic changes in brightness are perceived as linear changes. Next, being able to print a color plot in black and white from a published paper is sometimes mandatory and often desirable, and is related to the grey scale in a colormap. Finally, it is important to account for various types of color blindness when choosing a divergent colormap for the plot to be as accessible as possible. All of these concerns have implications for the design of colormaps, and will be examined in the context of the properties of the available Matplotlib colormaps in order to make a best choice for a given application.

Abstract and slides available [on Github.](https://github.com/dmcdougall/scipy14-colormaps)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristen M. Thyng</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2769/perceptions-of-matplotlib-colormaps</guid><enclosure url="http://www.youtube.com/watch?v=rkDgBvT-giw" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/rkDgBvT-giw/hqdefault.jpg"></media:thumbnail></item><item><title>Rasterio: Geospatial Raster Data Access for Programmers and Future Programmers</title><link>http://www.pyvideo.org/video/2771/rasterio-geospatial-raster-data-access-for-progr</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Learn to read, manipulate, and write georeferenced imagery and other kinds of geospatial raster data using a productive and fun GDAL and Numpy-based library named Rasterio. It's a new open source project from the satellite team at Mapbox and is informed by a decade of experience using Python and GDAL.
&lt;p&gt;Description&lt;/p&gt;
Rasterio is a GDAL and Numpy-based Python library guided by lessons learned over a decade of using GDAL and Python to solve geospatial problems. Among these lessons: the importance of productivity, enjoyability, and serendipity.

I will discuss the motivation for writing Rasterio and explain how and why it diverges from other GIS software and embraces Python types, protocols, and idioms.  I will also explain why Rasterio adheres to some GIS paradigms and bends or breaks others.

Finally, I will show examples of using Rasterio to read, manipulate, and write georeferenced raster data. Some examples will be familiar to users of older Python GIS software and will illustrate how Rasterio lets you get more done with less code and fewer bugs. I will also demonstrate fun and useful features of Rasterio not found in other geospatial libraries.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sean Gillies</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2771/rasterio-geospatial-raster-data-access-for-progr</guid><enclosure url="http://www.youtube.com/watch?v=yI9_ozSIKlk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/yI9_ozSIKlk/hqdefault.jpg"></media:thumbnail></item><item><title>Real time Crunching of Petabytes of Geospatial Data with Google Earth Engine</title><link>http://www.pyvideo.org/video/2772/real-time-crunching-of-petabytes-of-geospatial-da</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Google Earth Engine is a platform designed to enable petabyte-scale scientific analysis and visualization of geospatial datasets.  Earth Engine provides a consolidated environment including a massive data catalog co-located with thousands of computers for analysis. This talk will discuss products that Earth Engine has produced, and how to access Earth Engine via its Python API.
&lt;p&gt;Description&lt;/p&gt;
Background
==========

* What is [Earth Engine](https://earthengine.google.org) at a high level?
* Why did the Earth Engine (EE) project start? [To monitor global deforestation.](http://blog.google.org/2010/12/introducing-google-earth-engine.html)
* What architecture design decisions were made, and why?
    - Just-in-time computation model
    - Lazy evaluation for real-time feedback

The Earth Engine Python API
===========================

* PyPI package: [earthengine-api](https://pypi.python.org/pypi/earthengine-api)
* OAuth authentication
* Using IPython Notebooks for algorithm development
    - Special display methods for interactive maps

Philosophical goals and how they are manifested
===============================================

* Organize the world's (geospatial) information and make it universally accessible and useful
* Facilitate open transparent science
* Speed up science by reducing the effort required to test hypotheses
* Enable collaborative algorithm development

Selected Results
================

* Consumer-grade visualizations
    - Time-lapse global scale interactive video - [blog post](http://googleblog.blogspot.com/2013/05/a-picture-of-earth-through-time.html), [interactive viewer (centered on Austin)](https://earthengine.google.org/#timelapse/v=30.27632,-97.74597,10.812,latLng&amp;t=1.67)
* Science-grade Data Products
    - High-Resolution Global Maps of 21st-Century Forest Cover Change - [Science journal publication](http://www.sciencemag.org/content/342/6160/850), [blog post](http://googleresearch.blogspot.com/2013/11/the-first-detailed-maps-of-global.html), [interactive viewer](http://earthenginepartners.appspot.com/science-2013-global-forest)

The Future
==========

* Global-scale analysis challenges
* An invitation for developers
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Randy Sargent</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2772/real-time-crunching-of-petabytes-of-geospatial-da</guid><enclosure url="http://www.youtube.com/watch?v=8LZGBL4U3F4" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/8LZGBL4U3F4/hqdefault.jpg"></media:thumbnail></item><item><title>SimpletITK: Advanced Image Analysis for Python</title><link>http://www.pyvideo.org/video/2763/simpletitk-advanced-image-analysis-for-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SimpleITK brings advanced image analysis capabilities to Python. In particular, it provides support for 2D/3D and multi-components images with physical. SimpleITK exposes a large collection of image processing filters from ITK, including image segmentation and registration. SimpleITK is freely available as an open source package under the Apache 2.0 License.
&lt;p&gt;Description&lt;/p&gt;
SimpleITK provides scientific image analysis, processing, segmentation and registration for biomedical, microscopy and other scientific fields by supporting multi-dimensional images with physical locations [1]. It's is a layer build upon the Insight Segmentation and Registration Toolkit (ITK) [2].

While there are many Python packages to process 2D photographic images, scientific image analysis adds additional requirements.  Images encountered in these domains often have anisotropic pixel spacing, or spatial orientations, and calculations are best performed in physical space as opposed to pixel space. 

SimpleITK brings to Python a plethora of capabilities for performing image analysis. Although SimpleITK was developed by the biomedical imaging community, it is also used for generic image processing. It differentiates from OpenCV in offering 3D images and multi-component images, and it differentiates from scipy by offering the abstraction of image classes and their associated data structures. This applies to images modalities such as CT scans, MRI, fMRI, ultrasound, and in microscopy modalities such as confocal, SEM, TEM, and traditional bright and dark field.

Among the key functionalities supported by SimpleITK are over 260 advanced image filtering and segmentation algorithms as well as access to scientific image file formats, including specialized formats such as DICOM, Nifti, NRRD, VTK and other formats that preserve 3D metadata. Example algorithms include Level Sets Segmentation including multi-phase, Label Maps, Region Growing, Statistical Classification, Advanced Thresholding, Geometrical Transformations, Deconvolution, Anti-Aliasing, Edge Detection, Mathematical Morphology on both labels and grayscale images and Fourier Analysis [4,5]. 

SimpleITK is an open source project with an active community, that builds upon the large amount of image analysis experience of the ITK community [3] working in biomedical images analysis since 1999, and that continues to grow year by year, aggregating state of the art algorithms .

SimpleITK development is sponsored by the US National Library of Medicine.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bradley Lowekamp,Luis Ibanez,Matthew McCormick</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2763/simpletitk-advanced-image-analysis-for-python</guid><enclosure url="http://www.youtube.com/watch?v=1cX3DQ5w6F0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/1cX3DQ5w6F0/hqdefault.jpg"></media:thumbnail></item><item><title>Simulating X-ray Observations with Python</title><link>http://www.pyvideo.org/video/2774/simulating-x-ray-observations-with-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Constructing synthetic X-ray observations from hydrodynamical simulations and other data sources is made possible with a combination of the yt analysis toolkit with a number of other astronomical Python libraries.
&lt;p&gt;Description&lt;/p&gt;
X-ray astronomy is a rapidly expanding field, thanks to the many observations of existing observatories, such as _Chandra_ and _XMM-Newton_, and the anticipation of high-resolution spectral data from upcoming missions such as _Astro-H_ and _Athena+_. Understanding these observations and connecting them to astrophysical mechanisms requires not only detailed modeling of the underlying physics but reliable reproduction of the observed phenomena. I present a method of creating synthetic X-ray observations from numerical simulations, which leverages several astronomical Python libraries, including [yt](http://yt-project.org \"yt\"), [AstroPy](http://www.astropy.org \"AstroPy\"), and [PyXspec](https://heasarc.gsfc.nasa.gov/xanadu/xspec/python/html/ \"PyXspec\"). I will describe the method of generating the observations, the Python packages used, and applications of the method, including connecting observations of galaxy clusters with MHD simulations and preparing simulations for observation proposals.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John ZuHone</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2774/simulating-x-ray-observations-with-python</guid><enclosure url="http://www.youtube.com/watch?v=fUMq6rmNshc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/fUMq6rmNshc/hqdefault.jpg"></media:thumbnail></item><item><title>Software for Panoptes: A Citizen Science Observatory</title><link>http://www.pyvideo.org/video/2776/software-for-panoptes-a-citizen-science-observat</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In this presentation, we describe the current status of software for Project Panoptes.  Our goal is to build low cost, reliable, robotic telescopes which can be used to detect transiting exoplanets.  Panoptes is designed from the ground up to be a citizen science project which will involve the public in all aspects of the science, from data acquisition to data reduction.
&lt;p&gt;Description&lt;/p&gt;
The goal of Project Panoptes (Panoptic Astronomical Networked OPtical observatory for Transiting Exoplanets Survey, see http://projectpanoptes.org/) is to build low cost, reliable, robotic telescopes which can be used to detect transiting exoplanets.  The hardware is designed to be standardized, using as many commercial off the shelf components as possible so that a Panoptes "unit" can be reproduced quickly and easily by students or amateurs.  In this way, many units can be deployed at many different sites to provide continuous and redundant sky coverage.  Panoptes is designed from the ground up to be a citizen science project which will involve the public in all aspects of the science, from data acquisition to data reduction.

In this presentation, we describe the current status of the Panoptes Observatory Control System (POCS, see https://github.com/panoptes/POCS), an open source, collaborative, python-based software package.  POCS is designed to be a simple as possible in order to make it accessible to non-experts.  As such, POCS is a state machine which transitions between a few well defined operating states.  We make extensive use of existing modules (notably astropy and pyephem).  The challenge we face in writing POCS to to balance our desire for simplicity and accessibility against capability.

We will also briefly describe the other software challenges of our project, specifically an algorithm designed to extract accurate photometry from DSLR images (color images obtained using a Bayer color filter array) rather than from the more traditional filtered monochrome CCD image.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Josh Walawender,Michael Butterfield</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2776/software-for-panoptes-a-citizen-science-observat</guid><enclosure url="http://www.youtube.com/watch?v=VDWXJQggFH8" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/VDWXJQggFH8/hqdefault.jpg"></media:thumbnail></item><item><title>The History and Design Behind the Python Geophysical Modelling and Interpretation (PyGMI) Package</title><link>http://www.pyvideo.org/video/2780/the-history-and-design-behind-the-python-geophysi</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The development of geophysical software by individual scientists is achievable through languages such as Python. All goals behind developing a geophysical potential field interpretation and modelling software have been achieved to date. The implication of this is that innovation can be a driving force in projects, rather than waiting for commercial vendors to provide appropriate scientific tools.
&lt;p&gt;Description&lt;/p&gt;
The Council for Geoscience (CGS) is the so called "Geological Survey"
of South Africa. Like many similar institutions around the world, financial
restrictions play a significant role in limiting what tools are available to
scientists. It was from this need to stay scientifically current, while keeping
the software inexpensive, that the examination of Python first started and
ultimately ended up in the PyGMI project.

The origins of PyGMI started with two separate projects. The first was a joint
project where the CGS was responsible for the creation of a software interface
for cluster analysis code, developed by the University of Potsdam (Paasche et
al 2009). The resulting project was done entirely in Python. Data could be
imported, filtered, analyzed and displayed in graph form using Matplotlib.

The second project stemmed from the need to perform 3D modelling on geophysical
data. The creation of 3D models can be extremely time-consuming. Packages
available tend to follow either the modelling of individual 2.5D profiles,
which are then joined up into 3D sections, or modelling fully in
three dimensions using polygonal based models. The initial idea was to use the
VTK library as the means to create, display and interrogate the model, while
using the Scipy and Numpy libraries to perform the actual potential field
calculations. It soon became apparent that editing the resulting mesh quickly
became complex and time consuming. The ability to easily create and change a
model is the very basis of forward modelling and for this reason a new approach
was adopted. The newer 3D modelling package was designed to allow the user to
model simply by drawing the model, in the same way one would draw views of a
house using a paint program. This implies the need to have a front view, as
well as a top view. The model is therefore voxel based rather than polygonal.
The final model can be displayed either within the PyGMI software, or exported
to Google Earth for examination.

Ultimately these two projects formed the basis of what is now the actual PyGMI
package -- which is a modular collection of various techniques, including
multivariate statistical analysis and potential field modelling. The interface
follows a flow diagram approach and the individual modules are independent
enough to ensure that they do not interfere with code which has preceded them
in previous modules.

The PyGMI software is available for free download at: [https://code.google.com/p/pygmi/](https://code.google.com/p/pygmi/)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Patrick Cole</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2780/the-history-and-design-behind-the-python-geophysi</guid><enclosure url="http://www.youtube.com/watch?v=5kM3tKkjoSw" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/5kM3tKkjoSw/hqdefault.jpg"></media:thumbnail></item><item><title>The KBase Narrative Bioinformatics for the 99%</title><link>http://www.pyvideo.org/video/2762/the-kbase-narrative-bioinformatics-for-the-99</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The KBase Narrative builds on the IPython Notebook to provide a multi-user, virtualized Bioinformatics Laboratory Notebook that brings Experimental/Wetlab Biologists, students and the bio-curious into the world of Computational Biology. Tools for genome annotation, visualization, metabolic modeling and more are made available in a collaborative and educational web interface.
&lt;p&gt;Description&lt;/p&gt;
Computional Biology and Experimental Biology are two specialities that would deeply
benefit from more interaction - computationalists need access to data, biologists in
wetlabs need computational tools. The KBase Narrative is a computerized laboratory
notebook that puts the power of the KBase predictive biology platform into the hands of
experimentalists and students. KBase provides cluster computation, analysis and modeling pipelines,
large public datasets and a "pluggable" architecture for future services. The Narrative
is an interface enabling the sharing of data, approaches and workflows on KBase. It
also serves as a teaching tool and publishing platform, allowing other scientists and
students to observe and reproduce the processes that led to the published result.

The KBase Narrative is based on the IPython Notebook, extended in the following ways:

* Notebooks are stored in a remote object store that enables versioning, provenance and sharing
* Support for multiple users has been added, based on OAuth authentication against a "cloud" authentication service (Globus Online)
* A framework for dynamically building form inputs for services using Python introspection and the IPython Traitlets package (a version of Traits) and displaying the output in JS visualization widgets
* A Docker based provisioning system that builds and tears down sandboxed IPython Notebook servers on demand, providing a scalable, reasonable safe and easy to use environment for running hosted IPython notebooks with much smaller overhead than VM's
* A heavily modified user interface that has been designed to support computational biology workflows

The current KBase Narrative was developed over the span of roughly 6 months by a small
team of developers and user interface experts - the short time scale was possible due to the
huge amount of functionality already provided by the IPython Notebook, and taking advantage
of the productivity and power of the Python language.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bill Rihl</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2762/the-kbase-narrative-bioinformatics-for-the-99</guid><enclosure url="http://www.youtube.com/watch?v=52uP6NZa8rE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/52uP6NZa8rE/hqdefault.jpg"></media:thumbnail></item><item><title>Transient detection and image analysis pipeline for TOROS project</title><link>http://www.pyvideo.org/video/2770/transient-detection-and-image-analysis-pipeline-f</link><description>&lt;p&gt;Abstract&lt;/p&gt;
TOROS project will be an astronomical survey of the southern hemisphere in search of optical transients counterparts for aLIGO. This project will make extended use of Machine learning techniques to identify interesting transient candidates to aLIGO alerts. It also uses OpenCV library for image aligning and some of the subsequent processing.
&lt;p&gt;Description&lt;/p&gt;
In preparation for the [TOROS project](http://toros.phys.utb.edu/TOROS/Welcome.html \"TOROS project\") designed to survey the southern hemisphere sky in search for transients, we develop a pipeline for image analysis and processing based on Python.

The code makes extended use of the open source image processing library OpenCV to align the images astrometrically and makes use of other astronomical specific routines like the Astropy package to deal with FITS files. The design will involve integration with SciDB, Astrometry.net, parallelization and other pythonic astronomical tools.

This automated optical transients discovery tool will be tested with real (CSTAR and TORITOS telescopes) and simulated data samples as the input for a machine learning classification tool of light-curves based on AstroML and Scikit-Learn libraries.

The project is version controlled using git and we will handle future collaboration among scientist from different countries using the open source project manager Trac. It will be available as an open source project in popular web repositories like github or bitbucket.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Martin Beroiz</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2770/transient-detection-and-image-analysis-pipeline-f</guid><enclosure url="http://www.youtube.com/watch?v=Kq-hd69_1FM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Kq-hd69_1FM/hqdefault.jpg"></media:thumbnail></item><item><title>Using PyNIO and MPI for Python to help solve a big data problem for the CESM</title><link>http://www.pyvideo.org/video/2773/using-pynio-and-mpi-for-python-to-help-solve-a-bi</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The Community Earth System Model produces orders of magnitude more data than earlier models, and the old data handling methods are no longer adequate. We discuss how PyNIO together with MPI for Python has provided the most efficient solution yet tested for the task of converting the raw output of the model to NetCDF files suitable both for archiving and for convenient use by scientists.
&lt;p&gt;Description&lt;/p&gt;
Like most climate models, the CESM (Community Earth System Model)
steps through time as a particular model scenario evolves and, at set
intervals, outputs the state of all the important variables into
single NetCDF files for each component of the model (atmosphere, ocean, land,
and sea ice). Each file contains all the variables for a component at
a single time step. Because the data volume is large, it is
impractical to attempt to handle all the data for a complete model run
as a single aggregation. Therefore, a consensus has evolved to mandate
that the data be reorganized to contain single variables over some
convenient time period. Finding a solution that can take advantage of
multi-core architectures to do the job efficiently has not been
easy. Recently, in an effort to determine the best solution,
researchers at NCAR have conducted a set of benchmark tests to find
the best tool for the job. Contenders included NCO (NetCDF Operators,
the current incumbent for the task); an in-house Fortran code using
the parallel I/O library PIO; a serial Python script using PyNIO; a
version of the PyNIO script adapted to work with mpi4py in a very simple
manner; CDO; NCL; and Pagoda. Surprisingly, PyNIO parallelized with
mpi4py generally outperformed the other contenders by a large margin, 
and will now be tested as a replacement for the existing NCO scripts. 
This talk will look at the simple mpi4py and PyNIO code that achieves this result, 
discuss the reasons why the performance gain varies from case to case, and 
suggest ways to improve performance in challenging cases. Along the way, 
PyNIO's capabilities and recent improvements will be explained. In addition, other
possible contenders for this role, in particular NetCDF4-Python coupled
with mpi4py in a similar fashion, will be benchmarked using the same test suite.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Brown</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2773/using-pynio-and-mpi-for-python-to-help-solve-a-bi</guid><enclosure url="http://www.youtube.com/watch?v=egqo6NfAEtw" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/egqo6NfAEtw/hqdefault.jpg"></media:thumbnail></item><item><title>yt: volumetric data analysis</title><link>http://www.pyvideo.org/video/2779/yt-volumetric-data-analysis</link><description>&lt;p&gt;Abstract&lt;/p&gt;
yt started as a tool for visualizing data from astrophysical simulations, but it has evolved into a method for analyzing generic volumetric data.  In this talk we will present yt 3.0, which includes an increased focus on inquiry-driven analysis and visualization, a sympy-powered unit system, a revised user interface, and the ability to scale to petabyte datasets and tens of thousands of cores.
&lt;p&gt;Description&lt;/p&gt;
1. What is yt?

    1. "Lingua-franca for astrophysical simulations"
        1. [AGORA](https://sites.google.com/site/santacruzcomparisonproject/)
        2. Some selections from [the gallery](http://yt-project.org/gallery.html)
        3. 105 [citations to the yt method paper](http://adsabs.harvard.edu/cgi-bin/nph-ref_query?bibcode=2011ApJS..192....9T&amp;amp;refs=CITATIONS&amp;amp;db_key=AST)

    2. Massively parallel

        1. Examples of large-scale calculations and visualizations performed with yt

        2. Usage data on XSEDE visualization resources

    3. Volumetric data analysis beyond astrophysics

        1. [Neurodome](http://www.neurodome.org/), Whole-earth seismic wave data, Weather simulation data, Nuclear engineering, Radio astronomy

        2. ??? (insert your field here!)

2. What's new in yt-3.0?

    1. Rewrite of data selection, i/o, and field detection and creation

    2. Octree and particle support (i.e., discrete points)

    3. Unit conversions and dimensional analysis baked into the codebase

    4. Rethinking the API, 'rebranding' the project

    5. Advanced volume rendering

3. Growing the Community

    1. The gallery

    2. Workshops

    3. Contributor statistics.

4. The future

    1. New data styles

        1. Unstructured meshes

        2. Finite element analysis

        3. Spectral codes

    2. New domain-specific functionality (beyond astrophysics)

    3. Browser GUIs powered by IPython
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nathan Goldbaum</dc:creator><pubDate>Mon, 14 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2779/yt-volumetric-data-analysis</guid><enclosure url="http://www.youtube.com/watch?v=sNkN7nyj4nE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/sNkN7nyj4nE/hqdefault.jpg"></media:thumbnail></item><item><title>Activity Detection from GPS Tracker Data</title><link>http://www.pyvideo.org/video/2806/activity-detection-from-gps-tracker-data</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Data gathered by personal GPS trackers are becoming a major source of information pertaining to human activity and behaviour. This presentation will include python work flows which aim to accurately and efficiently carry out the necessary computation required to process volunteered GPS trajectories into useful spatial information.
&lt;p&gt;Description&lt;/p&gt;
Most modern GPS processing techniques started to utilise the time interval between points as a major indicator for finding POIs in a user's trajectories. We are taking step back in this process, and only account for the spatial distribution of measured points in order to extract POIs. Time is solely used as a secondary indicator and for ordering purposes.

Points are first cleaned of any highly inaccurate data and stored in a PostGIS environment. Using developed python module, we extract the point data and order them by time into one large trajectory. Then, for each point, we begin selecting its neighbours (both previous and following) until we reach one within a specified distance of 50m from the first point.The number of the selected points is added to the original point as a new attribute.Continuing with the process the newly calculated values create an imitation of signal, reflecting a point density.

Shift in strength of the signal signifies a change in a user's travel mode which can be used for segmentation of the trajectory into homogeneous parts. Identification of these parts is currently based on the decision tree, where individual aspects of the segment (like average speed, signal strength, time elapsed or centroid) are evaluated and the segment is categorized.

Current work seeks to incorporate neural networks into the processing framework. Since the signal pattern of the each mode of transportation (car, bus, walk, etc.) is independent from the user behaviour, a properly trained model should classify more accurately activities for a broad range of users.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan Vandrol</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2806/activity-detection-from-gps-tracker-data</guid><enclosure url="http://www.youtube.com/watch?v=8t__23WhJ1U" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/8t__23WhJ1U/hqdefault.jpg"></media:thumbnail></item><item><title>A Python Framework for 3D Gamma Ray Imaging</title><link>http://www.pyvideo.org/video/2804/a-python-framework-for-3d-gamma-ray-imaging</link><description>&lt;p&gt;Abstract&lt;/p&gt;
A system capable of imaging gamma rays in 3D in near real time has been developed. A flexible software framework has been developed using Python to acquire, analyze, and finally visualize data from multiple sensors, including novel gamma ray imaging detectors and a Microsoft Kinect.
&lt;p&gt;Description&lt;/p&gt;
**Introduction and Motivation**

Gamma-rays are photons with energies typically thousands to millions of
times greater than the energy of visible light photons. The vastly higher
energies of gamma-rays means that they interact differently with matter,
necessitating new sensors and imaging methods to localize gamma ray sources.
Many sensors and imaging approaches have been developed to image gamma-rays
in 2D, as in a conventional camera, with applications in astronomy, medical
imaging, and nuclear security.  We have developed a mobile gamma-ray imaging
system that merges data from both visual and gamma-ray imaging sensors to
generate a visualization of the 3D gamma-ray distribution in real-time. This
creates 3D maps of the physical environment and correlates that with the
objects emitting gamma-rays. We have used Python to develop a flexible
software framework for acquiring data from the multiple sensors, analyzing
and merging data streams, and finally visualizing the resulting 3D gamma-ray
maps. 

**Methods**

The system consists of a cart that contains a state-of-the art gamma-ray
imaging system, called a Compton Imager, coupled with an RGB-D imaging
system, a Microsoft Kinect. The software package has three main tasks:
gamma-ray acquisition and processing, visual data processing, and finally
the merger of these two streams. The gamma-ray data processing pipeline
involves many computationally intensive tasks, thus a threaded structure
built with multiprocessing forms the basis of the gamma-ray imaging
framework. Furthermore, many other Pythonic tools have been used to meet our
real-time goal; including numexpr, cython, and even the Python/C API.
Several GUI frontends, built with TraitsUI or PySide for example, are used
to monitor and control how the acquired data is processed in real-time,
while a suite of real-time diagnostics are displayed with matplotlib. The
visual pipeline is based on an open-source implementation of RGBDSLAM
(http://wiki.ros.org/rgbdslam), which is built on the Robot Operating System
(ROS) framework. Finally, these two data streams are sent to a laptop
computer via pyzmq, where the final merger and imaging (by solving a
statistical inversion problem constrained by the visual data) is
accomplished. The results are then displayed as they are produced by the
imaging algorithm using mayavi.

**Results**

Link to Video: [Moving Cart 3D scene](https://www.dropbox.com/s/1w5yrqwepjcbpt1/Moving%20Cart%203D%20scene.mov)

This system has been used to demonstrate real-time volumetric gamma ray
imaging for the first time [1]. The results from a typical run are shown in
the above video. The red line indicates the movement of the system through
the environment, while the blue arrows represent an aspect of the gamma-ray
data. The 3D point-cloud provided by RGBDSLAM appear incrementally as the
system traverses the environment. In the end, the location of a small
gamma-ray emitting source is correctly identified with the hotspot in the
image.

[1] [https://www.nss-mic.org/2013/ConferenceRecord/Details.asp?PID=N25-4]
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrew Haefner,Ross Barnowski</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2804/a-python-framework-for-3d-gamma-ray-imaging</guid><enclosure url="http://www.youtube.com/watch?v=I_YpsekEhhg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/I_YpsekEhhg/hqdefault.jpg"></media:thumbnail></item><item><title>A Success Story in Using Python in a Graduate Chemical Engineering Course</title><link>http://www.pyvideo.org/video/2800/a-success-story-in-using-python-in-a-graduate-che</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I recently used Python in a new required graduate level chemical reaction engineering core course. The course was taken by 60 Master's students with a broad set of educational backgrounds and programming experience. Several factors contributed to the success of this course, which I will present and discuss. Based on my experience, it is feasible to use Python in engineering courses.
&lt;p&gt;Description&lt;/p&gt;
Historically, Matlab has been the primary math software tool used in our
courses on Chemical Engineering. Last year, I taught the first course in the
department using Python. In this talk I will present how I did that, and why
it was possible. The first step was demonstrating that Python + numpy +
scipy + matplotlib can solve all the problems we used to solve with Matlab.
This was documented in a project called PYCSE through a series of over one
hundred blog posts and organized in a web site ([1][]). Second, the
development of Python distributions such as Enthought Canopy made it
possible to students to easily install and use Python. I had to augment this
with some additional functionality with PYCSE ([2][]) which adds some
statistical analysis, differential equation solvers, numerical
differentiation functions and a publish function to convert Python scripts
to PDF files with captured output for grading. The only feature of Python
missing is a robust units package; several partial solutions exist, but none
solve all the needs of engineering calculations. Third, Emacs + org-mode
enabled me to write the course notes with integrated Python code and output.
These notes were provided to the students in PDF form, and annotated during
lecture using a tablet PC. Finally, the course was administered with box.com
and a custom python module to automate assignment collection and return
([3][]). An integrated grade widget in the PDF files that was created when
the students published their assignments was used to aggregate the grades
for the gradebook. I used an innovative homework schedule of one problem
every 2-4 days with rapid feedback to keep students using Python frequently.
We used timed quizzes and online exams to assess their learning. Overall,
the course was successful. Student evaluations of the course were as good as
courses that used other software packages. Based on my experiences, I will
continue to use Python and expand its role in engineering education.



[1]: http://kitchingroup.cheme.cmu.edu/pycse

[2]: http://github.com/jkitchin/pycse

[3]: https://github.com/jkitchin/box-course
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">John Kitchin</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2800/a-success-story-in-using-python-in-a-graduate-che</guid><enclosure url="http://www.youtube.com/watch?v=IsSMs-4GlT8" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/IsSMs-4GlT8/hqdefault.jpg"></media:thumbnail></item><item><title>Campaign for IT literacy through FOSS and Spoken Tutorials</title><link>http://www.pyvideo.org/video/2787/campaign-for-it-literacy-through-foss-and-spoken</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Textbook Companion (TBC) has code for solved problems of textbooks,
coordinated by FOSSEE (http://fossee.in).
We explain a collaborative method
that helped create 500 Scilab TBCs (http://cloud.scilab.in) and over 100
Python TBCs (http://tbc-python.fossee.in/). We also explain a self learning
method that trained 250,000 students on FOSS systems using spoken
tutorials (http://spoken-tutorial.org).

&lt;p&gt;Description&lt;/p&gt;
The FOSSEE team has been promoting the use of FOSS in educational
institutions in India. It focuses on the following FOSS systems currently:
Scilab, Python, Oscad, OpenFOAM, COIN-OR and OpenFormal
([http://fossee.in](http://fossee.in)).  On each of these systems, the
following three standardised help are provided:

1. Support to conduct spoken tutorial based workshops, explained below
2. Creation of Textbook Companion (TBC) 
3. Support to Lab Migration.

A TBC is a collection of code for solved examples of standard textbooks.  We
have completed a large number of TBCs on Scilab and made them available for
online use at [Scilab](http://cloud.scilab.in/) and offline use at
[Completed Books](http://www.scilab.in/Completed_Books).  Similarly, one may
access the Python TBC at [Python TBC](http://tbc-python.fossee.in/).  These
TBCs are created by students and teachers from many colleges and each
creator is paid an honorarium through a project funded by the Government of
India.

[Spoken Tutorial](http://spoken-tutorial.org) is a screencast of ten minute
duration on a FOSS topic, created for self learning.  Using Spoken
Tutorials, we conduct two hour long workshops on FOSS topics through
volunteers, who need not be experts.  We conduct online tests and provide
certificates for all who pass the tests.  All of these are done completely
free of cost, thanks to the financial support from the Government of India.
Using this method, we have trained more than 200K students in the last two
years in India ([statistics](http://www.spoken-tutorial.org/statistics)).

The students love this method, see some testimonials at
[http://www.spoken-tutorial.org/testimonials](http://www.spoken-tutorial.org/testimonials).  It is being increasingly
accepted by colleges and universities, officially.  We expect to
conduct 5K to 10K workshops in 2014, training 200K to 500K students.
As we dub the spoken part into all 22 languages of India, the FOSS
topics are accessible also to students who are not fluent in English,
thereby helping us reach out to many students.  Spoken Tutorial, which
started as a documentation project for FOSS systems has transformed
into a massive training programme.  Our methods are scalable and are
available to the FOSS enthusiasts in the rest of the world.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kannan Moudgalya</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2787/campaign-for-it-literacy-through-foss-and-spoken</guid><enclosure url="http://www.youtube.com/watch?v=rCSoGLjqyyE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/rCSoGLjqyyE/hqdefault.jpg"></media:thumbnail></item><item><title>Climate &amp; GIS: User Friendly Data Access, Workflows, Manipulation, Analysis and Visualization of Climate Data</title><link>http://www.pyvideo.org/video/2783/climate-gis-user-friendly-data-access-workflo</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Understanding environmental and climate change requires data fusion, format conversions, processing and visualization to gain insight into the data. Our open source scientific Python and JavaScript based tools makes it easy to manipulate geo-spatial and climate data, create and execute workflows, and produce visualizations over the web for scientific and decision making tools.
&lt;p&gt;Description&lt;/p&gt;
The impact of climate change will resonate through a broad range of fields
including public health, infrastructure, water resources, and many others.
Long-term coordinated planning, funding, and action are required for climate
change adaptation and mitigation. Unfortunately, widespread use of climate data
(simulated and observed) in non-climate science communities is impeded by
factors such as large data size, lack of adequate metadata, poor documentation,
and lack of sufficient computational and visualization resources. Additionally,
working with climate data in its native format is not ideal for all types of
analyses and use cases often requiring technical skills (and software)
unnecessary to work with other geospatial data formats. 

We present open source tools developed as part of ClimatePipes and
OpenClimateGIS to address many of these challenges by creating an open source
platform that provides state-of-the-art user-friendly data access,
processing, analysis, and visualization for climate and other relevant
geospatial datasets making the climate and other geospatial data
available to non-researchers, decision-makers, and other stakeholders.

The overarching goals are: 

* Enable users to explore real-world questions related to environment and climate change.
* Provide tools for data access, geo-processing, analysis, and visualization.
* Facilitate collaboration by enabling users to share datasets, workflows, and visualization. 

Some of the key technical features include

1. Support for multiprocessing for large datasets using Python-celery distributed task queuing system
2. Generic iterators allowing data to be streamed to arbitrary formats (relatively) easily (e.g. ESRI Shapefile, CSV, keyed ESRI Shapefile, CSV, NetCDF)
3. NumPy based array computations allowing calculations such as monthly means or heat indices optionally on temporally grouped data slices
4. Decorators to expose existing Python API as a RESTful API
5. Simple to use, lightweight Web-framework and JavaScript libraries for analyzing and visualizing geospatial datasets using D3 and WebGL.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aashish Chaudhary</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2783/climate-gis-user-friendly-data-access-workflo</guid><enclosure url="http://www.youtube.com/watch?v=0SnB6C-Ixso" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/0SnB6C-Ixso/hqdefault.jpg"></media:thumbnail></item><item><title>CodaLab: A New Service for Data Exchange, Code Execution, Benchmarks &amp; Reproducible Research</title><link>http://www.pyvideo.org/video/2792/codalab-a-new-service-for-data-exchange-code-ex</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Learn, Share and Collaborate with CodaLab -- A new open source platform which lets communities create and explore experiments together and engage in benchmarking and competitions to enable true reproducibility and advance the state of the art in data-driven research
&lt;p&gt;Description&lt;/p&gt;
CodaLab is a web-based open source platform that allows researchers to share and browse code, data, and create experiments in a truly reproducible manner. CodaLab focuses on accomplishing the following:

* Serve as a data repository for data sets including large scale data sets that could only be hosted in a cloud computing environment 
* Serve as an algorithm repository that researchers can use in their experimentation, to teach and learn from others
* Host the execution of experiments as worksheets, sometimes referred to as "executable papers", which are annotated scientific documents that combine textual process descriptions with live data sets and functioning code.
* Enable the creation of benchmarks 

CodaLab is a community-driven effort led by Percy Liang from Stanford University who built the precursor of CodaLab, namely, MLComp. From a development viewpoint CodaLab supports both the Linux and Windows communities with code in GitHub and Python as one of the main language used to support the scientific community.

At SciPi, we invite the community to participate in CodaLab by creating experiments as executable papers and by sharing them with the rest of the community at [http://codalab.org](http://codalab.org). These worksheets or "executable papers" can then be freely reproduced, appended, and otherwise modified to improve productivity and accelerate the pace of discovery and learning among data-driven scientific professionals.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christophe Poulain</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2792/codalab-a-new-service-for-data-exchange-code-ex</guid><enclosure url="http://www.youtube.com/watch?v=vla-EQaPHYU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/vla-EQaPHYU/hqdefault.jpg"></media:thumbnail></item><item><title>Fast Algorithms for Binary Spatial Adjacency Measures</title><link>http://www.pyvideo.org/video/2793/fast-algorithms-for-binary-spatial-adjacency-meas</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Spatial weights matrices, $W$, represent potential interaction between all $i,j$ in a study area and play an essential role in many spatial analysis tasks. Commonly applied binary adjacency algorithms using decomposition and tree representations scale quadratically and are ill suited for large data sets. We present a linearly scaling, adjacency algorithm with significantly improved performance.
&lt;p&gt;Description&lt;/p&gt;
Spatial weights matrices, $W$, play an essential role in many spatial analysis tasks including measures of spatial association, regionalization, and spatial regression.  A spatial weight $w_{i,j}$ represents potential interaction between each $i,j$ pair in a set of $n$ spatial units.  The weights are generally defined as either binary $w_{i,j}=\\{1,0\\}$, depending on whether or not $i$ and $j$ are considered neighbors, or a continuous value reflecting some general distance relationship between $i$ and $j$.  This work focuses on the case of binary weights using a contiguity criteria where $i$ and $j$ are rook neighbors when sharing an edge and queen neighbors when sharing a vertex.

Population of the $W$ is computationally expensive, requiring, in the naive case, $O(n^2)$ point or edge comparisons.  To improve efficiency data decomposition techniques, in the form of regular grids and quad-trees, as well as spatial indexing techniques using r-trees have be utilized to reduce the total number of local point or edge comparisons.  Unfortunately, these algorithms still scale quadratically.  Recent research has also shown that even with the application of parallel processing techniques, the gridded decomposition method does not scale as $n$ increases.  

This work presents the development and testing of a high performance [implementation](https://github.com/jlaura/pysal/blob/weights/pysal/weights/_contW_binning.py#L181), written in pure Python, using time constant and $O(n)$ operations, by leveraging high performance containers and a vertex comparison method.  The figures below depict results of initial testing using synthetically generated lattices of triangles, squares, and hexagons with rook contiguity in black and queen contiguity in gray.  These geometries were selected to control for average neighbour cardinality and average vertex count per geometry.  From these initial tests, we report a significant speedup over r-tree implementations and a more modest speedup over gridded decomposition methods.  In addition to scaling linearly, while existing methods scale quadratically, this method is also more memory efficient.  Ongoing work is focusing on testing using randomly distributed data, and U.S. Census data, the application of parallelization techniques to test further performance improvements, and the use of fuzzy operator to account for spatial error .

![raw times](http://github.com/pysal/pPysal/blob/master/weights/figures/rawtime.png?raw=true \"Initial Results - Raw Speed\")

![speedup](http://github.com/pysal/pPysal/blob/master/weights/figures/speedup.png?raw=true \"Initial Results - Speedup\")

![linear speed](http://github.com/pysal/pPysal/blob/master/weights/figures/rawl.png?raw=true \"Initial Results - List Raw Speed\")
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Lauara</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2793/fast-algorithms-for-binary-spatial-adjacency-meas</guid><enclosure url="http://www.youtube.com/watch?v=kNcA-yE_iNI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/kNcA-yE_iNI/hqdefault.jpg"></media:thumbnail></item><item><title>GeoPandas: Geospatial Data + Pandas</title><link>http://www.pyvideo.org/video/2790/geopandas-geospatial-data-pandas</link><description>&lt;p&gt;Abstract&lt;/p&gt;
GeoPandas extends the pandas data analysis library to work with geographic objects.
&lt;p&gt;Description&lt;/p&gt;
[GeoPandas](https://github.com/kjordahl/geopandas) is a library built on top of pandas to extend its capabilities to allow spatial calculations.  The two main datatypes are `GeoSeries` and `GeoDataFrame`, extending pandas `Series` and `DataFrame`, respectively.  A `GeoSeries` contains a collection of geometric objects (such as `Point`, `LineString`, or `Polygon`) and implements nearly all `Shapely` operations.  These include unary operations (e.g. `centroid`), binary operations (e.g. `distance`, either elementwise to another `GeoSeries` or to a single geometry), and cumulative operations (e.g. `unary_union` to combine all items to a single geometry).

A `GeoDataFrame` object contains a column of geometries (itself a `GeoSeries`) that has special meaning.  GeoDataFrames can be easily created from spatial data in other formats, such as shapefiles.  Rows in the `GeoDataFrame` represent features, and columns represent attributes.  Pandas' grouping and aggregation methods are also supported.

GeoPandas objects can optionally be aware of coordinate reference systems (by adding a `crs` attribute) and transformed between map projections.  Basic support for plotting is included with GeoPandas.  Other features include geocoding, export to GeoJSON, and retrieving data from a PostGIS spatial database.

This talk will describe the main features of GeoPandas and show examples of its use.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelsey Jordahl</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2790/geopandas-geospatial-data-pandas</guid><enclosure url="http://www.youtube.com/watch?v=IJY6duEQVy0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/IJY6duEQVy0/hqdefault.jpg"></media:thumbnail></item><item><title>Geospatial Data and Analysis Stack</title><link>http://www.pyvideo.org/video/2799/geospatial-data-and-analysis-stack</link><description>&lt;p&gt;Abstract&lt;/p&gt;
There are a growing number of Python packages (fiona, geopandas, pysal, shapely, etc.) addressing various types of spatial data, as well as the geoprocessing of that data and its statistical analysis. This session explore ways to best collaborate between and strengthen these efforts.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Serge Rey</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2799/geospatial-data-and-analysis-stack</guid><enclosure url="http://www.youtube.com/watch?v=h8aI6_cIAqY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/h8aI6_cIAqY/hqdefault.jpg"></media:thumbnail></item><item><title>IPython-Reveal.js Attacks Again, But Now... It Is Alive!</title><link>http://www.pyvideo.org/video/2798/ipython-revealjs-attacks-again-but-now-it-is</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Recently, the IPython Notebook has began to be used for presentations in several conferences. But, there is not a full-featured and executable IPython presentation tool available. So, we developed a new Reveal.js-powered live slideshow extension, designed specifically to be used directly from the notebook and to be as executable as the notebook is, and also powered with great features.

&lt;p&gt;Description&lt;/p&gt;
**Detailed Abtract**

In the last years, **IPython**, a comprehensive environment for interactive and exploratory computing, has arose as must-have application to run in the daily scientific *work-flow* because provides, not only an enhanced interactive **Python** shell (terminal or qt-based), but also a very popular interactive browser-based **notebook** with an unimaginable scope.

The presentation of our research results and the teaching of our knownledge are very important steps in the scientific research work-flow, and recently, the **IPython Notebook** has began to be used for all kind of oral communications in several conferences, curses, classes and bootcamps.  

Despite the fact that we can present our talks with the IPython notebook itself or through a static **Reveal.js**-based slideshow powered by IPython.nbconvert (a tool we presented at SciPy 2013), there is not a **full-featured** and **executable** (live) IPython presentation tool available. So, we developed a new **IPython**-**Reveal.js**-powered **live** slideshow extension, designed specifically to be used **directly** from the IPython notebook and to be as **executable** as the notebook is (because deep inside, it is the notebook itself but rendered with another face), and also powered with a lot of *features* to address the most common tasks performed during the oral presentation and spreading of our scientific work, such as: main slides and nested slides, fragments views, transitions, themes and speaker notes.

To conclude, we have developed a new visualization tool for the **IPython Notebook**, suited for the final steps of our scientific research *work-flow*, providing us with, not only an enhanced and new experience in the oral presentation and communication of our results, but also with a super-powerful tool at teaching time, helping us to easily tranfer our concepts and spread our knowledge.

**Important**

You can see the extension in action in the following video link: [http://www.youtube.com/watch?v=Pc-1FS0l2vg](http://www.youtube.com/watch?v=Pc-1FS0l2vg)

And you can also see the source code of the extension at the following repository: [http://github.com/damianavila/live_reveal](http://github.com/damianavila/live_reveal)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damian Avila</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2798/ipython-revealjs-attacks-again-but-now-it-is</guid><enclosure url="http://www.youtube.com/watch?v=sZBKruEh0jI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/sZBKruEh0jI/hqdefault.jpg"></media:thumbnail></item><item><title>Lightning Talks | SciPy 2014 | July 10th, 2014</title><link>http://www.pyvideo.org/video/2803/lightning-talks-scipy-2014-july-10th-2014</link><description></description><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2803/lightning-talks-scipy-2014-july-10th-2014</guid><enclosure url="http://www.youtube.com/watch?v=ln4nE_EVDCg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/ln4nE_EVDCg/hqdefault.jpg"></media:thumbnail></item><item><title>Mapping Networks of Violence</title><link>http://www.pyvideo.org/video/2807/mapping-networks-of-violence</link><description>&lt;p&gt;Abstract&lt;/p&gt;
A novel approach to both violence prevention and the measurement of propensity to violence is presented.  The work is part of the evaluation of Cure Violence's (Ransford, Kane and Slutkin 2009; Slutkin 2012) implementation in NYC. Python libraries such as IPython, PySAL, Numpy, Basemap, Fiona, Shapely, Matplotlib, bNetworkX, Pandas and scikit-learn feature prominently in the work.

&lt;p&gt;Description&lt;/p&gt;
Violence remains a significant problem in New York City's poor
neighborhoods.  There were more than 9,000 gun homicides in 2008 (FBI,
2009) and the CDC (2012) reports that there were more than 71K
non-fatal wounds in the US. One novel approach to the problem of
violence is the Cure Violence Model (Ransford, Kane and Slutkin 2009;
Slutkin 2012).  Cure Violence treats violence as a disease passed
between people in a social network.  The program tries to use the same
network to change how people who are prone to and have been the
victims of violence react to stress and conflict.  Cure Violence is
viewed as having been successful in Chicago and shown promising in
other cities (Skogin 2009, Wilson 2010, Webster 2009).  All of these
studies have used reported incidents of violence before and after the
program to assess the efficacy. The NYC Council and Robert Wood
Johnson Foundation have commited significant resoures to this
approach.  Both have retained the CUNY John Jay Research &amp; Evaluation
center to evaluate the efficacy. Our research adds to the literature
by being the first to attempt to measure the change in the propensity
to violence of people in the community. Novel preliminary research is
presented on network cliques of respondents and the demographic,
education, victimization experiences that constitute greatest risk.
All of the analysis was conducted in Python libraries including
IPython, PySAL, Numpy, Basemap, Fiona, Shapely, Matplotlib, bNetworkX,
Pandas and scikit-learn.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Evan Misshula,Sheyla Delgado</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2807/mapping-networks-of-violence</guid><enclosure url="http://www.youtube.com/watch?v=SDi4TNqaR2s" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/SDi4TNqaR2s/hqdefault.jpg"></media:thumbnail></item><item><title>MASA: A Tool for the Verification of Scientific Software</title><link>http://www.pyvideo.org/video/2796/masa-a-tool-for-the-verification-of-scientific-s</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Numerical simulations have a broad range of application, from aircraft design to drug discovery. However, any prediction from a computer must be tested to ensure its reliability. Verification ensures the outputs of a computation accurately reflect the underlying model. This talks covers MASA, a tool for the verification of software used in a large class of problems in applied mathematics.
&lt;p&gt;Description&lt;/p&gt;
Numerical simulations have an incredibly broad range of applicability, from computer aided aircraft design to drug discovery. However, any prediction arising from a computer model must be rigorously tested to ensure its reliability. Verification is a process that ensures that the outputs of a computation accurately reflect the solution of the mathematical models. 

This talk will first provide an introduction to the method of manufactured solutions (MMS), which is a powerful approach for verification of model problems in which a solution cannot be determined analytically. However, verifying computational science software using manufactured

solutions requires the generation of the solutions with associated forcing terms and their reliable implementation in software. There are several issues that arise in generating solutions, including ensuring that they are meaningful, and the algebraic complexity of the forcing terms. After briefly discussing these issues, the talk will introduce MASA, the Manufactured Analytical Solution Abstraction library. MASA is an open-source library written in C++ (with python interfaces) which is designed for the veri?cation of software used for solving a large class of problems stemming from numerical methods in applied mathematics including nonlinear equations, systems of algebraic equations, and ordinary and partial differential equations.

Example formulations in MASA include the Heat Equation, Laplace's Equation, and the Navier-Stokes Equations, but in principle MASA supports instantiating any model that can be written down mathematically. This talk will end with details on two methods to import manufactured solutions into the library, either by generate the source terms, or using the automatic differentiation capabilities provided in MASA.

The library is available at: [https://github.com/manufactured-solutions/MASA](https://github.com/manufactured-solutions/MASA)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicholas Malaya</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2796/masa-a-tool-for-the-verification-of-scientific-s</guid><enclosure url="http://www.youtube.com/watch?v=OvyqO4fxdKs" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/OvyqO4fxdKs/hqdefault.jpg"></media:thumbnail></item><item><title>Measuring Rainshafts: Bringing Python to Bear on Remote Sensing Data</title><link>http://www.pyvideo.org/video/2788/measuring-rainshafts-bringing-python-to-bear-on</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This presentation details how Python is being used to extract geophysical insight from active remote sensing data, namely Radars. By using a common data model our work bridges the gap between the domains of radar engineering and image analysis.
&lt;p&gt;Description&lt;/p&gt;
Remote sensing data is complicated, very complicated! It is not only geospatially tricky but also indirect as the sensor measures the interaction of the media with the probing radiation, not the geophysics. However the problem is made tractable by the large number of algorithms available in the Scientific Python community, what is needed is a common data model for active remote sensing data that can act as a layer between highly specialized file formats and the cloud of scientific software in Python. This presentation motivates this work by asking: How big is a rainshaft? What is the natural dimensionality of rainfall patterns and how well is this represented in fine scale atmospheric models. Rather than being specific to the domain of meteorology we will break down how we approach this problem in terms what tools across numerous packages we used to read, correct, map and reduce the data to forms able to answer our science questions. This is a "how" presentation, covering signal processing using linear programming methods, mapping using KD Trees, and image analysis using ndimage and, of course graphics using Matplotlib.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Scott Collis</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2788/measuring-rainshafts-bringing-python-to-bear-on</guid><enclosure url="http://www.youtube.com/watch?v=1D0aTToHrCY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/1D0aTToHrCY/hqdefault.jpg"></media:thumbnail></item><item><title>Ocean Model Assessment for Everyone</title><link>http://www.pyvideo.org/video/2795/ocean-model-assessment-for-everyone</link><description>&lt;p&gt;Abstract&lt;/p&gt;
An end-to-end workflow for assessing storm-driven water levels predicted by coastal ocean models will be discussed which uses `OWSLib` for CSW Catalog access, `Iris` for ocean model access and `pyoos` for Sensor Observation Service data access. Analysis and visualization is done with `Pandas` and `Cartopy`, and the entire workflow is shared as in IPython Notebook with custom environment in Wakari.
&lt;p&gt;Description&lt;/p&gt;
To assess how well ocean models are performing, the model products need to be compared with data. Finding what models and data exist has historically been challenging because this information is held and distributed by numerous providers.  Accessing data has been challenging because ocean models produce gigabytes or terabytes of information, is usually stored in scientific data formats like HDF or NetCDF, while ocean observations are often stored in scientific data formats or in databases.

To solve this problem, the Integrated Ocean Observing System (IOOS) has been building a distributed information system based on standard web services for discovery and access.  IOOS is now embarking on a nationwide system-test using python to formulate queries, process responses, and analyze and visualize the data. An end-to-end (search-access-analyze-visualize) workflow for assessing storm-driven water levels predicted by coastal ocean models will be discussed, which uses `OWSLib` for OGC CSW service catalog access, `Iris` for ocean model access and `pyoos` (which wraps OWSLib) for Sensor Observation Service data access. Analysis and visualization is done with `Pandas` and `Cartopy`, and the entire end-to-end workflow is shared as in IPython Notebook with custom environment in Wakari.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Richard Signell</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2795/ocean-model-assessment-for-everyone</guid><enclosure url="http://www.youtube.com/watch?v=WHjU_rg81BI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/WHjU_rg81BI/hqdefault.jpg"></media:thumbnail></item><item><title>Plasticity in OOF</title><link>http://www.pyvideo.org/video/2802/plasticity-in-oof</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We discuss recent advances in the Object Oriented Finite-Element project at NIST, a Python and C++ tool designed to bring sophisticated numerical modeling capabilities to users in the field of Materials Science.
&lt;p&gt;Description&lt;/p&gt;
We discuss recent advances in the Object Oriented Finite-Element project at
NIST (also called [OOF](http://www.ctcms.nist.gov/oof/oof2)), a Python and C++
tool designed to bring sophisticated numerical modeling capabilities to users
in the field of Materials Science.

As part of the effort to expand the solid-mechanics capabilities of the code,
the solver has been extended to include the ability to handle history-dependent
properties, such as occur in viscoplastic systems, and inequality constraints,
which are present in conventional isotropic plasticity, as well as surface
interactions.

This software provides numerous tools for constructing finite-element meshes from microstructural images, and for implementing material properties from a very broad class which includes elasticity, chemical and thermal diffusion, and electrostatics.

The code is a hybrid of Python and C++ code, with the high level user interface and control code in Python, and the heavy numeric work being done in C++. Numerous tools are provided for constructing finite-element meshes from microstructural images, and for implementing material properties from a very broad class which includes elasticity, chemical and thermal diffusion, and electrostatics. The software can be operated either as an interactive, GUI-driven application, as a scripted command-line tool, or as a supporting library, providing useful access to users of varying levels of expertise. At every level, the user-interface objects are intended to be familiar to the materials-science user.

The modular object-oriented design of the code, and the strategy of separating the finite-element infrastructure from the material constitutive rules proved itself in implementing the new solid-mechanics capabilities.

Development on a fully-3D version of the code has also made significant progress, overcoming several challenges associated with user-interface issues. A nontrivial, solved 3D problem will be presented.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrew Reid</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2802/plasticity-in-oof</guid><enclosure url="http://www.youtube.com/watch?v=VzXiPmR_wQk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/VzXiPmR_wQk/hqdefault.jpg"></media:thumbnail></item><item><title>Prototyping a Geophysical Algorithm in Python</title><link>http://www.pyvideo.org/video/2784/prototyping-a-geophysical-algorithm-in-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Spitz' paper on FX pattern recognition contains a long paragraph that describes a model, an algorithm, and the results of applying the algorithm to the model.  The algorithm requires Fourier transforms, convolutional filtering, matrix multiplication, and solving linear equations.  I describe how to use numpy, scipy, and mapplotlib to prototype the algorithm and display the processed model.
&lt;p&gt;Description&lt;/p&gt;
A geophysics paper by Spitz has a long paragraph that describes a model, an algorithm, and the results of applying the algorithm to the model.  I wanted to implement and test the algorithm to ensure I fully understood the method.  This is a good illustration of Python for geophysics because the implementation requires:

1. Fourier transforms provided by numpy.fft
2. Setting up linear equations using numpy.array and numpy.matrix
3. solving the linear equations using scipy.linalg.solve
4. Applying convolutional filters using scipy.signal.lfilter

A bandlimited flat event model is created using array slicing in numpy and is bandlimited in the frequency domain.  Another component of the model is created by convolving a short derivative filter on a similar flat event model.  After Fourier transform, linear equations are set up to compute a prediction filter in the FX domain.  These equations are created using data slicing,  conjugate transpose, matrix multiple (all available in numpy).  Scipy.linalg.solve is used to solve for the prediction error filter.  A final filter is computed using the recursive filter capability in scipy.signal.lfilter.  Results are displayed using matplotlib.

This is quite a tour of scipy and numpy to implement an algorithm described in a single paragraph.  Many operations commonly used in geophysics are illustrated in the program.  The resulting program is less than 200 lines of code.  I will describe the algorithm and share the prototype code.

References:

Spitz, S. (1999). Pattern recognition, spatial predictability, and subtraction of multiple events. The Leading Edge, 18(1), 55-58. [doi: 10.1190/1.1438154](http://dx.doi.org/10.1190/1.1438154)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Karl Schleicher</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2784/prototyping-a-geophysical-algorithm-in-python</guid><enclosure url="http://www.youtube.com/watch?v=kFXVOaZZr-E" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/kFXVOaZZr-E/hqdefault.jpg"></media:thumbnail></item><item><title>Putting the v in IPython: vim-ipython and ipython-vimception</title><link>http://www.pyvideo.org/video/2797/putting-the-v-in-ipython-vim-ipython-and-ipython</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This talk will explain how to intimately integrate IPython with your favorite text editor, as well as how to customize the IPython Notebook interface to behave in a way that makes sense to *you*. Though the concrete examples are centered around the world-view of a particular text editor, the content will be valuable to anyone wishing to extend and customize IPython for their own purposes.

&lt;p&gt;Description&lt;/p&gt;
This talk will cover two projects: [vim-ipython](https://github.com/ivanov/vim-ipython) (1) and [ipython-vimception](https://github.com/ivanov/ipython-vimception) (2)

**1.** Most people think of IPython as an application - but much of it is written as a
library, making it possible to integrate with other tools.

vim-ipython is a Vim plugin that was first written during the sprints at SciPy
2011 as a two-way interface between the Vim text editor and a running IPython
kernel. It turns vim into a frontend for IPython kernels, like the qtconsole
and the notebook interface. It allows you to send lines or whole files for
IPython to execute, and also get back object introspection and word completions
in Vim, like what you get with: object?`&lt;enter&gt;` and object.`&lt;tab&gt;` in IPython.  It
currently has over 430 star gazers on GitHub.  Because vim-ipython simply
leverages much of existing IPython machinery, it allows users to interact with
non-Python kernels (such as IJulia and IHaskell) in the same manner from the
convenience of their favorite text editor. More recently, vim-ipython has
gained the ability to conveniently view and edit IPython notebooks (.ipynb
files) without a running an IPython Notebook server. 

vim-ipython has a small and accessible code base (13 people have contributed
patches to the project), which has frequently made it *the* reference example
for how to implement and utilize the IPython messaging protocol that allows for
the language-independent communication between frontends and kernels.

**2.** The IPython Notebook user interface has become highly customizable, and
authoring code and content in the Notebook can be more pleasant and productive
experience if you take the time to make it yours.

IPython 2.0 brings a modal notion to the Notebook interface.  There are two
modes: edit and mode command mode. In command mode, many single-key keyboard
shortcuts are available. For example, `m` changes the current cell type to
Markdown, `a` and `b` will insert a new cell above and below the current one,
and so on. Edit mode removes these single key shortcuts so that new code and
text can be typed in, but still retains a few familiar shortcuts, such as
`Ctrl-Enter`, `Alt-Enter`, and `Shift-Enter` for cell execution (with some nuanced
differences). 

Part of the motivation behind the introduction of this modal interface was that
performing operations on notebook cells became a tedious and awkward, as most
operations required `Ctrl-m` to be typed too many times. For example, inserting 3 cells involved
`Ctrl-m a Ctrl-m a Ctrl-m a`, whereas now it's just `aaa` in Command mode. But
the other major reason for the modal refactor was to make it possible to add
and remove shortcuts. For example, a user who finds it annoying that `a` stands
for "insert above" and `b` for "insert below" and thinks that `a` for "insert
after" and `b` for "insert before" makes more sense will now be able to make
that change for herself.

Some of the keyboard shortcuts in command mode are already vi-like (`j` and `k`
to move up and down between cells) but many are not, and a few are confusingly
placed. ipython-vimception aims to be a reference implementation for how to
perform shortcut and user interface customization in the notebook. In
particular, along with vim-ipython's new ability to edit .ipynb files,
ipython-vimception addresses the concerns of many die-hard vim aficionados.
Many of them have otherwise shied away form the notebook interface as it
offends their sensibilities for how text editing and document manipulation
should be done. However, with the new customizable shortcut system in IPython,
along with a vim emulation mode in cell text input areas, they finally will
have a way to stay productive without having to change their ways.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Paul Ivanov</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2797/putting-the-v-in-ipython-vim-ipython-and-ipython</guid><enclosure url="http://www.youtube.com/watch?v=p9gnhmX1sPo" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/p9gnhmX1sPo/hqdefault.jpg"></media:thumbnail></item><item><title>pySI  A Python Framework for Spatial Interaction Modelling</title><link>http://www.pyvideo.org/video/2789/pysi-a-python-framework-for-spatial-interaction</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Spatial Interaction Modelling is a method for calibrating parameters for components within a system of flows, such as migration or trade, and then using those parameters to estimate new flows. Despite their popularity, a unified Python framework to employ them does not exist. In response, pySI was created as a coherent tool for calibrating models and simulating flows for a variety of models.
&lt;p&gt;Description&lt;/p&gt;
Functions from libraries such as scipy.optimize, scipy.spatial, statsmodels,
and numdifftools comprise the core of the pySI.calibrate routines, which are
automatically constructed depending upon the specified model inputs. As a
result, the user can focus on identifying different flow systems  and
understanding the associated spatial processes, rather than the algorithmic
divergences which emerge between different models. After calibration is
completed, the estimated parameters and their diagnostic statistics can be
reported in a uniform fashion.  Using functions within pySI.simulate, the
parameter estimates can act as inputs in order to predict new flows. More
recently developed models, which do not require input parameters, are also made
available, allowing comparisons amongst results from differing conceptual
formulations. Finally, results may be visualized with plots and networks via
matplotlib, igraph, and networkx. Overall, the pySI framework will increase the
accessibility of spatial interaction modelling while also serving as a tool
which can help new users understand the associated methodological intricacies. 

Within this presentation, the concept of spatial interaction and a few key
modelling terms will first be introduced, along with several example
applications. Next, two traditional techniques for calibrating spatial
interaction models, Poisson generalized linear regression and direct maximum
likelihood estimation will be contrasted. It will then be demonstrated how this
new framework will allow users to execute either form of calibration using
identical input variables, which are based upon a pandas DataFrame
specification, without any significant mathematical or statistical training.
Results from two different conceptual models will be compared to illustrate how
pySI can be used to explore different methods and models of spatial
interaction.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Taylor Oshan</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2789/pysi-a-python-framework-for-spatial-interaction</guid><enclosure url="http://www.youtube.com/watch?v=VokeBZarsNM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/VokeBZarsNM/hqdefault.jpg"></media:thumbnail></item><item><title>Python Beyond CPython: Adventures in Software Distribution</title><link>http://www.pyvideo.org/video/2785/python-beyond-cpython-adventures-in-software-dis</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nick Coghlan</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2785/python-beyond-cpython-adventures-in-software-dis</guid><enclosure url="http://www.youtube.com/watch?v=IVzjVqr_Bzs" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/IVzjVqr_Bzs/hqdefault.jpg"></media:thumbnail></item><item><title>Python for economists (and other social scientists!)</title><link>http://www.pyvideo.org/video/2786/python-for-economists-and-other-social-scientist</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I have developed a curriculum for a three part, graduate level course on computational methods designed to increase the exposure of graduate students and researchers to basic techniques used in computational modeling and simulation using the Python programming language.
&lt;p&gt;Description&lt;/p&gt;
Together with theory and experimentation, computational modeling and simulation has become a "third pillar" of scientific enquiry.  I am developing a curriculum for a three part, graduate level course on computational methods designed to increase the exposure of graduate students and researchers in the College of Humanities and Social Sciences at the University of Edinburgh to basic techniques used in computational modeling and simulation using the Python programming language. My course requires no prior knowledge or experience with computer programming or software development and all current and future course materials will be made freely available online via GitHub.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Pugh</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2786/python-for-economists-and-other-social-scientist</guid><enclosure url="http://www.youtube.com/watch?v=xHkGW1l5X8k" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/xHkGW1l5X8k/hqdefault.jpg"></media:thumbnail></item><item><title>Reflexive Data Science on SciPy Communities</title><link>http://www.pyvideo.org/video/2809/reflexive-data-science-on-scipy-communities</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I present tools for collecting data generated by Scientific Python community development infrastructure (mailing list archives, pull requests, issue trackers) and analyzing it with Pandas and NetworkX. Showing preliminery results using social network analysis and complex systems modeling, I demonstrate using reflexive data science to enrich our understanding of open source development.
&lt;p&gt;Description&lt;/p&gt;
### Background/Motivation

The Scientific Python community's contributions to greater scientific understanding have been underappreciated by academic institutions. One reason for this is that software engineering is widely misunderstood and not recognized as research work in its own right, as opposed to paper publication and patents. A better understanding of the open source software development process itself will help academic institutions recognize the contributions of open source developers.


### Methods

I collect historical data from development of Scientific Python projects and render these into formats suitable for analysis using SciPy tools. To demonstrate the potential of this work, I will show two ways of analyzing this data scientifically: as a self-excited Hawkes process exibiting shock behavior, and as information diffusion over a social network.

### Results

The purpose of this talk is twofold.

First, to introduce tools and techniques for turning data from open source software production into scientific data suitable for analysis.  This talk proposes that there's an opportunity for SciPy to engage in _reflexive data science_, using its own data to learn more about how it functions and how to operate more efficiently.

Second, this talk will present visualizations of the data based on complex systems research and social network analysis. Building on prior work, these results will focus on the role of productive bursts in communications. Drawing on social network analysis and prior work on roles in Usenet communities and open source communities, this talk will provide historical insight into the interaction between SciPy communities.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Sebastian Benthall</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2809/reflexive-data-science-on-scipy-communities</guid><enclosure url="http://www.youtube.com/watch?v=IL9KqJtTGw0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/IL9KqJtTGw0/hqdefault.jpg"></media:thumbnail></item><item><title>Scientific Knowledge Management with Web of Trails</title><link>http://www.pyvideo.org/video/2808/scientific-knowledge-management-with-web-of-trail</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Do you hate repeating yourself?  Want to know when your publication is repeating someone else?  The Web of Trails project is a solution to knowledge management that empowers users to quickly find repetition of key phrases.  Using syntactic indexing, as opposed to lexical techniques, this approach is capable of representing the literature using less space while providing high value results.
&lt;p&gt;Description&lt;/p&gt;
Web of Trails (WOT) is an open source project that uses context-free grammars (CFG's) as the basic building block for search. Current search technology relies upon the presence of words on a page, sometimes augmented with statistical correlations among words. Even with these restrictions, maintenance of an index requires storage much greater than the input size (a polynomial function of it). CFG's have been used for decades in compilation and language tools, and more recently in data compression.

The primary advantage of this CFG approach, based upon the Sequitur algorithm, is that it indexes content in linear-space, not polynomial-space. The secondary advantage is that combined with research in inference, grammars can express human concepts and connections rather than just correlations. This project uses grammar and syntactic analysis to replace lexical and word-based approaches to the problem of searching collections of digital artifacts. Benchmarking in web content indexing will be shown relative to popular alternatives such Apache Lucene and Amazon Cloud Search.

In addition to implementing content indexing with Sequitur, this project will enable domain-specific extensions of WOT. Once complete, we will research novel techniques for generalizing the grammars inferred by Sequitur. As this fundamental research develops, it will inform later framework development and increase search precision. This is a big leap in the state of the art, as text artifacts are no longer represented as bags of words, but as bags on non-terminals in a growing and adapting grammar.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Riehl</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2808/scientific-knowledge-management-with-web-of-trail</guid><enclosure url="http://www.youtube.com/watch?v=oEkKxHQb8iA" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/oEkKxHQb8iA/hqdefault.jpg"></media:thumbnail></item><item><title>Spatial-Temporal Prediction of Climate Change Impacts using pyimpute, scikit learn and GDAL</title><link>http://www.pyvideo.org/video/2794/spatial-temporal-prediction-of-climate-change-imp</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In this talk, I\u2019ll show how we apply climate change models to predict shifts in agricultural zones across the western US. I will outline the use of the pyimpute, GDAL and scikit-klearn to perform supervised classification; training a model using current climatic conditions to predict spatially-explicit zones under future climate scenarios.
&lt;p&gt;Description&lt;/p&gt;
As the field of climate modeling continues to mature, we must anticipate the practical implications of the climatic shifts predicted by these models. In this talk, I'll show how we apply the results of climate change models to predict shifts in agricultural zones across the western US. I will outline the use of the Geospatial Data Abstraction Library ([GDAL](http://www.gdal.org/)) and Scikit-Learn ([sklearn](http://scikit-learn.org/)) to perform supervised classification, training the model using current climatic conditions and predicting the zones as spatially-explicit raster surfaces across a range of future climate scenarios. Finally, I'll present a python module ([pyimpute](https://github.com/perrygeo/pyimpute)) which provides an API to optimize and streamline the process of spatial classification and regression problems.

#### Outline
This talk will consist of four parts:

1. A brief overview of climate data and the concept of agro-ecological zones
2. The theory and intuition behind bioclimatic envelope modeling using supervised classification
3. Visualization and interpretation of our results
4. Detailed demonstration of the pyimpute/GDAL/sklearn workflow 

    * Loading spatial data into numpy arrays 
    * Random stratified sampling
    * Training, assessing and selecting the sklearn classifier
    * Prediction of zones given future climate data as explanatory variables
    * Quantifying and interpreting uncertainty
    * Writing results to spatial data formats
    * Discussion of performance and memory limitations
    * Visualizing and interacting with the results
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Perry</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2794/spatial-temporal-prediction-of-climate-change-imp</guid><enclosure url="http://www.youtube.com/watch?v=7mmbRNE9VsA" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/7mmbRNE9VsA/hqdefault.jpg"></media:thumbnail></item><item><title>Synthesis and analysis of circuits in the IPython notebook</title><link>http://www.pyvideo.org/video/2801/synthesis-and-analysis-of-circuits-in-the-ipython</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Building on the new IPython 2.0 widget model and the jsPlumb package we create a schematic capture tool that allows graphically editing a circuit as well as its components' parameters and then instantly updating a domain specific modeling backend. This allows for an integrated circuit modeling workflow and to extend widget-based user interfaces for engineering and research projects.
&lt;p&gt;Description&lt;/p&gt;
Circuits, i.e., a network of interconnected components with ports, have found application in various scientific and engineering domains, ranging from applications close to the physical implementation, such as electrical circuits, photonic circuits for optical information processing, superconducting quantum circuits for quantum information applications to more abstract circuit representations of dynamical systems, biological processes or even software algorithms.

This has already led to the development of quite general domain-independent circuit modeling toolkits such as [Modelica](https://www.modelica.org/), but to date, there exist very few open source graphical general circuit editing environments that can be tightly integrated with custom, domain-specific implementation simulation or analysis backends as well as [IPython](http://ipython.org).

Here we present our first attempt at creating such a tool as well as some applications from our own research on nano-photonic quantum circuit models. Our existing [QNET](http://mabuchilab.github.io/QNET/) software package allows to model these circuits in a purely symbolic fashion and interfaces with various codes for numerical simulation. 

We demonstrate that the extension of our package with a visual circuit editor leads to a rich integrated simulation and analysis workflow in which an engineer or researcher can receive very fast feedback when making changes to his model.

As a consequence, it is much easier to build intuition for the particular kinds of circuit models and find novel and creative solutions to an engineering task.

Finally, given the broad range of applications for circuit models and representations, we outline how our visual circuit editor can be adapted to export a circuit for interfacing with other domain specific software such as Modelica.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nikolas Tezak</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2801/synthesis-and-analysis-of-circuits-in-the-ipython</guid><enclosure url="http://www.youtube.com/watch?v=-kUzWdKOgqc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/-kUzWdKOgqc/hqdefault.jpg"></media:thumbnail></item><item><title>The PlaceIQ Location Based Analytic Platform</title><link>http://www.pyvideo.org/video/2791/the-placeiq-location-based-analytic-platform</link><description>&lt;p&gt;Abstract&lt;/p&gt;
PlaceIQ's patented platform analyzes half a trillion diverse data points about location, time, and real-world behavior to define human audiences and allow businesses to understand consumers at scale. It ingests large volumes of mobile activity data and geographic data, calling for creative use machine learning techniques to enable the high-fidelity abstractions insightful to businesses.
&lt;p&gt;Description&lt;/p&gt;
The surge in mobile device adoption and the subsequent abundance of time-stamped location data have given rise to possibilities and interest in understanding movement-based human behavior. The PlaceIQ analytic platform is a large-scale data analysis system that addresses this demand, providing a large-scale, flexible, and reliable platform created around the concepts of location and audience. The platform's data processing piece ingests large volumes of mobile activity data daily and overlays them onto geospatial data. These data include: the discretization of the U.S. into 1 billion 100 meter by 100 m tiles; more than 400,000 proprietary polygons delineating the shapes of properties and businesses; business listings and census data; and terabytes of mobile activity data. We discuss here the methodologies - namely DBSCAN clustering and kd-trees - used to de-dupe disparate geodata sources and evaluate the quality of noisy activity data at scale. The resulting overlays of people, places, and time create high-fidelity abstractions and manipulations insightful to businesses, particularly in the mobile advertising domain.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eliza Chang</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2791/the-placeiq-location-based-analytic-platform</guid><enclosure url="http://www.youtube.com/watch?v=ygI1LP_jD9A" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/ygI1LP_jD9A/hqdefault.jpg"></media:thumbnail></item><item><title>TracPy: Wrapping the FORTRAN Lagrangian trajectory model TRACMASS</title><link>http://www.pyvideo.org/video/2805/tracpy-wrapping-the-fortran-lagrangian-trajector</link><description>&lt;p&gt;Abstract&lt;/p&gt;
An example of a Python wrapper of a FORTRAN code, applications of a Lagrangian trajectory model, and lessons learned about code development.
&lt;p&gt;Description&lt;/p&gt;
Numerical Lagrangian tracking is a way to follow parcels of fluid as they are advected by a numerical circulation model. This is a natural method to investigate transport in a system and understand the physics on the wide range of length scales that are actually experienced by a drifter. TRACMASS is a tool for Lagrangian trajectory modeling that has been developed over the past two decades. It has been used to better understand physics and its applications to real-world problems in many areas around the world, in both atmospheric and oceanic settings. TRACMASS is written in FORTRAN, which is great for speed but not as great for ease of use. This code has been wrapped in Python to run batches of simulations and improve accessibility --- and dubbed TracPy. 

In this talk, I will outline some of the interesting features of the TRACMASS algorithm and several applications, then discuss the layout of the TracPy code. The code setup and organization have been a learning process and I will also share some of my hard-earned lessons.

TracPy is continually in development and is available [on GitHub](https://github.com/kthyng/tracpy).
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kristen Thyng</dc:creator><pubDate>Sun, 13 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2805/tracpy-wrapping-the-fortran-lagrangian-trajector</guid><enclosure url="http://www.youtube.com/watch?v=8poLWacun50" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/8poLWacun50/hqdefault.jpg"></media:thumbnail></item><item><title>Advanced 3D Seismic Visualizations in Python</title><link>http://www.pyvideo.org/video/2821/advanced-3d-seismic-visualizations-in-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
3D reflection seismic data collected as a part of the NanTroSEIZE project revealed complex interactions between active sedimentation and tectonics in the Nankai Trough, Japan. We implemented co-rendering of multiple attributes and stratal slicing in python to better visualize the structural and stratigraphic relationships within the piggyback slope basins of the accretionary prism.
&lt;p&gt;Description&lt;/p&gt;
3D reflection seismic data acquired [offshore of southeast Japan](http://www.geology.wisc.edu/~jkington/scipy2014/LocationMap.png "Location Map") as part of the Nankai Trough Seismogenic Zone Experiment  (NanTroSEIZE) provides a unique opportunity to study active accretionary prism processes.  The 3D seismic volume revealed complex interactions between active sedimentation and tectonics within [multiple slope basins](http://www.geology.wisc.edu/~jkington/scipy2014/inline_2695_w_interp_brown_seismic.png "Cross section through entire accretionary prism illustrating multiple isolated sedimentary basins") above the accretionary prism. However, our ability to understand these interactions was hindered without access to expensive specialized software packages. 

We implemented stratal slicing of the 3D volume and co-rendering of multiple attributes in python to better visualize our results.  Stratal slicing allows volumetric attributes to be displayed [in map view along an arbitrary geologic timeline](http://www.geology.wisc.edu/~jkington/scipy2014/stratal_slicing_animation.gif "Stratal Slicing Animation")(~30MB animated gif) by interpolating between interpreted geologic surfaces.  This enhances the visibility of subtle changes in stratigraphic architecture through time. Co-rendering coherence on top of seismic amplitudes facilitates fault interpretation in both cross section and map view.  This technique allowed us to [confidently interpret faults](http://www.geology.wisc.edu/~jkington/scipy2014/ContemporaneousStrikeSlipAndNormalFaults.png "Corendering attributes for fault interpretation") near the limit of seismic resolution.  

The scientific python ecosystem proved to be an effective platform both for making publication-quality cross sections and for rapidly implementing state-of-the-art seismic visualization techniques. We created [publication quality cross sections](http://www.geology.wisc.edu/~jkington/scipy2014/Basin_uplift.png "Example cross section") (some annotations added in Inkscape) and interactive 2D visualizations in ``matplotlib``.  For 3D display of seismic volumes we used ``mayavi`` to easily create interactive scenes. ``scipy.ndimage`` provided most of the underlying image processing capability and allowed us to preform memory-efficient operations on &gt;10GB arrays.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joe Kington</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2821/advanced-3d-seismic-visualizations-in-python</guid><enclosure url="http://www.youtube.com/watch?v=PFOd01fqcyQ" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/PFOd01fqcyQ/hqdefault.jpg"></media:thumbnail></item><item><title>Behind the Scenes of the University and Supplier Relationship</title><link>http://www.pyvideo.org/video/2813/behind-the-scenes-of-the-university-and-supplier</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The University of California, Berkeley and San Francisco combined are one of the largest buyers in the Bay Area. Historically, it has been a time-consuming process to analyze suppliers' proposed price files and ensure the University is not paying more than contracted. Through the use of Pandas and Python, this once tedious and manual process can routinely be done in a matter of a few seconds.

&lt;p&gt;Description&lt;/p&gt;
For the University of California, Berkeley and San Francisco, a routine management process of supplier price files used to be a time-consuming process. It is essential to analyze the sometimes tens of thousands of items a supplier offers to make sure the University doesn't accept larger price increases than is in compliance with a contract. A historical figure of past purchases is matched against the current and proposed catalogs and then analyzed to ultimately find out the percentage increase and number of products removed. Each Universities' motivation is to not accept a file that has larger price increases than contracted nor a file with several previously purchased products removed. 

To combat the tedious and time-consuming process of manually analyzing the previous spend with the current and proposed files, a Python script was written. This heavily uses Pandas as well as Numpy for computations. The code uploads all three files as a dataframes and creates a common variable to compare similar products. It matches what was previously purchased to the identical products in the current and proposed catalogs. After filtering any 'bad' input that would skew the results, several values are computed and the code outputs the necessary figures to determine if a supplier's price file is acceptable. The code even documents each catalog result automatically so the historical changes are organized and noted in a csv.  

This code is an exponential improvement to the manual process that was historically done. The end numbers are known in a matter of seconds as opposed to hours of Excel or Access analysis. For some suppliers, Excel is even incapable of uploading the entire catalog, thus making any analysis nearly impossible. Python and Pandas have not only made the analysts time more efficient but have opened the door for several possibilities.

Although this code has greatly improved this continuous analysis, more advanced techniques could potentially improve the process. The department soon hopes to use a forecasted spend figure rather than a historical snapshot to project spend against the proposed catalog. Moving forward, having the analysts armed with Python knowledge, Strategic Sourcing hopes to yield more meaning through the daily flow of spend data through machine learning techniques.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexis Perez</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2813/behind-the-scenes-of-the-university-and-supplier</guid><enclosure url="http://www.youtube.com/watch?v=dIpLLwIVCYY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/dIpLLwIVCYY/hqdefault.jpg"></media:thumbnail></item><item><title>Blaze: Building a Foundation for Array-Oriented Computing in Python</title><link>http://www.pyvideo.org/video/2811/blaze-building-a-foundation-for-array-oriented-c</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The Blaze project is a collection of libraries being built towards the goal of generalizing NumPy's data model and working on distributed data. This talk covers each of these libraries, and how they work together to accomplish this goal.
&lt;p&gt;Description&lt;/p&gt;
Python's scientific computing and data analysis ecosystem, built around NumPy, SciPy, Matplotlib, Pandas, and a host of other libraries, is a tremendous success. NumPy provides an array object, the array-oriented ufunc primitive, and standard practices for exposing and writing numerical libraries to Python all of which have assisted in making it a solid foundation for the community. Over time, however, it has become clear that there are some limitations of NumPy that are difficult to address via evolution from within. Notably, the way NumPy arrays are restricted to data with regularly strided memory structure on a single machine is not easy to change.

Blaze is a project being built with the goal of addressing these limitations, and becoming a foundation to grow Python's success in array-oriented computing long into the future. It consists of a small collection of libraries being built to generalize NumPy's notions of array, dtype, and ufuncs to be more extensible, and to represent data and computation that is distributed or does not fit in main memory.

Datashape is the array type system that describes the structure of data, including a specification of a grammar and set of basic types, and a library for working with them. LibDyND is an in-memory array programming library, written in C++ and exposed to Python to provide the local representation of memory supporting the datashape array types. BLZ is a chunked column-oriented persistence storage format for storing Blaze data, well-suited for out of core computations. Finally, the Blaze library ties these components together with a deferred execution graph and execution engine, which can analyze desired computations together with the location and size of input data, and carry out an execution plan in memory, out of core, or in a distributed fashion as is needed.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Wiebe,Matthew Rocklin</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2811/blaze-building-a-foundation-for-array-oriented-c</guid><enclosure url="http://www.youtube.com/watch?v=9HPR-1PdZUk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/9HPR-1PdZUk/hqdefault.jpg"></media:thumbnail></item><item><title>Building petabyte-scale comparative genomics pipelines</title><link>http://www.pyvideo.org/video/2818/building-petabyte-scale-comparative-genomics-pipe</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This talk will educate the audience about Python tools and best practices for creating reproducible petabyte-scale pipelines. This is done within the context of demonstrating a new grammar-based approach to comparative genomics. The genome grammars are produced using public data from the National Institutes of Health, streamed over a high-throughput Internet2 connection to Amazon Web Services.
&lt;p&gt;Description&lt;/p&gt;
We introduce a high-performance, open-source application written in Python that models genomic data with a context-free grammar (CFG), a construct from formal language theory. This approach is intended to advance fundamental science by delivering a more extensive model of the genetic interaction of diseases. Current comparative models treat genomic sequences as strings, and recent advances are little more than optimizations of the \"grep approach\". However a genome is a grammar: it is parsed, follows rules, and has an inherent hierarchical structure. Understanding the structure and rules of this implied grammar are essential for mapping loci to diseases when those loci are distributed across genomic regions.

To produce the CFGs, we have implemented the Sequitur algorithm to run on the AWS Elastic MapReduce platform. This application is written in Python and uses the following packages: MRjob, boto, and pandas. This is a petascale computing pipeline that is successful because it uses inherently scalable services and is able to take advantage of the 100G Internet2 connection between Amazon Web Services and the National Institutes of Health (NIH). This architecture delivers unprecedented transfer speeds and relatively low latency.

We discuss the advantages of this architecture, especially for groups without comparable local resources. In reviewing the results of our computation, we not only look at methods to measure the utility of our CFG models, but also the computational advantages of this approach. Just like the fastest alignment algorithms, this complex approach still operates  within linear-space. In addition, future pairwise comparisons are faster because our CFGs act as a compressed representation of the raw sequence data. Our hope is that this CFG approach is further tested as a replacement for raw sequence analysis. In addition, we hope that our  bioinformatics pipeline serves as an example for the SciPy community on how to perform large computations across the many petabytes made available by NIH.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Cope</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2818/building-petabyte-scale-comparative-genomics-pipe</guid><enclosure url="http://www.youtube.com/watch?v=g_QVAp1YkRI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/g_QVAp1YkRI/hqdefault.jpg"></media:thumbnail></item><item><title>Clustering of high content images to discover off target phenotypes</title><link>http://www.pyvideo.org/video/2817/clustering-of-high-content-images-to-discover-off</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In high content imaging screens, cells are subjected to various treatments (usually shutting down specific genes) in high throughput, imaged, and a phenotype of interest measured. We argue that there is a wealth of information to be found in off-target phenotypes, and present an image clustering approach to discover these and infer gene function.
&lt;p&gt;Description&lt;/p&gt;
In the decade between 1999 and 2008, more newly-approved, first-in-class drugs were found by phenotypic screens than by molecular target-based approaches. This is despite far more resources being invested in the latter, and highlights the rising importance of screens in biomedical research. ([Swinney and Anthony, Nat Rev Drug Discov, 2011](http://www.nature.com/nrd/journal/v10/n7/full/nrd3480.html))

Despite this success, the data from phenotypic screens is vastly underutilized. A typical analysis takes millions of images, obtained at a cost of, say, $250,000, and reduces each to a single number, a quantification of the phenotype of interest. The images are then ranked by that value and the top-ranked images are flagged for further investigation. ([Zanella et al, Trends Biotech, 2010](https://www.cell.com/trends/biotechnology/abstract/S0167-7799(10)00035-1))

The images, however, contain a lot more information than just a single phenotypic number. For one, usually only the mean phenotype of all the cells in the image is reported, with no information about variability, even though the distribution of cell shapes in a single image is highly informative ([Yin et al, Nat Cell Biol, 2013](http://www.nature.com/ncb/journal/v15/n7/full/ncb2764.html)). Additionally, cells display a variety of off-target phenotypes, independently of the target, that can provide biological insight and new research avenues.

We are developing an unsupervised clustering pipeline, tentatively named high-content-screen unsupervised sample clustering ([HUSC](http://github.com/jni/husc)), that leverages the scientific Python stack, particularly `scipy.stats`, `pandas`, `scikit-image`, and `scikit-learn`, to summarize images with feature vectors, cluster them, and infer the functions of genes corresponding to each cluster. The library includes functions for preprocessing images, computing an array of features designed specifically for microscopy images, and accessing a MongoDB database containing sample data. Its API allows easy extensibility by placing screen-specific functions under the `screens` sub-package. An example IPython notebook with a preliminary analysis can be found [here](http://jni.github.io/notebooks/hcs_nb.html).

We plan to use this library to develop a flexible web interface for flexible and extensible analysis of high-content screens, and relish the opportunity to enlist the help and expertise of the SciPy crowd.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2817/clustering-of-high-content-images-to-discover-off</guid><enclosure url="http://www.youtube.com/watch?v=fn8F_NerTug" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/fn8F_NerTug/hqdefault.jpg"></media:thumbnail></item><item><title>Lightning Talks | SciPy 2014 | July 9, 2014</title><link>http://www.pyvideo.org/video/2814/lightning-talks-scipy-2014-july-9-2014</link><description></description><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2814/lightning-talks-scipy-2014-july-9-2014</guid><enclosure url="http://www.youtube.com/watch?v=SMyto7WHiNs" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/SMyto7WHiNs/hqdefault.jpg"></media:thumbnail></item><item><title>scikit-bio: core bioinformatics data structures and algorithms in Python</title><link>http://www.pyvideo.org/video/2815/scikit-bio-core-bioinformatics-data-structures-a</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We present scikit-bio, a library based on the Python scientific computing stack implementing core bioinformatics data structures, algorithms and parsers. scikit-bio is useful for students in bioinformatics, who can learn topics such as iterative progressive multiple sequence alignment from the source code and accompanying documentation, and for real-world bioinformatics applications developers.
&lt;p&gt;Description&lt;/p&gt;
Python is widely used in computational biology, with many high profile bioinformatics software projects, such as [Galaxy](http://galaxyproject.org/), [Khmer](http://khmer.readthedocs.org/en/latest/) and [QIIME](http://www.qiime.org), being largely or entirely written in Python. We present [scikit-bio](http://www.scikit-bio.org), a new library based on the standard Python scientific computing stack (e.g., numpy, scipy, and matplotlib) implementing core bioinformatics data structures, algorithms, parsers, and formatters. scikit-bio is the first bioinformatics-centric [scikit](https://scikits.appspot.com/), and arises from over ten years of development efforts on [PyCogent](http://www.pycogent.org) and [QIIME](http://www.qiime.org), representing an effort to update the functionality provided by these extensively used tools, and to make that functionality more accessible. scikit-bio is intended to be useful both as a resource for students, who can learn topics such as heuristic-based sequence database searching or iterative progressive multiple sequence alignment from the source code and accompanying documentation, and as a powerful library for 'real-world' bioinformatics developers. To achieve these goals, scikit-bio development is centered around test-driven, peer-reviewed software development; C/Cython integration for computationally expensive algorithms; extensive API documentation and doc-testing based on the [numpy docstring standards](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt); user documentation and [theoretical discussion of topics in IPython Notebooks](http://caporasolab.us/An-Introduction-To-Applied-Bioinformatics/); adherence to PEP8; and continuous integration testing. scikit-bio is available free of charge under the BSD license.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">J Gregory Caporaso</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2815/scikit-bio-core-bioinformatics-data-structures-a</guid><enclosure url="http://www.youtube.com/watch?v=hgBx_DBiPxA" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/hgBx_DBiPxA/hqdefault.jpg"></media:thumbnail></item><item><title>Software Carpentry: Lessons Learned</title><link>http://www.pyvideo.org/video/2812/software-carpentry-lessons-learned-0</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Greg Wilson</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2812/software-carpentry-lessons-learned-0</guid><enclosure url="http://www.youtube.com/watch?v=1e26rp6qPbA" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/1e26rp6qPbA/hqdefault.jpg"></media:thumbnail></item><item><title>Teaching Python to undergraduate students</title><link>http://www.pyvideo.org/video/2819/teaching-python-to-undergraduate-students</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Teaching undergraduate students in programming is interesting and challenging at the same time, because one has to deal mostly with two types: Those who have already experience and those who have not. I will present two models from Bonn University for Physics students with now much more responsibility for the tutors and would like to initiate discussions about different systems all over the world.
&lt;p&gt;Description&lt;/p&gt;
Teaching undergraduate students in programming languages like Python is interesting and challenging at the same time. You have to deal mostly with two types of students: Those who have already some experience in programming (not neccessarily Python), e.g. from high school, and those who have not. At Bonn University we have recently changed the structure of such a course for Bachelor of Science in Physics students. First, we include the Python tutorial in a lecture in the first term instead of a voluntary course in the lecture-free time before the fourth semester, where the "Numerical Methods for Physicists" course takes place. Second, instead of a weekly lecture, in which the topics are explained in detail, and a 2 hours exercise class, the new system provides only one introduction lecture per topic but a 3 hours exercise class per week. So especially the tutors are much more responsible for the success of the students in the final report of this course. Furthermore, as a student representative and also tutor for both courses, I have been heavily involved in this process.

I like to initiate a larger discussion about how to teach programming to undergraduate students, especially because in the last decades programming got more and more important in science and due to e.g. the Bologna reform in Europe, it should be easier to change between universities after e.g. the Bachelor program.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dominik Klaes</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2819/teaching-python-to-undergraduate-students</guid><enclosure url="http://www.youtube.com/watch?v=B_WoteRvK2I" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/B_WoteRvK2I/hqdefault.jpg"></media:thumbnail></item><item><title>The Berkeley Institute for Data Science a place for people like us</title><link>http://www.pyvideo.org/video/2820/the-berkeley-institute-for-data-science-a-place-f</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I will describe the new Berkeley Institute for Data Science (BIDS), part of a collaboration with UW and NYU funded by the Moore and Sloan Foundations. It will be a space for the open and interdisciplinary work that is typical of the SciPy community.  In the creation of BIDS, the role of open source scientific tools for Data Science, and specifically the SciPy ecosystem, played an important role.
&lt;p&gt;Description&lt;/p&gt;
In 2013, the Gordon and Betty Moore and the Alfred P. Sloan foundations [awarded](http://www.moore.org/programs/science/data-driven-discovery/data-science-environments) UC Berkeley, U. Washington and NYU for a collaborative, $38M in support of a 5-year initiative to create novel environments for Data Science.  This project was driven by the recognition that computing and data analysis have now become the backbone of all scientific research, and yet the teams, collaborations and individuals that make this possible typically encounter significant barriers in today's academic environments.

The SciPy community is one of the poster children of this issue: many of our members live "officially" in traditional, discipline-oriented scientific research, and yet we have committed time and effort to creating an open ecosystem of tools for research.  As we all know, this is often done with little support from the standard incentive structures of science, be it publication venues, funding agencies or hiring, tenure and promotion committees.

The launch of this initiative is an important moment, as it signals the recognition of this problem by important and well-respected foundations in science.  At UC Berkeley, we took this opportunity to create the new [Berkeley Institute for Data Science](http://vcresearch.berkeley.edu/datascience/bids-launch-dec-12). In this effort, the open source tools of the SciPy community will play a central role.

In this talk, I will describe the larger context in which this initiative has been created, as well as the scientific scope of our team, our goals, and the opportunities that we will try to provide with this space.  We expect that this new institute, together with our partners at UW and NYU, will play an important role in support of the great work of the SciPy ecosystem.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2820/the-berkeley-institute-for-data-science-a-place-f</guid><enclosure url="http://www.youtube.com/watch?v=q5yAy4WWTyU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/q5yAy4WWTyU/hqdefault.jpg"></media:thumbnail></item><item><title>The Road to Modelr: Building a Commercial Web Application on an Open Source Foundation</title><link>http://www.pyvideo.org/video/2816/the-road-to-modelr-building-a-commercial-web-app</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Lessons learned along the bumpy road from Python noob to an open source geophysics web application, with a commercial web service front end.
&lt;p&gt;Description&lt;/p&gt;
Software for applied geoscientists in the petroleum industry is usually expensive, hard to use, Windows or Linux only, and slow to evolve. Furthermore, it is almost always stridently proprietary and therefore black-box. Open source software is rare. There are few developers working outside of seismic processing and enterprise database development, and consequently there is very little in the web and mobile domain. Reconciling a commitment to open source with a desire to earn a good living is one of the great conundrums of software engineering. We have chosen a hybrid approach of open core (like OpendTect, which has proprietary add-ons) and software-as-a-service (like WordPress.org vs WordPress.com). 

Open source back-end
--------------------

Our open core is a Python web app for producing synthetic seismic models, in much the same way that the now-deprecated [Google Image Charts API](https://developers.google.com/chart/image/) used to work: the user provides a URL, which contains all the relevant data, and a JPEG image generated by matplotlib is returned. Along with the image, we return some computed data about the model, such as the elastic properties of the rocks involved. The mode of the tool is described by "scripts", which for now reside on the server, but which we plan to allow users to provide as part of the API. Scripts have various parameters, such as the P-wave and S-wave velocities, and the bulk density of the rocks in the model, and it is these parameters that make up most of the data in the API call. Other parameters include the type and frequency of wavelet to use, and the computation method for the reflectivity (for example the Zoeppritz equations, or the Aki\u2013Richards approximation). The app has no user interface to speak of, only a web API. It is licensed under the Apache 2 license and can be found [on GitHub](https://github.com/agile-geoscience/modelr). We are running an instance of our app on a "T1.micro" [Amazon EC2 instance](http://aws.amazon.com/ec2/) running Ubuntu.

Proprietary front-end
---------------------

The commercial, proprietary front end is a Python web app that lives in the [Google App Engine](https://developers.google.com/appengine/) walled garden. This app, which uses the [Twitter Bootstrap framework](http://getbootstrap.com/), is serving at [modelr.io](https://www.modelr.io/) and provides a user object in which a geoscientist can save rocks and scenarios  consisting of a script and all its parameters. We chose App Engine for its strong infrastructure, good track record, and the easy availability of tools like the datastore, memcache, login, and so on. We also host support channels and materials through this front end, which has a very lightweight "demo" mode, and otherwise requires a $9/month subscription to use, handled by [Stripe](https://stripe.com/ca). This necessitated serving both the front and back ends over HTTPS, something we wanted to do anyway, because of industry mistrust of the cloud.

Summary
-------
Some of the things we picked up along the way:

* We started with a strong need of our own, so had clear milestones from day 1.
* We left the project alone for months, but good documentation and GitHub meant this was not a problem.
* Sprinting with a professional developer at the start meant less thrashing later.
* The cloud landscape is exciting, but it's easy to be distracted by all the APIs. Keeping it simple is a constant struggle.
* Pushing through Xeno's paradox to get to a live, public-facing app took stamina and focus.
* There's nothing like having other users to get you to up your coding game.

We hope that by telling this story of the early days of a commercial scientific web application, built by a bunch of consulting scientists in Nova Scotia, not a tech startup in San Francisco, we can speed others along the path to creating a rich ecosystem of new geoscience tools and web APIs.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matt Hall</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2816/the-road-to-modelr-building-a-commercial-web-app</guid><enclosure url="http://www.youtube.com/watch?v=TgOk_3JEcwY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/TgOk_3JEcwY/hqdefault.jpg"></media:thumbnail></item><item><title>Zero Dependency Python</title><link>http://www.pyvideo.org/video/2810/zero-dependency-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We present a new method for distributing and using Python that requires no dependencies beyond the Google Chrome web browser based on Portable Native Client (PNaCl).  We will demonstrate an IPython notebook run completely client side with no out-of-browser components, backed by Google Drive, an HTML5 File System, and able to pass numpy arrays as typed arrays without serialization as JSON.
&lt;p&gt;Description&lt;/p&gt;
We present a new method for distributing and using Python that requires no
dependencies beyond the Google Chrome web browser.  By combining the static
linking methodology of traditional supercomputer-style deployments of Python
with the technology Portable Native Client (PNaCl) we have constructed a method
for building, deploying, and sharing fully-sandboxed scientific python stacks
that require no client-side installation: the entire IPython notebook and
scientific python stack, in a website, at native speeds.  We will present this
technology, along with some of its potential applications, describing its
shortcomings and future extensibility.  We will conclude by demonstrating an
IPython notebook run completely client side with no out-of-browser components,
backed by Google Drive and an HTML5 File System, and able to pass numpy arrays
as typed arrays into the browser without serialization as JSON.

1. We will begin by briefly describing the problems with deploying scientific python
as a stack, particularly the dependency graph, installation time, and so
on.

2. We'll describe the PNaCl technology and build system for
scientific python, including how individuals can create their own .pexes with
their own application stack

3. We'll describe potential applications,
such as bundling safe, sandboxed executables with scripts and lessons

4. We will demonstrate a complete system for running the IPython notebook in a
sandboxed, Google Chrome window

5. We'll conclude by describing methods that this system could be extended to run sandboxed python executables on any
system, independent of the Chrome web browser, such as supercomputers and
non-virtualized hosting providers
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kester Tong,Matthew Turk</dc:creator><pubDate>Thu, 10 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2810/zero-dependency-python</guid><enclosure url="http://www.youtube.com/watch?v=bITvUUyvUAY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/bITvUUyvUAY/hqdefault.jpg"></media:thumbnail></item><item><title>A Common Scientific Compute Environment for Research and Education</title><link>http://www.pyvideo.org/video/2718/a-common-scientific-compute-environment-for-resea</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I provide an overview of the challenges weve tackled at UC Berkeley deploying scientific compute environments in both educational and research contexts. After a discussion of how these needs can be served by devops tools like Docker and Ansible, I argue that a coherent, easy-to-understand philosophy around reproducible compute environments is fundamental.

&lt;p&gt;Description&lt;/p&gt;
As the line between developer and researcher becomes ever more blurred, the challenge of sharing your compute environment with students and colleagues becomes ever more complex. Large, private organizations have been grappling with this issue for a while, spawning a great deal of enthusiasm around tools like Docker, Puppet, Vagrant, and Packer. And lets not forget notable python-based upstarts, Ansible and Salt! These tools can generate immense enthusiasm, followed by the question, Why are we doing this?

The problem is that researcher / developers can become overwhelmed by the complexity and variety inherent in devops tools - all the while losing sight of the real reason for using these tools: a philosophy of documenting your research compute environments in a reproducible fashion, with a focus on scripting as much as is reasonable.

At UC Berkeley, members of the D-Lab, the Statistical Compute Facility, Computer Science and Research IT have organized a project to develop the Berkeley Common Environment (BCE). Ill provide an overview of the challenges weve tackled in both educational and research contexts, and the needs served by the above-mentioned devops tools. In the end, I argue that a coherent, easy-to-understand philosophy around scientific compute environments is fundamental - the tools are just a way to make your collaboration architecture a little easier for the people building these environments a few times a year. What we should focus on, though, is end-user experience and research community buy-in.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dav Clark</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2718/a-common-scientific-compute-environment-for-resea</guid><enclosure url="http://www.youtube.com/watch?v=e7jaZ5SFvFk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/e7jaZ5SFvFk/hqdefault.jpg"></media:thumbnail></item><item><title>Airspeed Velocity: Tracking Performance of Python Projects Over Their Lifetime</title><link>http://www.pyvideo.org/video/2732/airspeed-velocity-tracking-performance-of-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Presenting "airspeed velocity", a new tool for benchmarking Python software projects over their lifetime.
&lt;p&gt;Description&lt;/p&gt;
As software projects mature and become more robust against bugs, they may also lose some of their runtime performance and memory efficiency.  Airspeed velocity (asv) is a new tool to help find those performance degradations before they get out to end users.  It automatically runs a benchmark suite over a range of commits in a project's repository, as well as in a matrix of configurations of Python versions and other dependencies.  The results, possibly from multiple machines, are then collated and published in a web-based report.

While filling a similar role as projects such as "codespeed" and "vbench", airspeed velocity is designed to be easier to set up and deploy, since it uses only a DVCS repository as its database and the report is deployable to any static web server.

Airspeed velocity provides an easy way to write benchmarks, inspired by "nosetests" and "py.test".  It is possible to benchmark runtime, memory usage, or any user-defined metric.

Other features either implemented or in the planning stages include:

1. tight integration with existing profiling tools, such as RunSnakeRun
2. parameterized benchmarks to investigate how an algorithm scales with data size
3. automatic search for degrading commits

The presentation will provide a demo of airspeed velocity, and discuss its early usage for benchmarking the astropy project.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael Droettboom</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2732/airspeed-velocity-tracking-performance-of-python</guid><enclosure url="http://www.youtube.com/watch?v=OsxJ5O6h8s0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/OsxJ5O6h8s0/hqdefault.jpg"></media:thumbnail></item><item><title>Anatomy of Matplotlib - Part 1</title><link>http://www.pyvideo.org/video/2757/anatomy-of-matplotlib-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will be the introduction to matplotlib. Users will learn the types of plots and experiment with them. Then the fundamental concepts and terminologies of matplotlib are introduced. Next, we will learn how to change the "look and feel" of their plots. Finally, users will be introduced to other toolkits that extends matplotlib.
&lt;p&gt;Description&lt;/p&gt;
Introduction
============

Purpose of matplotlib
---------------------

Online Documentation
--------------------
### [matplotlib.org](http://matplotlib.org "")
### Mailing Lists and StackOverflow
### Github Repository
### Bug Reports &amp; Feature Requests

What is this "backend" thing I keep hearing about?
==================================================
### Interactive versus non-interactive
### Agg
### Tk, Qt, GTK, MacOSX, Wx, Cairo

Plotting Functions
==================
### Graphs (plot, scatter, bar, stem, etc.)
### Images (imshow, pcolor, pcolormesh, contour[f], etc.)
### Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)

What goes in a Figure?
======================
### Axes
### Axis
### ticks (and ticklines and ticklabels) (both major &amp; minor)
### axis labels
### axes title
### figure suptitle
### axis spines
### colorbars (and the oddities thereof)
### axis scale
### axis gridlines
### legend

Manipulating the "Look-and-Feel"
================================

### Introducing matplotlibrc
### Properties
- color (and edgecolor, linecolor, facecolor, etc...)
- linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())
- linestyle
- fonts
- zorder
- visible

What are toolkits?
==================
### axes_grid1
### mplot3d
### basemap
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2757/anatomy-of-matplotlib-part-1</guid><enclosure url="http://www.youtube.com/watch?v=A2adyFMsut0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/A2adyFMsut0/hqdefault.jpg"></media:thumbnail></item><item><title>Anatomy of Matplotlib - Part 2</title><link>http://www.pyvideo.org/video/2756/anatomy-of-matplotlib-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will be the introduction to matplotlib. Users will learn the types of plots and experiment with them. Then the fundamental concepts and terminologies of matplotlib are introduced. Next, we will learn how to change the "look and feel" of their plots. Finally, users will be introduced to other toolkits that extends matplotlib.
&lt;p&gt;Description&lt;/p&gt;
Introduction
============

Purpose of matplotlib
---------------------

Online Documentation
--------------------
### [matplotlib.org](http://matplotlib.org "")
### Mailing Lists and StackOverflow
### Github Repository
### Bug Reports &amp; Feature Requests

What is this "backend" thing I keep hearing about?
==================================================
### Interactive versus non-interactive
### Agg
### Tk, Qt, GTK, MacOSX, Wx, Cairo

Plotting Functions
==================
### Graphs (plot, scatter, bar, stem, etc.)
### Images (imshow, pcolor, pcolormesh, contour[f], etc.)
### Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)

What goes in a Figure?
======================
### Axes
### Axis
### ticks (and ticklines and ticklabels) (both major &amp; minor)
### axis labels
### axes title
### figure suptitle
### axis spines
### colorbars (and the oddities thereof)
### axis scale
### axis gridlines
### legend

Manipulating the "Look-and-Feel"
================================

### Introducing matplotlibrc
### Properties
- color (and edgecolor, linecolor, facecolor, etc...)
- linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())
- linestyle
- fonts
- zorder
- visible

What are toolkits?
==================
### axes_grid1
### mplot3d
### basemap
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2756/anatomy-of-matplotlib-part-2</guid><enclosure url="http://www.youtube.com/watch?v=YXdaLmzYEKU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/YXdaLmzYEKU/hqdefault.jpg"></media:thumbnail></item><item><title>Anatomy of Matplotlib - Part 3</title><link>http://www.pyvideo.org/video/2755/anatomy-of-matplotlib-part-3</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will be the introduction to matplotlib. Users will learn the types of plots and experiment with them. Then the fundamental concepts and terminologies of matplotlib are introduced. Next, we will learn how to change the "look and feel" of their plots. Finally, users will be introduced to other toolkits that extends matplotlib.
&lt;p&gt;Description&lt;/p&gt;
Introduction
============

Purpose of matplotlib
---------------------

Online Documentation
--------------------
### [matplotlib.org](http://matplotlib.org "")
### Mailing Lists and StackOverflow
### Github Repository
### Bug Reports &amp; Feature Requests

What is this "backend" thing I keep hearing about?
==================================================
### Interactive versus non-interactive
### Agg
### Tk, Qt, GTK, MacOSX, Wx, Cairo

Plotting Functions
==================
### Graphs (plot, scatter, bar, stem, etc.)
### Images (imshow, pcolor, pcolormesh, contour[f], etc.)
### Lesser Knowns: (pie, acorr, hexbin, streamplot, etc.)

What goes in a Figure?
======================
### Axes
### Axis
### ticks (and ticklines and ticklabels) (both major &amp; minor)
### axis labels
### axes title
### figure suptitle
### axis spines
### colorbars (and the oddities thereof)
### axis scale
### axis gridlines
### legend

Manipulating the "Look-and-Feel"
================================

### Introducing matplotlibrc
### Properties
- color (and edgecolor, linecolor, facecolor, etc...)
- linewidth and edgewidth and markeredgewidth (and the oddity that happens in errorbar())
- linestyle
- fonts
- zorder
- visible

What are toolkits?
==================
### axes_grid1
### mplot3d
### basemap
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Root</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2755/anatomy-of-matplotlib-part-3</guid><enclosure url="http://www.youtube.com/watch?v=QFIfLiw7gAI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/QFIfLiw7gAI/hqdefault.jpg"></media:thumbnail></item><item><title>Astropy and astronomical tools Part I</title><link>http://www.pyvideo.org/video/2739/astropy-and-astronomical-tools-part-i</link><description>&lt;p&gt;Abstract&lt;/p&gt;
The introductory session will start with an overview of the astropy project and the goals of the tutorial, followed by the basics on accessing astronomical data and the associated attributes of such data, including the units and coordinates. Also covered is how to use the new and powerful quantities facility, which allows physical quantities to be explicitly bound to the units they are defined in.
&lt;p&gt;Description&lt;/p&gt;
**Outline**

* Overview of astropy (15 minutes) [Greenfield]
    - Exercise: Import astropy, demonstrate that tools are present and echo simple examples given. (15 minutes: allowing for typical start-up problems)
* Units/quantities (15 minutes) [Droettboom]
    - Exercise: Solve problems using standard units; define new unit; use unit equivalencies; define blackbody function using quantities/units (15 minutes)
* Tables (20 minutes) [Aldcroft]
    - Exercise: read in provided table files and apply requested table manipulations (20 minutes)
* Break (15 minutes)
* Accessing and updating data
    - FITS (30 minutes) [Bray]
        + Exercise: Open supplied data files; manipulate header information; manipulate data; write results; update and append to existing file (30 minutes)
    - ascii tables (15 minutes) [Aldcroft]
        + Exercise: Open supplied ascii files, modify and convert into csv files (15 minutes) 
* coordinates (sky/time) (15 minutes) [Robitaille]
    - Exercises: solve coordinate/time conversion problems; read in various string representations for coordinates/times; print alternate string representations (15 minutes)

</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Erik Bray,Perry Greenfield,Thomas Robitaille,Tom Aldcroft</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2739/astropy-and-astronomical-tools-part-i</guid><enclosure url="http://www.youtube.com/watch?v=eiSg1enFFV4" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/eiSg1enFFV4/hqdefault.jpg"></media:thumbnail></item><item><title>Bayesian Statistical Analysis using Python - Part 1</title><link>http://www.pyvideo.org/video/2760/bayesian-statistical-analysis-using-python-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This hands-on tutorial will introduce statistical analysis in Python using Bayesian methods. Bayesian statistics offer a flexible &amp; powerful way of  analyzing data, but are computationally-intensive, for which Python is ideal. As a gentle introduction, we will solve simple problems using NumPy and SciPy, before moving on to Markov chain Monte Carlo methods to build more complex models using PyMC.
&lt;p&gt;Description&lt;/p&gt;
The aim of this course is to introduce new users to the Bayesian approach of statistical modeling and analysis, so that they can use Python packages such as NumPy, SciPy and [PyMC](https://github.com/pymc-devs/pymc) effectively to analyze their own data. It is designed to get users quickly up and running with Bayesian methods, incorporating just enough statistical background to allow users to understand, in general terms, what they are implementing. The tutorial will be example-driven, with illustrative case studies using real data. Selected methods will include approximation methods, importance sampling, Markov chain Monte Carlo (MCMC) methods such as Metropolis-Hastings and Slice sampling. In addition to model fitting, the tutorial will address important techniques for model checking, model comparison, and steps for preparing data and processing model output. Tutorial content will be derived from the instructor's book *Bayesian Statistical Computing using Python*, to be published by Springer in late 2014.

![PyMC forest plot](http://d.pr/i/pqWT+)

![DAG](http://d.pr/i/AHZV+)

All course content will be available as a GitHub repository, including IPython notebooks and example data.

## Tutorial Outline

1.  Overview of Bayesian statistics.
2.  Bayesian Inference with NumPy and SciPy
3.  Markov chain Monte Carlo (MCMC)
4.  The Essentials of PyMC
5.  Fitting Linear Regression Models
6.  Hierarchical Modeling
7.  Model Checking and Validation

## Installation Instructions

The easiest way to install the Python packages required for this tutorial is via [Anaconda](https://store.continuum.io/cshop/anaconda/), a scientific Python distribution offered by Continuum analytics. Several other tutorials will be recommending a similar setup. 

One of the key features of Anaconda is a command line utility called `conda` that can be used to manage third party packages. We have built a PyMC package for `conda` that can be installed from your terminal via the following command:

    conda install -c https://conda.binstar.org/pymc pymc

This should install any prerequisite packages that are required to run PyMC.

One caveat is that conda does not yet have a build of PyMC for **Python 3**. Therefore, you would have to build it yourself via pip:

    pip install git+git://github.com/pymc-devs/pymc.git@2.3

For those of you on Mac OS X that are already using the [Homebrew](http://brew.sh) package manager, I have prepared a script that will install the entire Python scientific stack, including PyMC 2.3. You can download the script [here](https://gist.github.com/fonnesbeck/7de008b05e670d919b71) and run it via:

    sh install_superpack_brew.sh</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2760/bayesian-statistical-analysis-using-python-part-1</guid><enclosure url="http://www.youtube.com/watch?v=vOBB_ycQ0RA" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/vOBB_ycQ0RA/hqdefault.jpg"></media:thumbnail></item><item><title>Bayesian Statistical Analysis using Python - Part 2</title><link>http://www.pyvideo.org/video/2759/bayesian-statistical-analysis-using-python-part-0</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This hands-on tutorial will introduce statistical analysis in Python using Bayesian methods. Bayesian statistics offer a flexible &amp; powerful way of  analyzing data, but are computationally-intensive, for which Python is ideal. As a gentle introduction, we will solve simple problems using NumPy and SciPy, before moving on to Markov chain Monte Carlo methods to build more complex models using PyMC.
&lt;p&gt;Description&lt;/p&gt;
The aim of this course is to introduce new users to the Bayesian approach of statistical modeling and analysis, so that they can use Python packages such as NumPy, SciPy and [PyMC](https://github.com/pymc-devs/pymc) effectively to analyze their own data. It is designed to get users quickly up and running with Bayesian methods, incorporating just enough statistical background to allow users to understand, in general terms, what they are implementing. The tutorial will be example-driven, with illustrative case studies using real data. Selected methods will include approximation methods, importance sampling, Markov chain Monte Carlo (MCMC) methods such as Metropolis-Hastings and Slice sampling. In addition to model fitting, the tutorial will address important techniques for model checking, model comparison, and steps for preparing data and processing model output. Tutorial content will be derived from the instructor's book *Bayesian Statistical Computing using Python*, to be published by Springer in late 2014.

![PyMC forest plot](http://d.pr/i/pqWT+)

![DAG](http://d.pr/i/AHZV+)

All course content will be available as a GitHub repository, including IPython notebooks and example data.

## Tutorial Outline

1.  Overview of Bayesian statistics.
2.  Bayesian Inference with NumPy and SciPy
3.  Markov chain Monte Carlo (MCMC)
4.  The Essentials of PyMC
5.  Fitting Linear Regression Models
6.  Hierarchical Modeling
7.  Model Checking and Validation

## Installation Instructions

The easiest way to install the Python packages required for this tutorial is via [Anaconda](https://store.continuum.io/cshop/anaconda/), a scientific Python distribution offered by Continuum analytics. Several other tutorials will be recommending a similar setup. 

One of the key features of Anaconda is a command line utility called `conda` that can be used to manage third party packages. We have built a PyMC package for `conda` that can be installed from your terminal via the following command:

    conda install -c https://conda.binstar.org/pymc pymc

This should install any prerequisite packages that are required to run PyMC.

One caveat is that conda does not yet have a build of PyMC for **Python 3**. Therefore, you would have to build it yourself via pip:

    pip install git+git://github.com/pymc-devs/pymc.git@2.3

For those of you on Mac OS X that are already using the [Homebrew](http://brew.sh) package manager, I have prepared a script that will install the entire Python scientific stack, including PyMC 2.3. You can download the script [here](https://gist.github.com/fonnesbeck/7de008b05e670d919b71) and run it via:

    sh install_superpack_brew.sh</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2759/bayesian-statistical-analysis-using-python-part-0</guid><enclosure url="http://www.youtube.com/watch?v=gFYPCdWB2-w" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/gFYPCdWB2-w/hqdefault.jpg"></media:thumbnail></item><item><title>Bayesian Statistical Analysis using Python - Part 3</title><link>http://www.pyvideo.org/video/2758/bayesian-statistical-analysis-using-python-part</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This hands-on tutorial will introduce statistical analysis in Python using Bayesian methods. Bayesian statistics offer a flexible &amp; powerful way of  analyzing data, but are computationally-intensive, for which Python is ideal. As a gentle introduction, we will solve simple problems using NumPy and SciPy, before moving on to Markov chain Monte Carlo methods to build more complex models using PyMC.
&lt;p&gt;Description&lt;/p&gt;
The aim of this course is to introduce new users to the Bayesian approach of statistical modeling and analysis, so that they can use Python packages such as NumPy, SciPy and [PyMC](https://github.com/pymc-devs/pymc) effectively to analyze their own data. It is designed to get users quickly up and running with Bayesian methods, incorporating just enough statistical background to allow users to understand, in general terms, what they are implementing. The tutorial will be example-driven, with illustrative case studies using real data. Selected methods will include approximation methods, importance sampling, Markov chain Monte Carlo (MCMC) methods such as Metropolis-Hastings and Slice sampling. In addition to model fitting, the tutorial will address important techniques for model checking, model comparison, and steps for preparing data and processing model output. Tutorial content will be derived from the instructor's book *Bayesian Statistical Computing using Python*, to be published by Springer in late 2014.

![PyMC forest plot](http://d.pr/i/pqWT+)

![DAG](http://d.pr/i/AHZV+)

All course content will be available as a GitHub repository, including IPython notebooks and example data.

## Tutorial Outline

1.  Overview of Bayesian statistics.
2.  Bayesian Inference with NumPy and SciPy
3.  Markov chain Monte Carlo (MCMC)
4.  The Essentials of PyMC
5.  Fitting Linear Regression Models
6.  Hierarchical Modeling
7.  Model Checking and Validation

## Installation Instructions

The easiest way to install the Python packages required for this tutorial is via [Anaconda](https://store.continuum.io/cshop/anaconda/), a scientific Python distribution offered by Continuum analytics. Several other tutorials will be recommending a similar setup. 

One of the key features of Anaconda is a command line utility called `conda` that can be used to manage third party packages. We have built a PyMC package for `conda` that can be installed from your terminal via the following command:

    conda install -c https://conda.binstar.org/pymc pymc

This should install any prerequisite packages that are required to run PyMC.

One caveat is that conda does not yet have a build of PyMC for **Python 3**. Therefore, you would have to build it yourself via pip:

    pip install git+git://github.com/pymc-devs/pymc.git@2.3

For those of you on Mac OS X that are already using the [Homebrew](http://brew.sh) package manager, I have prepared a script that will install the entire Python scientific stack, including PyMC 2.3. You can download the script [here](https://gist.github.com/fonnesbeck/7de008b05e670d919b71) and run it via:

    sh install_superpack_brew.sh</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2758/bayesian-statistical-analysis-using-python-part</guid><enclosure url="http://www.youtube.com/watch?v=54sFjp7AvXM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/54sFjp7AvXM/hqdefault.jpg"></media:thumbnail></item><item><title>Bokeh: Interactive Visualizations in the Browser</title><link>http://www.pyvideo.org/video/2736/bokeh-interactive-visualizations-in-the-browser</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Bokeh is a Python visualization library for large datasets that natively uses the latest web technologies. Its goal is to provide concise construction of novel graphics, while delivering high-performance interactivity over large data to thin clients. This talk will cover the motivation and architecture behind Bokeh, demonstrate interesting uses and capability, and discuss future plans. 
&lt;p&gt;Description&lt;/p&gt;
With support from the DARPA XDATA Initiative, and contributions from community members, the Bokeh visualization library (http://bokeh.pydata.org) has grown into a large, successful open source project with heavy interest and following on GitHub (https://github.com/ContinuumIO/bokeh). The principal goals of Bokeh are to provide capability to developers and domain experts:

* easily create novel and powerful visualizations
* that extract insight from remote, possibly large data sets
* published to the web for others to explore and interact

This talk will describe how the architecture of Bokeh enables these goals, and demonstrate how it can be leveraged by anyone using python for analysis to visualize and present their work. We will talk about current development and future plans, including a brief discussion of Joseph Cottam's exciting academic work on abstract rendering for large data sets that is going into Bokeh (https://github.com/JosephCottam/AbstractRendering).




</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bryan Van de Ven</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2736/bokeh-interactive-visualizations-in-the-browser</guid><enclosure url="http://www.youtube.com/watch?v=B9NpLOyp-dI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/B9NpLOyp-dI/hqdefault.jpg"></media:thumbnail></item><item><title>Conda: A Cross Platform Package Manager for any Binary Distribution</title><link>http://www.pyvideo.org/video/2735/conda-a-cross-platform-package-manager-for-any-b</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Conda is an open source package manager, which can be used to manage binary packages and virtual environments on any platform. It is the package manager of the Anaconda Python distribution, although it can be used independently of Anaconda.  We will look at how conda solves many of the problems that have plagued Python packaging in the past, followed by a demonstration of its features.
&lt;p&gt;Description&lt;/p&gt;
We will look at the issues that have plagued packaging in the Python ecosystem in the past, and discuss how Conda solves these problems. We will show how to use conda to manage multiple environments. Finally, we will look at how to build your own conda packages. 

- What is the packaging problem? We will briefly look at the history of the problem and the various solutions to it.  There are two sides to the packaging problem: the problem of installing existing packages and the problem of building packages to be installed. We look at the history of distutils, setuptools, distribute, and pip, the some of the problems they solved, and issues that arose, particularly for the scientific Python community. 

- We will look at the conda package format, the design decisions that guided the format, and the implications of those decisions. A conda package is a bz2 compressed tarfile of all the files installed in a prefix, along with a metadata directory for the package. A conda package is typically installed by hard linking these files into the install prefix. Conda packages should be relocatable, so that they can be installed into any prefix. This allows conda packages to be installed into many virtual environments at once. A conda package is not Python specific. 

- We will look at how basic commands for installation and environment management. Conda uses a SAT solver to solve package dependency constraints, which is a simple, rigorous, and modern way to ensure that the set of packages that are installed are consistent with one another. 

- Conda has an extensive build framework which allows anybody to build their own conda packages. We will show how to use these tools and how to upload them to Binstar, a free packaging hosting service.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer,Ilan Schnell</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2735/conda-a-cross-platform-package-manager-for-any-b</guid><enclosure url="http://www.youtube.com/watch?v=UaIvrDWrIWM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/UaIvrDWrIWM/hqdefault.jpg"></media:thumbnail></item><item><title>Creating a browser based virtual computer lab for classroom instruction</title><link>http://www.pyvideo.org/video/2721/creating-a-browser-based-virtual-computer-lab-for</link><description>&lt;p&gt;Abstract&lt;/p&gt;
With laptops and tablets becoming more powerful and more ubiquitous in the classroom, traditional computer labs with rows of expensive desktops are beginning to lose their relevance. This presentation will discuss browser-based virtual computer labs for teaching Python, using a notebook interface, as an alternative approach to classroom instruction.
&lt;p&gt;Description&lt;/p&gt;
One of the difficulties in using Python for scientific applications is that one needs a fairly complete set of Python data processing and visualization packages to be installed, beyond the standard Python distribution. Freely available scientific Python distributions like Enthought Canopy and Anaconda address this problem. A typical approach to teaching Python is to use a dedicated computer lab, where one of these distributions is installed on a set of machines with identical computing environments for use by students. With laptop computers becoming cheap and ubiquitous, an alternative approach is to allow students to use their own computers, where they install one of the scientific Python distributions by themselves. This approach requires more set-up time, because the software often requires some minor tweaking for each software platform, but requires no dedicated hardware and has the advantage of allowing students to easily run programs after class on their own computers. This presentation discusses a third approach that involves creating a software environment for Python using cloud computing. There are already commercial products available that provide well-supported Python computing environments in the cloud. This presentation focuses on alternative roll your own solutions using open-source software that are specifically targeted for use in an interactive classroom instruction setting.

Creating a virtual computing lab usually involves instantiating a server using a cloud infrastructure provider, such as Amazon Web Services. A new server can be set-up within minutes, with a scientific Python distribution automatically installed during set-up. Students can then login to their own accounts on the server using a browser-based interface to execute Python programs and visualize graphical output. Typically, each student would use a notebook interface to work on lessons.

Different approaches can be used to create separate accounts for multiple users. The simplest would be to create different user accounts on a Linux virtual machine. If greater isolation is required, lightweight linux containers can be created on-demand for each user. Although IPython Notebook can currently be run as a public server to work with multiple notebooks simultaneously, true multi-user support is expected to be implemented further down the road. However, there are a few open-source projects, such as JiffyLab, that already support a multi-user IPython Notebook environment. Another option is to use the open-source GraphTerm server, which supports a multi-user  graphical terminal environment with a notebook interface. The pros and cons of these different approaches to building a virtual computer lab will be discussed.

Also discussed will be additional features that could be useful in a virtual computing lab such as the capability for the instructor to chat with the students and monitor their individual progress using a dashboard. Allowing students to collaborate in groups, with ability to view and edit each others code, can help promote classroom interaction. Enhancements to the notebook interface, such as fill in the blanks notebooks, can facilitate more structured instruction. The implementation of some of these features in the GraphTerm server will be discussed.

LINKS:

[JiffyLab source](https://github.com/ptone/jiffylab)

[GraphTerm source](https://github.com/mitotic/graphterm)

[GraphTerm talk from SciPy 2013](http://conference.scipy.org/proceedings/scipy2013/pdfs/saravanan.pdf)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ramalingam Saravanan</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2721/creating-a-browser-based-virtual-computer-lab-for</guid><enclosure url="http://www.youtube.com/watch?v=LiZJMYxvJbQ" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/LiZJMYxvJbQ/hqdefault.jpg"></media:thumbnail></item><item><title>Frequentism and Bayesianism: What's the Big Deal?</title><link>http://www.pyvideo.org/video/2714/frequentism-and-bayesianism-whats-the-big-deal</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Statistical analysis comes in two main flavors: frequentist and Bayesian. The subtle differences between the two can lead to widely divergent approaches to common data analysis tasks. After a brief discussion of the philosophical distinctions between the views, Ill utilize well-known Python libraries to demonstrate how this philosophy affects practical approaches to several common analysis tasks.
&lt;p&gt;Description&lt;/p&gt;
In scientific data mining and machine learning, a fundamental division is that of the frequentist and Bayesian approaches to statistics. Often the fodder for impassioned debate among statisticians and other practitioners, the subtle philosophical differences between the two camps can lead to surprisingly different practical approaches to the analysis of scientific data. 

In this talk I will delve into both the philosophical and practical aspects of Bayesian and frequentist approaches, drawing from a [series of posts](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) from my blog.

I'll start by addressing the philosophical differences between frequentism and Bayesianism, which boil down to different definitions of probability. I'll next move briefly into the mathematical details behind the two approaches, at a level which will be informative to a general scientific audience. I'll then show some examples of the two approaches applied to some increasingly more complicated problems using standard Python packages, namely: [NumPy](http://numpy.org), [SciPy](http://scipy.org), [Matplotlib](http://matplotlib.org), and [emcee](http://dan.iel.fm/emcee/).

With this combination of philosophy and practical examples, the audience should walk away with a much better understanding of the differences between frequentist and Bayesian approaches to statistical analysis, and especially how the philosophy of each approach affects the practical aspects of computation in data-intensive scientific research.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jake VanderPlas</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2714/frequentism-and-bayesianism-whats-the-big-deal</guid><enclosure url="http://www.youtube.com/watch?v=KhAUfqhLakw" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/KhAUfqhLakw/hqdefault.jpg"></media:thumbnail></item><item><title>Fundamentals of the IPython Display Architecture+Interactive Widgets</title><link>http://www.pyvideo.org/video/2743/fundamentals-of-the-ipython-display-architecture</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In this tutorial, attendees will learn how to use the IPython Notebooks display architecture and interactive widgets. As we cover these topics, attendees will learn about the underlying architecture, how to use IPythons existing APIs, and how to extend them for their own purposes. This tutorial will not cover the basics of the IPython Notebook.

&lt;p&gt;Description&lt;/p&gt;
IPython provides an architecture for interactive computing. The IPython Notebook is a web-based interactive computing environment for exploratory and reproducible computing. With the IPython Notebook, users create documents, called notebooks, that contain formatted text, figures, equations, programming code, and code output.

The IPython Notebook generalizes the notion of output to include images, LaTeX, video, HTML, JavaScript, PDF, etc. These output formats are displayed in the Notebook using IPythons display architecture, embedded in notebook documents and rendered on the IPython Notebook Viewer. By taking advantage of these rich output formats users can build notebooks that include rich representations and visualizations of data and other content. In this tutorial, we will describe the display architecture, existing Python APIs and libraries that already use it (mpld3, vincent, polotly, etc.), and how users can define custom display logic for their own Python objects.

As of version 2.0, the IPython Notebook also includes interactive JavaScript widgets.  These widgets provide a way for users to interact with UI controls in the browser that are tied to Python code in running in the kernel. We will begin by covering the highest-level API for these widgets, interact, which automatically builds a user interface for exploring a Python function. Next we will describe the lower-level widget objects that are included with IPython: sliders, text boxes, buttons, etc. However, the full potential of the widget framework lies with its extensibility.  Users can create their own custom widgets using Python, JavaScript, HTML and CSS. We will conclude with a detailed look at custom widget creation.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Brian Granger,Jonathan Frederic</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2743/fundamentals-of-the-ipython-display-architecture</guid><enclosure url="http://www.youtube.com/watch?v=aIXED26Wppg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/aIXED26Wppg/hqdefault.jpg"></media:thumbnail></item><item><title>Geospatial data in Python: Database, Desktop, and the Web part 1</title><link>http://www.pyvideo.org/video/2709/geospatial-data-in-python-database-desktop-and</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Using the wide range of tools and libraries available for working with geospatial data, it is now possible to transport geospatial data from a database to a web-interface in only a few lines of code. In this tutorial, we explore some of these libraries and work through examples which showcase the power of Python for geospatial data.
&lt;p&gt;Description&lt;/p&gt;
Tools and libraries for working with geospatial data in Python are currently undergoing rapid development and expansion. Libraries such as shapely, fiona, rasterio, geopandas, and others now provide Pythonic ways of reading, writing, editing, and manipulating geographic data. In this tutorial, participants will be exposed to a number of new and legacy geospatial libraries in Python, with a focus on simple and rapid interaction with geospatial data.

We will utilize Python to interact with geographic data from a database to a web interface, all the while showcasing how Python can be used to access data from online resources, query spatially enabled databases, perform coordinate transformations and geoprocessing functions, and export geospatial data to web-enabled formats for visualizing and sharing with others. Time permitting, we will also briefly explore Python plugin development for the QGIS Desktop GIS environment.

This tutorial should be accessible to anyone who has basic Python knowledge (though familiarity with Pandas, NumPy, matplotlib, etc. will be helpful) as well as familiarity with IPython Notebook. We will take some time at the start of the tutorial to go over installation strategies for geospatial libraries (GDAL/OGR, Proj.4, GEOS) and their Python bindings (Shapely, Fiona, GeoPandas) on Windows, Mac, and Linux. Some knowledge of geospatial concepts such as map projections and GIS data formats will also be helpful.

### Outline
- Introduction to geospatial data
    - Map projections, data formats, and looking at maps
- Introduction to geospatial libraries
    - GDAL/OGR (Fiona); Shapely (GEOS); PostGIS; GeoPandas; and more
- GeoPandas
    - Reading data from various sources
    - Data manipulation and plotting
    - Writing data to various sources
    - Getting data from the web
    - Pushing data to the web (for maps)
- Putting it all together
    - Quick example: From database to web
- Introduction to QGIS Desktop GIS (time permitting)
    - Python interface (PyQGIS)
    - Building a simple plugin
    - Plugin deployment
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2709/geospatial-data-in-python-database-desktop-and</guid><enclosure url="http://www.youtube.com/watch?v=ctdjAir4TUg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/ctdjAir4TUg/hqdefault.jpg"></media:thumbnail></item><item><title>Geospatial data in Python: Database, Desktop, and the Web part 2</title><link>http://www.pyvideo.org/video/2710/geospatial-data-in-python-database-desktop-and-0</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Using the wide range of tools and libraries available for working with geospatial data, it is now possible to transport geospatial data from a database to a web-interface in only a few lines of code. In this tutorial, we explore some of these libraries and work through examples which showcase the power of Python for geospatial data.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2710/geospatial-data-in-python-database-desktop-and-0</guid><enclosure url="http://www.youtube.com/watch?v=t-BJnls4o_s" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/t-BJnls4o_s/hqdefault.jpg"></media:thumbnail></item><item><title>Geospatial data in Python: Database, Desktop and the Web - Part 3</title><link>http://www.pyvideo.org/video/2761/geospatial-data-in-python-database-desktop-and-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Using the wide range of tools and libraries available for working with geospatial data, it is now possible to transport geospatial data from a database to a web-interface in only a few lines of code. In this tutorial, we explore some of these libraries and work through examples which showcase the power of Python for geospatial data.
&lt;p&gt;Description&lt;/p&gt;
Tools and libraries for working with geospatial data in Python are currently undergoing rapid development and expansion. Libraries such as shapely, fiona, rasterio, geopandas, and others now provide Pythonic ways of reading, writing, editing, and manipulating geographic data. In this tutorial, participants will be exposed to a number of new and legacy geospatial libraries in Python, with a focus on simple and rapid interaction with geospatial data.

We will utilize Python to interact with geographic data from a database to a web interface, all the while showcasing how Python can be used to access data from online resources, query spatially enabled databases, perform coordinate transformations and geoprocessing functions, and export geospatial data to web-enabled formats for visualizing and sharing with others. Time permitting, we will also briefly explore Python plugin development for the QGIS Desktop GIS environment.

This tutorial should be accessible to anyone who has basic Python knowledge (though familiarity with Pandas, NumPy, matplotlib, etc. will be helpful) as well as familiarity with IPython Notebook. We will take some time at the start of the tutorial to go over installation strategies for geospatial libraries (GDAL/OGR, Proj.4, GEOS) and their Python bindings (Shapely, Fiona, GeoPandas) on Windows, Mac, and Linux. Some knowledge of geospatial concepts such as map projections and GIS data formats will also be helpful.

### Outline
- Introduction to geospatial data
    - Map projections, data formats, and looking at maps
- Introduction to geospatial libraries
    - GDAL/OGR (Fiona); Shapely (GEOS); PostGIS; GeoPandas; and more
- GeoPandas
    - Reading data from various sources
    - Data manipulation and plotting
    - Writing data to various sources
    - Getting data from the web
    - Pushing data to the web (for maps)
- Putting it all together
    - Quick example: From database to web
- Introduction to QGIS Desktop GIS (time permitting)
    - Python interface (PyQGIS)
    - Building a simple plugin
    - Plugin deployment
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Carson Farmer</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2761/geospatial-data-in-python-database-desktop-and-1</guid><enclosure url="http://www.youtube.com/watch?v=0VSEfF0i1mI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/0VSEfF0i1mI/hqdefault.jpg"></media:thumbnail></item><item><title>HDF5 is for Lovers part 2</title><link>http://www.pyvideo.org/video/2708/hdf5-is-for-lovers-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
HDF5 is a hierarchical, binary database format that has become the de facto standard for 
scientific computing. While the spec may be used in a relatively simple way it also supports several high-level features that prove invaluable. HDF5 bindings exist for almost every language - including two Python libraries (PyTables and h5py). This tutorial will cover HDF5 through the lens of PyTables.
&lt;p&gt;Description&lt;/p&gt;
Description
------------
HDF5 is a hierarchical, binary database format that has become the de facto standard for 
scientific computing.  While the specification may be used in a relatively simple way 
(persistence of static arrays) it also supports several high-level features that prove 
invaluable.  These include chunking, ragged data, extensible data, parallel I/O, 
compression, complex selection, and in-core calculations.  Moreover, HDF5 bindings
exist for almost every language - including two Python libraries (PyTables and h5py). This tutorial will cover HDF5 itself through the lens of PyTables.

This tutorial will discuss tools, strategies, and hacks for really squeezing every ounce
of performance out of HDF5 in new or existing projects.  It will also go over fundamental 
limitations in the specification and provide creative and subtle strategies for getting around 
them.  Overall, this tutorial will show how HDF5 plays nicely with all parts of an application 
making the code and data both faster and smaller.  With such powerful features at the 
developer's disposal, what is not to love?!

Knowledge of Python, NumPy, C or C++, and basic HDF5 is recommended but not required.

Outline
--------------
* Meaning in layout (20 min)

    - Tips for choosing your hierarchy

* Advanced datatypes (20 min)

    - Tables
    - Nested types
    - Tricks with malloc() and byte-counting

* **Exercise on above topics** (20 min)

* Chunking (20 min)

    - How it works
    - How to properly select your chunksize

* Queries and Selections (20 min)

    - In-core vs Out-of-core calculations
    - PyTables.where()
    - Datasets vs Dataspaces

* **Exercise on above topics** (20 min)

* The Starving CPU Problem (1 hr)

    - Why you should always use compression
    - Compression algorithms available
    - Choosing the correct one
    - Exercise

* Integration with other databases (1 hr)

    - Migrating to/from SQL
    - HDF5 in other databases (JSON example)
    - Other Databases in HDF5 (JSON example)
    - Exercise
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anthony Scopatz</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2708/hdf5-is-for-lovers-part-2</guid><enclosure url="http://www.youtube.com/watch?v=EcM6g9Yp004" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/EcM6g9Yp004/hqdefault.jpg"></media:thumbnail></item><item><title>HDF5 is for Lovers, Tutorial part 1</title><link>http://www.pyvideo.org/video/2705/hdf5-is-for-lovers-tutorial-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
HDF5 is a hierarchical, binary database format that has become the de facto standard for 
scientific computing. While the spec may be used in a relatively simple way it also supports several high-level features that prove invaluable. HDF5 bindings exist for almost every language - including two Python libraries (PyTables and h5py). This tutorial will cover HDF5 through the lens of PyTables.
&lt;p&gt;Description&lt;/p&gt;
Description
------------
HDF5 is a hierarchical, binary database format that has become the de facto standard for 
scientific computing.  While the specification may be used in a relatively simple way 
(persistence of static arrays) it also supports several high-level features that prove 
invaluable.  These include chunking, ragged data, extensible data, parallel I/O, 
compression, complex selection, and in-core calculations.  Moreover, HDF5 bindings
exist for almost every language - including two Python libraries (PyTables and h5py). This tutorial will cover HDF5 itself through the lens of PyTables.

This tutorial will discuss tools, strategies, and hacks for really squeezing every ounce
of performance out of HDF5 in new or existing projects.  It will also go over fundamental 
limitations in the specification and provide creative and subtle strategies for getting around 
them.  Overall, this tutorial will show how HDF5 plays nicely with all parts of an application 
making the code and data both faster and smaller.  With such powerful features at the 
developer's disposal, what is not to love?!

Knowledge of Python, NumPy, C or C++, and basic HDF5 is recommended but not required.

Outline
--------------
* Meaning in layout (20 min)

    - Tips for choosing your hierarchy

* Advanced datatypes (20 min)

    - Tables
    - Nested types
    - Tricks with malloc() and byte-counting

* **Exercise on above topics** (20 min)

* Chunking (20 min)

    - How it works
    - How to properly select your chunksize

* Queries and Selections (20 min)

    - In-core vs Out-of-core calculations
    - PyTables.where()
    - Datasets vs Dataspaces

* **Exercise on above topics** (20 min)

* The Starving CPU Problem (1 hr)

    - Why you should always use compression
    - Compression algorithms available
    - Choosing the correct one
    - Exercise

* Integration with other databases (1 hr)

    - Migrating to/from SQL
    - HDF5 in other databases (JSON example)
    - Other Databases in HDF5 (JSON example)
    - Exercise
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Anthony Scopatz</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2705/hdf5-is-for-lovers-tutorial-part-1</guid><enclosure url="http://www.youtube.com/watch?v=EoqGt32gkPc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/EoqGt32gkPc/hqdefault.jpg"></media:thumbnail></item><item><title>Image analysis in Python with scipy and scikit image 4</title><link>http://www.pyvideo.org/video/2752/image-analysis-in-python-with-scipy-and-scikit-im</link><description>&lt;p&gt;Abstract&lt;/p&gt;
From telescopes to satellite cameras to electron microscopes, scientists are producing more images than they can manually inspect. This tutorial will introduce automated image analysis using the "images as numpy arrays" abstraction, run through various fundamental image analysis operations (filters, morphology, segmentation), and finally complete one or two more advanced real-world examples.
&lt;p&gt;Description&lt;/p&gt;
Image analysis is central to a boggling number of scientific endeavors. Google needs it for their self-driving cars and to match satellite imagery and mapping data. Neuroscientists need it to understand the brain. NASA needs it to [map asteroids](http://www.bbc.co.uk/news/technology-26528516) and save the human race. It is, however, a relatively underdeveloped area of scientific computing. Attendees will leave this tutorial confident of their ability to extract information from their images in Python.

Attendees will need a working knowledge of numpy arrays, but no further knowledge of images or voxels or other doodads. After a brief introduction to the idea that images are just arrays and vice versa, we will introduce fundamental image analysis operations: filters, which can be used to extract features such as edges, corners, and spots in an image; morphology, inferring shape properties by modifying the image through local operations; and segmentation, the division of an image into meaningful regions.

We will then combine all these concepts and apply them to several real-world examples of scientific image analysis:
given an image of a pothole, measure its size in pixels
compare the fluorescence intensity of a protein of interest in the centromeres vs the rest of the chromosome.
observe the distribution of cells invading a wound site

Attendees will also be encouraged to bring their own image analysis problems to the session for guidance, and, if time allows, we will cover more advanced topics such as image registration and stitching.

The entire tutorial will be coordinated with the IPython notebook, with various code cells left blank for attendees to fill in as exercises.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias,Tony Yu</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2752/image-analysis-in-python-with-scipy-and-scikit-im</guid><enclosure url="http://www.youtube.com/watch?v=pWnYjqudKHs" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/pWnYjqudKHs/hqdefault.jpg"></media:thumbnail></item><item><title>Image analysis in Python with scipy and scikit image, Part 1</title><link>http://www.pyvideo.org/video/2863/image-analysis-with-scikit-image-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
From telescopes to satellite cameras to electron microscopes, scientists are producing more images than they can manually inspect. This tutorial will introduce automated image analysis using the "images as numpy arrays" abstraction, run through various fundamental image analysis operations (filters, morphology, segmentation), and finally complete one or two more advanced real-world examples.
&lt;p&gt;Description&lt;/p&gt;
Image analysis is central to a boggling number of scientific endeavors. Google needs it for their self-driving cars and to match satellite imagery and mapping data. Neuroscientists need it to understand the brain. NASA needs it to [map asteroids](http://www.bbc.co.uk/news/technology-26528516) and save the human race. It is, however, a relatively underdeveloped area of scientific computing. Attendees will leave this tutorial confident of their ability to extract information from their images in Python.

Attendees will need a working knowledge of numpy arrays, but no further knowledge of images or voxels or other doodads. After a brief introduction to the idea that images are just arrays and vice versa, we will introduce fundamental image analysis operations: filters, which can be used to extract features such as edges, corners, and spots in an image; morphology, inferring shape properties by modifying the image through local operations; and segmentation, the division of an image into meaningful regions.

We will then combine all these concepts and apply them to several real-world examples of scientific image analysis:
given an image of a pothole, measure its size in pixels
compare the fluorescence intensity of a protein of interest in the centromeres vs the rest of the chromosome.
observe the distribution of cells invading a wound site

Attendees will also be encouraged to bring their own image analysis problems to the session for guidance, and, if time allows, we will cover more advanced topics such as image registration and stitching.

The entire tutorial will be coordinated with the IPython notebook, with various code cells left blank for attendees to fill in as exercises.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias,Tony Yu</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2863/image-analysis-with-scikit-image-part-1</guid><enclosure url="http://www.youtube.com/watch?v=MP-MTiCETYg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/MP-MTiCETYg/hqdefault.jpg"></media:thumbnail></item><item><title>Image analysis in Python with scipy and scikit image, Part 2</title><link>http://www.pyvideo.org/video/2862/image-analysis-with-scikit-image-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
From telescopes to satellite cameras to electron microscopes, scientists are producing more images than they can manually inspect. This tutorial will introduce automated image analysis using the "images as numpy arrays" abstraction, run through various fundamental image analysis operations (filters, morphology, segmentation), and finally complete one or two more advanced real-world examples.
&lt;p&gt;Description&lt;/p&gt;
Image analysis is central to a boggling number of scientific endeavors. Google needs it for their self-driving cars and to match satellite imagery and mapping data. Neuroscientists need it to understand the brain. NASA needs it to [map asteroids](http://www.bbc.co.uk/news/technology-26528516) and save the human race. It is, however, a relatively underdeveloped area of scientific computing. Attendees will leave this tutorial confident of their ability to extract information from their images in Python.

Attendees will need a working knowledge of numpy arrays, but no further knowledge of images or voxels or other doodads. After a brief introduction to the idea that images are just arrays and vice versa, we will introduce fundamental image analysis operations: filters, which can be used to extract features such as edges, corners, and spots in an image; morphology, inferring shape properties by modifying the image through local operations; and segmentation, the division of an image into meaningful regions.

We will then combine all these concepts and apply them to several real-world examples of scientific image analysis:
given an image of a pothole, measure its size in pixels
compare the fluorescence intensity of a protein of interest in the centromeres vs the rest of the chromosome.
observe the distribution of cells invading a wound site

Attendees will also be encouraged to bring their own image analysis problems to the session for guidance, and, if time allows, we will cover more advanced topics such as image registration and stitching.

The entire tutorial will be coordinated with the IPython notebook, with various code cells left blank for attendees to fill in as exercises.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias,Tony Yu</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2862/image-analysis-with-scikit-image-part-2</guid><enclosure url="http://www.youtube.com/watch?v=SE7h0IWD93Y" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/SE7h0IWD93Y/hqdefault.jpg"></media:thumbnail></item><item><title>Image analysis in Python with scipy and scikit image, Part 3</title><link>http://www.pyvideo.org/video/2861/image-analysis-with-scikit-image-part-3</link><description>&lt;p&gt;Abstract&lt;/p&gt;
From telescopes to satellite cameras to electron microscopes, scientists are producing more images than they can manually inspect. This tutorial will introduce automated image analysis using the "images as numpy arrays" abstraction, run through various fundamental image analysis operations (filters, morphology, segmentation), and finally complete one or two more advanced real-world examples.
&lt;p&gt;Description&lt;/p&gt;
Image analysis is central to a boggling number of scientific endeavors. Google needs it for their self-driving cars and to match satellite imagery and mapping data. Neuroscientists need it to understand the brain. NASA needs it to [map asteroids](http://www.bbc.co.uk/news/technology-26528516) and save the human race. It is, however, a relatively underdeveloped area of scientific computing. Attendees will leave this tutorial confident of their ability to extract information from their images in Python.

Attendees will need a working knowledge of numpy arrays, but no further knowledge of images or voxels or other doodads. After a brief introduction to the idea that images are just arrays and vice versa, we will introduce fundamental image analysis operations: filters, which can be used to extract features such as edges, corners, and spots in an image; morphology, inferring shape properties by modifying the image through local operations; and segmentation, the division of an image into meaningful regions.

We will then combine all these concepts and apply them to several real-world examples of scientific image analysis:
given an image of a pothole, measure its size in pixels
compare the fluorescence intensity of a protein of interest in the centromeres vs the rest of the chromosome.
observe the distribution of cells invading a wound site

Attendees will also be encouraged to bring their own image analysis problems to the session for guidance, and, if time allows, we will cover more advanced topics such as image registration and stitching.

The entire tutorial will be coordinated with the IPython notebook, with various code cells left blank for attendees to fill in as exercises.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Juan Nunez-Iglesias,Tony Yu</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2861/image-analysis-with-scikit-image-part-3</guid><enclosure url="http://www.youtube.com/watch?v=Yxpnvc4RHy4" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Yxpnvc4RHy4/hqdefault.jpg"></media:thumbnail></item><item><title>Integrating Python and C++ with Boost Python part 1</title><link>http://www.pyvideo.org/video/2749/integrating-python-and-c-with-boost-python-part-0</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Python and C++ can be powerful complements to one another. C++ is great for performance-critical calculations, while Python is great for everything else. In this tutorial well look at how to integrate Python and C++ using the Boost.Python library. Youll learn techniques for easily developing hybrid systems that use the right language for the right task, resulting in better software.

&lt;p&gt;Description&lt;/p&gt;
Python and C++ are both popular languages that each bring a lot to the table. The languages also complement one another well: Python is high-level, dynamic, and easy to use while C++ is at-the-metal, static, and (in)famously tricky. There are times when there are real advantages to combining these disparate natures, and Pythons C API provides a strong interface for doing just that. Boost.Python is a C++ library that builds upon and improves Pythons C API to give users a simpler, more intuitive, and safer means to integrate Python and C++.

In this tutorial well look at how to use Boost.Python to effectively bridge the Python/C++ boundary. Well start by briefly looking at the fundamentals of the Python C API since that defines the ground rules; this includes things like reference counting, the basic object model, and so forth. Well then quickly look at the Boost.Python API and show how it provides the same functionality as the underlying C API, but does so in a way that doesnt obscure the real semantics of the Python language.

After this introduction, the rest of the tutorial will involve writing code to explore various elements of Boost.Python. Well focus on techniques for extending Python with C++, that is, writing Python modules in C++. Boost.Python can be used for embedding (i.e. invoking Python code from C++), but that involves a different set of techniques, and in practice most scientific Python developers are more interested in developing extensions.

The syllabus for the four-hour tutorial will be like this:

1. Introduction: C-API and Boost.Python basics

    Note that this can be reduced or eliminated of participants are already comfortable with the topics.

2. Hello World: Exposing a basic function

    In this section well get a minimal Boost.Python module working. This will not only introduce students to the infrastructure of Boost.Python, but it will also give us a chance to make sure that everyones build environment is working.

3. Exposing functions

    In this section well look at the details of exposing C++ functions to Python. The topics well cover will include overloading (including Boost.Pythons auto-overload feature), default argument values, and a brief look at call policies.

4. Exposing classes

    Here well look at how to expose C++ classes to Python. Topics will include the basic `class_&lt;T&gt;` template, member functions, data members, properties, inheritance, and virtual functions.

5. `boost::python::object`

    The `boost::python::object` class is Boost.Pythons primary interface to Pythons `PyObject` structure. Understanding how to work with this class is a key building-block for developing Python modules with Boost.Python. Well explore its API and features, including areas like attribute access, reference counting, and converting between Python and C++ objects.

6. Derived object types

    Boost.Python provides a number of `boost::python::object` subclasses for important Python classes like `list`, `dict`, and `tuple`. In this section well look at these subclasses and how to use them in Boost.Python modules.

7. Enums

    Boost.Python provides `enum_&lt;T&gt;` for exposing C++ enums to Python. Python doesnt have a notion of enums *per se*, but in this section well explore how this template makes it straightforward to use C++ enums in Python in a simple and intuitive way.

8. Type conversion

    In this section well look at Boost.Pythons support for doing automatic type-conversion across the Python/C++ boundary. Well see how you can register type-converters with Boost.Python which will be invoked whenever Boost.Python needs to convert a Python object to a C++ object or vice versa.

This is a fairly ambitious set of topics, and its possible that we wont be able to cover them all. The topics are roughly in most-often-used to least-often-used order, however, so students will be sure to be exposed to the most important and relevant elements of the course.

Likewise, the four-hour format of the course means that we wont be able to go into great depth on many topics. The main goal of the course, then, is to give students enough orientation and hands-on experience with Boost.Python that they can continue to learn on their own. Inter-language integration - especially between languages as dissimilar as C++ and Python - can be quite complex, but this tutorial will give students the grounding they need to successfully apply Boost.Python to their problems.  
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2749/integrating-python-and-c-with-boost-python-part-0</guid><enclosure url="http://www.youtube.com/watch?v=GE8EsGUsC2w" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/GE8EsGUsC2w/hqdefault.jpg"></media:thumbnail></item><item><title>Integrating Python and C++ with Boost Python part 2</title><link>http://www.pyvideo.org/video/2748/integrating-python-and-c-with-boost-python-part</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Python and C++ can be powerful complements to one another. C++ is great for performance-critical calculations, while Python is great for everything else. In this tutorial well look at how to integrate Python and C++ using the Boost.Python library. Youll learn techniques for easily developing hybrid systems that use the right language for the right task, resulting in better software.

&lt;p&gt;Description&lt;/p&gt;
Python and C++ are both popular languages that each bring a lot to the table. The languages also complement one another well: Python is high-level, dynamic, and easy to use while C++ is at-the-metal, static, and (in)famously tricky. There are times when there are real advantages to combining these disparate natures, and Pythons C API provides a strong interface for doing just that. Boost.Python is a C++ library that builds upon and improves Pythons C API to give users a simpler, more intuitive, and safer means to integrate Python and C++.

In this tutorial well look at how to use Boost.Python to effectively bridge the Python/C++ boundary. Well start by briefly looking at the fundamentals of the Python C API since that defines the ground rules; this includes things like reference counting, the basic object model, and so forth. Well then quickly look at the Boost.Python API and show how it provides the same functionality as the underlying C API, but does so in a way that doesnt obscure the real semantics of the Python language.

After this introduction, the rest of the tutorial will involve writing code to explore various elements of Boost.Python. Well focus on techniques for extending Python with C++, that is, writing Python modules in C++. Boost.Python can be used for embedding (i.e. invoking Python code from C++), but that involves a different set of techniques, and in practice most scientific Python developers are more interested in developing extensions.

The syllabus for the four-hour tutorial will be like this:

1. Introduction: C-API and Boost.Python basics

    Note that this can be reduced or eliminated of participants are already comfortable with the topics.

2. Hello World: Exposing a basic function

    In this section well get a minimal Boost.Python module working. This will not only introduce students to the infrastructure of Boost.Python, but it will also give us a chance to make sure that everyones build environment is working.

3. Exposing functions

    In this section well look at the details of exposing C++ functions to Python. The topics well cover will include overloading (including Boost.Pythons auto-overload feature), default argument values, and a brief look at call policies.

4. Exposing classes

    Here well look at how to expose C++ classes to Python. Topics will include the basic `class_&lt;T&gt;` template, member functions, data members, properties, inheritance, and virtual functions.

5. `boost::python::object`

    The `boost::python::object` class is Boost.Pythons primary interface to Pythons `PyObject` structure. Understanding how to work with this class is a key building-block for developing Python modules with Boost.Python. Well explore its API and features, including areas like attribute access, reference counting, and converting between Python and C++ objects.

6. Derived object types

    Boost.Python provides a number of `boost::python::object` subclasses for important Python classes like `list`, `dict`, and `tuple`. In this section well look at these subclasses and how to use them in Boost.Python modules.

7. Enums

    Boost.Python provides `enum_&lt;T&gt;` for exposing C++ enums to Python. Python doesnt have a notion of enums *per se*, but in this section well explore how this template makes it straightforward to use C++ enums in Python in a simple and intuitive way.

8. Type conversion

    In this section well look at Boost.Pythons support for doing automatic type-conversion across the Python/C++ boundary. Well see how you can register type-converters with Boost.Python which will be invoked whenever Boost.Python needs to convert a Python object to a C++ object or vice versa.

This is a fairly ambitious set of topics, and its possible that we wont be able to cover them all. The topics are roughly in most-often-used to least-often-used order, however, so students will be sure to be exposed to the most important and relevant elements of the course.

Likewise, the four-hour format of the course means that we wont be able to go into great depth on many topics. The main goal of the course, then, is to give students enough orientation and hands-on experience with Boost.Python that they can continue to learn on their own. Inter-language integration - especially between languages as dissimilar as C++ and Python - can be quite complex, but this tutorial will give students the grounding they need to successfully apply Boost.Python to their problems.  
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2748/integrating-python-and-c-with-boost-python-part</guid><enclosure url="http://www.youtube.com/watch?v=WLKuHbkh6jU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/WLKuHbkh6jU/hqdefault.jpg"></media:thumbnail></item><item><title>Integrating Python and C++ with Boost Python part 3</title><link>http://www.pyvideo.org/video/2750/integrating-python-and-c-with-boost-python-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Python and C++ can be powerful complements to one another. C++ is great for performance-critical calculations, while Python is great for everything else. In this tutorial well look at how to integrate Python and C++ using the Boost.Python library. Youll learn techniques for easily developing hybrid systems that use the right language for the right task, resulting in better software.

&lt;p&gt;Description&lt;/p&gt;
Python and C++ are both popular languages that each bring a lot to the table. The languages also complement one another well: Python is high-level, dynamic, and easy to use while C++ is at-the-metal, static, and (in)famously tricky. There are times when there are real advantages to combining these disparate natures, and Pythons C API provides a strong interface for doing just that. Boost.Python is a C++ library that builds upon and improves Pythons C API to give users a simpler, more intuitive, and safer means to integrate Python and C++.

In this tutorial well look at how to use Boost.Python to effectively bridge the Python/C++ boundary. Well start by briefly looking at the fundamentals of the Python C API since that defines the ground rules; this includes things like reference counting, the basic object model, and so forth. Well then quickly look at the Boost.Python API and show how it provides the same functionality as the underlying C API, but does so in a way that doesnt obscure the real semantics of the Python language.

After this introduction, the rest of the tutorial will involve writing code to explore various elements of Boost.Python. Well focus on techniques for extending Python with C++, that is, writing Python modules in C++. Boost.Python can be used for embedding (i.e. invoking Python code from C++), but that involves a different set of techniques, and in practice most scientific Python developers are more interested in developing extensions.

The syllabus for the four-hour tutorial will be like this:

1. Introduction: C-API and Boost.Python basics

    Note that this can be reduced or eliminated of participants are already comfortable with the topics.

2. Hello World: Exposing a basic function

    In this section well get a minimal Boost.Python module working. This will not only introduce students to the infrastructure of Boost.Python, but it will also give us a chance to make sure that everyones build environment is working.

3. Exposing functions

    In this section well look at the details of exposing C++ functions to Python. The topics well cover will include overloading (including Boost.Pythons auto-overload feature), default argument values, and a brief look at call policies.

4. Exposing classes

    Here well look at how to expose C++ classes to Python. Topics will include the basic `class_&lt;T&gt;` template, member functions, data members, properties, inheritance, and virtual functions.

5. `boost::python::object`

    The `boost::python::object` class is Boost.Pythons primary interface to Pythons `PyObject` structure. Understanding how to work with this class is a key building-block for developing Python modules with Boost.Python. Well explore its API and features, including areas like attribute access, reference counting, and converting between Python and C++ objects.

6. Derived object types

    Boost.Python provides a number of `boost::python::object` subclasses for important Python classes like `list`, `dict`, and `tuple`. In this section well look at these subclasses and how to use them in Boost.Python modules.

7. Enums

    Boost.Python provides `enum_&lt;T&gt;` for exposing C++ enums to Python. Python doesnt have a notion of enums *per se*, but in this section well explore how this template makes it straightforward to use C++ enums in Python in a simple and intuitive way.

8. Type conversion

    In this section well look at Boost.Pythons support for doing automatic type-conversion across the Python/C++ boundary. Well see how you can register type-converters with Boost.Python which will be invoked whenever Boost.Python needs to convert a Python object to a C++ object or vice versa.

This is a fairly ambitious set of topics, and its possible that we wont be able to cover them all. The topics are roughly in most-often-used to least-often-used order, however, so students will be sure to be exposed to the most important and relevant elements of the course.

Likewise, the four-hour format of the course means that we wont be able to go into great depth on many topics. The main goal of the course, then, is to give students enough orientation and hands-on experience with Boost.Python that they can continue to learn on their own. Inter-language integration - especially between languages as dissimilar as C++ and Python - can be quite complex, but this tutorial will give students the grounding they need to successfully apply Boost.Python to their problems.  
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2750/integrating-python-and-c-with-boost-python-part-1</guid><enclosure url="http://www.youtube.com/watch?v=kR5tREIotlI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/kR5tREIotlI/hqdefault.jpg"></media:thumbnail></item><item><title>Integrating Python and C++ with Boost Python part 4</title><link>http://www.pyvideo.org/video/2751/integrating-python-and-c-with-boost-python-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Python and C++ can be powerful complements to one another. C++ is great for performance-critical calculations, while Python is great for everything else. In this tutorial well look at how to integrate Python and C++ using the Boost.Python library. Youll learn techniques for easily developing hybrid systems that use the right language for the right task, resulting in better software.

&lt;p&gt;Description&lt;/p&gt;
Python and C++ are both popular languages that each bring a lot to the table. The languages also complement one another well: Python is high-level, dynamic, and easy to use while C++ is at-the-metal, static, and (in)famously tricky. There are times when there are real advantages to combining these disparate natures, and Pythons C API provides a strong interface for doing just that. Boost.Python is a C++ library that builds upon and improves Pythons C API to give users a simpler, more intuitive, and safer means to integrate Python and C++.

In this tutorial well look at how to use Boost.Python to effectively bridge the Python/C++ boundary. Well start by briefly looking at the fundamentals of the Python C API since that defines the ground rules; this includes things like reference counting, the basic object model, and so forth. Well then quickly look at the Boost.Python API and show how it provides the same functionality as the underlying C API, but does so in a way that doesnt obscure the real semantics of the Python language.

After this introduction, the rest of the tutorial will involve writing code to explore various elements of Boost.Python. Well focus on techniques for extending Python with C++, that is, writing Python modules in C++. Boost.Python can be used for embedding (i.e. invoking Python code from C++), but that involves a different set of techniques, and in practice most scientific Python developers are more interested in developing extensions.

The syllabus for the four-hour tutorial will be like this:

1. Introduction: C-API and Boost.Python basics

    Note that this can be reduced or eliminated of participants are already comfortable with the topics.

2. Hello World: Exposing a basic function

    In this section well get a minimal Boost.Python module working. This will not only introduce students to the infrastructure of Boost.Python, but it will also give us a chance to make sure that everyones build environment is working.

3. Exposing functions

    In this section well look at the details of exposing C++ functions to Python. The topics well cover will include overloading (including Boost.Pythons auto-overload feature), default argument values, and a brief look at call policies.

4. Exposing classes

    Here well look at how to expose C++ classes to Python. Topics will include the basic `class_&lt;T&gt;` template, member functions, data members, properties, inheritance, and virtual functions.

5. `boost::python::object`

    The `boost::python::object` class is Boost.Pythons primary interface to Pythons `PyObject` structure. Understanding how to work with this class is a key building-block for developing Python modules with Boost.Python. Well explore its API and features, including areas like attribute access, reference counting, and converting between Python and C++ objects.

6. Derived object types

    Boost.Python provides a number of `boost::python::object` subclasses for important Python classes like `list`, `dict`, and `tuple`. In this section well look at these subclasses and how to use them in Boost.Python modules.

7. Enums

    Boost.Python provides `enum_&lt;T&gt;` for exposing C++ enums to Python. Python doesnt have a notion of enums *per se*, but in this section well explore how this template makes it straightforward to use C++ enums in Python in a simple and intuitive way.

8. Type conversion

    In this section well look at Boost.Pythons support for doing automatic type-conversion across the Python/C++ boundary. Well see how you can register type-converters with Boost.Python which will be invoked whenever Boost.Python needs to convert a Python object to a C++ object or vice versa.

This is a fairly ambitious set of topics, and its possible that we wont be able to cover them all. The topics are roughly in most-often-used to least-often-used order, however, so students will be sure to be exposed to the most important and relevant elements of the course.

Likewise, the four-hour format of the course means that we wont be able to go into great depth on many topics. The main goal of the course, then, is to give students enough orientation and hands-on experience with Boost.Python that they can continue to learn on their own. Inter-language integration - especially between languages as dissimilar as C++ and Python - can be quite complex, but this tutorial will give students the grounding they need to successfully apply Boost.Python to their problems.  
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Austin Bingham</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2751/integrating-python-and-c-with-boost-python-part-2</guid><enclosure url="http://www.youtube.com/watch?v=GgtlExpPM1c" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/GgtlExpPM1c/hqdefault.jpg"></media:thumbnail></item><item><title>Interactive Parallel Computing with IPython Part 1</title><link>http://www.pyvideo.org/video/2738/interactive-parallel-computing-with-ipython-part</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Learn about interactive parallel computing in IPython.parallel, with examples including parallel image processing, machine learning, and physical simulations. IPython provides an easy way to interact with your multicore laptop or compute cluster.
&lt;p&gt;Description&lt;/p&gt;
IPython provides tools for interactive exploration of code and data. IPython.parallel is the part of IPython that enables an interactive model for parallel execution, and aims to make distributing your work on a multicore computer, local clusters or cloud services such as AWS or MS Azure simple and straightforward. The tutorial will cover how to do interactive and asynchronous parallel computing with IPython, and how to get the most out of your IPython cluster. Some of IPythons novel interactive features will be demonstrated, such as automatically parallelizing code with magics in the IPython Notebook and interactive debugging of remote execution. Examples covered will include parallel image processing, machine learning, and physical simulations, with exercises to solve along the way.

* Introduction to IPython.parallel
   * Deploying IPython
   * Using DirectViews and LoadBalancedViews
   * The basic model for execution
* Getting to know your IPython cluster:
   * Working with remote namespaces
   * AsyncResult: the API for asynchronous execution
   * Interacting with incomplete results. Remember, its about interactivity
   * Interactive parallel plotting
* More advanced topics:
   * Using IPython.parallel with traditional (MPI) parallel programs
   * Debugging parallel code
   * Minimizing data movement
   * Task dependencies
   * Caveats and tuning tips for IPython.parallel</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez,Min RK</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2738/interactive-parallel-computing-with-ipython-part</guid><enclosure url="http://www.youtube.com/watch?v=y4hgalfhc1Y" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/y4hgalfhc1Y/hqdefault.jpg"></media:thumbnail></item><item><title>Interactive Parallel Computing with IPython Part 2</title><link>http://www.pyvideo.org/video/2740/interactive-parallel-computing-with-ipython-part-0</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Learn about interactive parallel computing in IPython.parallel, with examples including parallel image processing, machine learning, and physical simulations. IPython provides an easy way to interact with your multicore laptop or compute cluster.
&lt;p&gt;Description&lt;/p&gt;
IPython provides tools for interactive exploration of code and data. IPython.parallel is the part of IPython that enables an interactive model for parallel execution, and aims to make distributing your work on a multicore computer, local clusters or cloud services such as AWS or MS Azure simple and straightforward. The tutorial will cover how to do interactive and asynchronous parallel computing with IPython, and how to get the most out of your IPython cluster. Some of IPythons novel interactive features will be demonstrated, such as automatically parallelizing code with magics in the IPython Notebook and interactive debugging of remote execution. Examples covered will include parallel image processing, machine learning, and physical simulations, with exercises to solve along the way.

* Introduction to IPython.parallel
   * Deploying IPython
   * Using DirectViews and LoadBalancedViews
   * The basic model for execution
* Getting to know your IPython cluster:
   * Working with remote namespaces
   * AsyncResult: the API for asynchronous execution
   * Interacting with incomplete results. Remember, its about interactivity
   * Interactive parallel plotting
* More advanced topics:
   * Using IPython.parallel with traditional (MPI) parallel programs
   * Debugging parallel code
   * Minimizing data movement
   * Task dependencies
   * Caveats and tuning tips for IPython.parallel</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez,Min RK</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2740/interactive-parallel-computing-with-ipython-part-0</guid><enclosure url="http://www.youtube.com/watch?v=-9ijnHPCYhY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/-9ijnHPCYhY/hqdefault.jpg"></media:thumbnail></item><item><title>Interactive Parallel Computing with IPython Part 3</title><link>http://www.pyvideo.org/video/2741/interactive-parallel-computing-with-ipython-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Learn about interactive parallel computing in IPython.parallel, with examples including parallel image processing, machine learning, and physical simulations. IPython provides an easy way to interact with your multicore laptop or compute cluster.
&lt;p&gt;Description&lt;/p&gt;
IPython provides tools for interactive exploration of code and data. IPython.parallel is the part of IPython that enables an interactive model for parallel execution, and aims to make distributing your work on a multicore computer, local clusters or cloud services such as AWS or MS Azure simple and straightforward. The tutorial will cover how to do interactive and asynchronous parallel computing with IPython, and how to get the most out of your IPython cluster. Some of IPythons novel interactive features will be demonstrated, such as automatically parallelizing code with magics in the IPython Notebook and interactive debugging of remote execution. Examples covered will include parallel image processing, machine learning, and physical simulations, with exercises to solve along the way.

* Introduction to IPython.parallel
   * Deploying IPython
   * Using DirectViews and LoadBalancedViews
   * The basic model for execution
* Getting to know your IPython cluster:
   * Working with remote namespaces
   * AsyncResult: the API for asynchronous execution
   * Interacting with incomplete results. Remember, its about interactivity
   * Interactive parallel plotting
* More advanced topics:
   * Using IPython.parallel with traditional (MPI) parallel programs
   * Debugging parallel code
   * Minimizing data movement
   * Task dependencies
   * Caveats and tuning tips for IPython.parallel</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Fernando Prez,Min RK</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2741/interactive-parallel-computing-with-ipython-part-1</guid><enclosure url="http://www.youtube.com/watch?v=U5mhpKkIx2Y" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/U5mhpKkIx2Y/hqdefault.jpg"></media:thumbnail></item><item><title>Intergrating Pylearn2 and Hyperopt: Taking Deep Learning Further with Hyperparamter Optimization</title><link>http://www.pyvideo.org/video/2731/intergrating-pylearn2-and-hyperopt-taking-deep-l</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This talk/poster will outline and present recent work in integrating Hyperopt, a package for the optimization of the hyperparameters of machine learning algorithms, with Pylearn2, a machine learning research and prototyping framework focused on "deep learning" algorithms, the technical challenges we faced and how we addressed them.
&lt;p&gt;Description&lt;/p&gt;
Deep learning algorithms have recently garnered much attention for their successes in solving very difficult industrial machine perception problems. However, for many practical purposes, these algorithms are unwieldy due to the rapid proliferation of "hyperparameters" in their specification -- architectural and optimization constants which ordinarily must be specified a priori by the practitioner. There is a growing interest within the machine learning community, and acutely so amongst deep learning researchers, in intelligently automating the selection of hyperparameters for machine learning algorithms by through the use of sequential model-based optimization techniques. [Hyperopt][http://hyperopt.github.io/hyperopt/] is software package designed for this purpose, architected as a general framework for hyperparameter optimization algorithms with support for complicated, awkward hyperparameter spaces that, e.g., involve many hyperparameters that are only meaningful in the context of certain values of other hyperparameters.

[Pylearn2][http://deeplearning.net/software/pylearn2] is a framework for machine learning developed by the LISA laboratory at Universit de Montral; it is a research and prototyping library aimed primarily at machine learning researchers, with a focus on "deep learning" algorithms. Despite being far from a stable release, it has had considerable impact and developed a very active user community outside of the laboratory that birthed it.

This talk will deecribe recent efforts in building a flexible, user-friendly bridge between Pylearn2 and Hyperopt for the purpose of optimizing the hyperparameters of deep learning algorithms. Briefly, it will outline the relevant problem domain and the two packages, the technical challenges we've met in adapting the two for use with one another and our solutions to them, in particular the development of a novel common deferred evaluation/call-graph description language based on `functools.partial`, which we hope to make available in the near future as a standalone package.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David Warde-Farley</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2731/intergrating-pylearn2-and-hyperopt-taking-deep-l</guid><enclosure url="http://www.youtube.com/watch?v=t50CGzbtcrY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/t50CGzbtcrY/hqdefault.jpg"></media:thumbnail></item><item><title>Introduction to Julia - Part 1</title><link>http://www.pyvideo.org/video/2754/introduction-to-julia-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
An introduction to the new Julia language from scratch, emphasising similarities and differences with scientific Python.
&lt;p&gt;Description&lt;/p&gt;
Julia is a new, up-and-coming language that has many similarities to Python, but some differences.
One of its main advantages is the speed gain obtained by automatically compiling all code (in a somewhat similar way to `PyPy`, `Cython`, `numba`, etc.), despite having an interactive interface very similar to that of Python. 

This will be a tutorial on the basic features of Julia from scratch, given by a user (rather than a developer) of the language, emphasising those features which are similar to Python (and hence do not require much explanation) and those features which are rather different. 

The idea of the tutorial is to give an idea of why there is suddenly such a buzz around Julia and why it can be useful for certain projects.

This tutorial is aimed at people who are already familiar with the basic scientific Python packages; it is not aimed at beginners in scientific programming.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David P. Sanders</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2754/introduction-to-julia-part-1</guid><enclosure url="http://www.youtube.com/watch?v=vWkgEddb4-A" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/vWkgEddb4-A/hqdefault.jpg"></media:thumbnail></item><item><title>Introduction to Julia - Part 2</title><link>http://www.pyvideo.org/video/2753/introduction-to-julia-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
An introduction to the new Julia language from scratch, emphasising similarities and differences with scientific Python.
&lt;p&gt;Description&lt;/p&gt;
Julia is a new, up-and-coming language that has many similarities to Python, but some differences.
One of its main advantages is the speed gain obtained by automatically compiling all code (in a somewhat similar way to `PyPy`, `Cython`, `numba`, etc.), despite having an interactive interface very similar to that of Python. 

This will be a tutorial on the basic features of Julia from scratch, given by a user (rather than a developer) of the language, emphasising those features which are similar to Python (and hence do not require much explanation) and those features which are rather different. 

The idea of the tutorial is to give an idea of why there is suddenly such a buzz around Julia and why it can be useful for certain projects.

This tutorial is aimed at people who are already familiar with the basic scientific Python packages; it is not aimed at beginners in scientific programming.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David P. Sanders</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2753/introduction-to-julia-part-2</guid><enclosure url="http://www.youtube.com/watch?v=I3JH5Bg46yU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/I3JH5Bg46yU/hqdefault.jpg"></media:thumbnail></item><item><title>Keynote: Computational Thinking is Computational Learning</title><link>http://www.pyvideo.org/video/2719/keynote-lorena-barba</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lorena Barba</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2719/keynote-lorena-barba</guid><enclosure url="http://www.youtube.com/watch?v=TWxwKDT88GU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/TWxwKDT88GU/hqdefault.jpg"></media:thumbnail></item><item><title>Lightning Talks Tuesday July 8 2014</title><link>http://www.pyvideo.org/video/2716/lightning-talks-tuesday-july-8-2014</link><description></description><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2716/lightning-talks-tuesday-july-8-2014</guid><enclosure url="http://www.youtube.com/watch?v=JDrhn0-r9Eg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/JDrhn0-r9Eg/hqdefault.jpg"></media:thumbnail></item><item><title>Multibody Dynamics and Control with Python part 1</title><link>http://www.pyvideo.org/video/2747/multibody-dynamics-and-control-with-python-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In this tutorial, attendees will learn how to derive, simulate, and visualize
the motion of a multibody dynamic system with Python tools. These methods and
techniques play an important role in the design and understanding of robots,
vehicles, spacecraft, manufacturing machines, human motion, etc. Attendees will
develop code to simulate the motion of a human or humanoid robot.
&lt;p&gt;Description&lt;/p&gt;
In this tutorial, attendees will learn how to derive, simulate, and visualize
the motion of a multibody dynamic system with Python tools. The tutorial will
demonstrate an advanced symbolic and numeric pipeline for a typical multibody
simulation problem. These methods and techniques play an important role in the
design and understanding of robots, vehicles, spacecraft, manufacturing
machines, human motion, etc. At the end, the attendees will have developed code
to simulate the uncontrolled and controlled motion of a human or humanoid
robot.

We will highlight the derivation of realistic models of motion with the SymPy
Mechanics package. We will walk through the derivation of the equations of
motion of a multibody system (i.e. the model or the plant), simulating and
visualizing the free motion of the system, and finally we will addfeedback
controllers to control the plants that we derive.

It is best if the attendees have some background with calculus-based college
level physics. They should also be familiar with the SciPy Stack, in particular
IPython, SymPy, NumPy, and SciPy. Our goal is that attendees will come away
with the ability to model basic multibody systems, simulate and visualize the
motion, and apply feedback controllers all in a Python framework.

The tutorial materials including an outline can be viewed here:

[https://github.com/pydy/pydy-tutorial-pycon-2014](https://github.com/pydy/pydy-tutorial-pycon-2014)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason K. Moore</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2747/multibody-dynamics-and-control-with-python-part-1</guid><enclosure url="http://www.youtube.com/watch?v=lWbeuDwYVto" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/lWbeuDwYVto/hqdefault.jpg"></media:thumbnail></item><item><title>Multibody Dynamics and Control with Python part 2</title><link>http://www.pyvideo.org/video/2745/multibody-dynamics-and-control-with-python-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In this tutorial, attendees will learn how to derive, simulate, and visualize
the motion of a multibody dynamic system with Python tools. These methods and
techniques play an important role in the design and understanding of robots,
vehicles, spacecraft, manufacturing machines, human motion, etc. Attendees will
develop code to simulate the motion of a human or humanoid robot.
&lt;p&gt;Description&lt;/p&gt;
In this tutorial, attendees will learn how to derive, simulate, and visualize
the motion of a multibody dynamic system with Python tools. The tutorial will
demonstrate an advanced symbolic and numeric pipeline for a typical multibody
simulation problem. These methods and techniques play an important role in the
design and understanding of robots, vehicles, spacecraft, manufacturing
machines, human motion, etc. At the end, the attendees will have developed code
to simulate the uncontrolled and controlled motion of a human or humanoid
robot.

We will highlight the derivation of realistic models of motion with the SymPy
Mechanics package. We will walk through the derivation of the equations of
motion of a multibody system (i.e. the model or the plant), simulating and
visualizing the free motion of the system, and finally we will addfeedback
controllers to control the plants that we derive.

It is best if the attendees have some background with calculus-based college
level physics. They should also be familiar with the SciPy Stack, in particular
IPython, SymPy, NumPy, and SciPy. Our goal is that attendees will come away
with the ability to model basic multibody systems, simulate and visualize the
motion, and apply feedback controllers all in a Python framework.

The tutorial materials including an outline can be viewed here:

[https://github.com/pydy/pydy-tutorial-pycon-2014](https://github.com/pydy/pydy-tutorial-pycon-2014)
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason K. Moore</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2745/multibody-dynamics-and-control-with-python-part-2</guid><enclosure url="http://www.youtube.com/watch?v=1-KqRvNX0po" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/1-KqRvNX0po/hqdefault.jpg"></media:thumbnail></item><item><title>Object oriented Programming with NumPy using CPython &amp; PyPy</title><link>http://www.pyvideo.org/video/2733/object-oriented-programming-with-numpy-using-cpyt</link><description>&lt;p&gt;Abstract&lt;/p&gt;
In the paper we compare object-oriented implementations of an advection algorithm written in Python, C++ and modern FORTRAN. The main angles of comparison are code brevity and syntax clarity (and hence maintainability and auditability) as well as performance. A notable performance gain when switching from CPython to PyPy will be exemplified, and the reasons for it will be briefly explained. 
&lt;p&gt;Description&lt;/p&gt;
In the paper we compare object-oriented implementations of an advection algorithm written in Python, C++ and modern FORTRAN. The MPDATA advection algorithm (Multidimensional Positive-Definite Advective Transport Algorithm) used as a core of weather, ocean and climate modelling systems serves as an example.

In the context of scientific programming, employment of object-oriented programming (OOP) techniques may help to improve code readability, and hence its auditability and maintainability. OOP offers, in particular, the possibility to reproduce in the program code the mathematical "blackboard abstractions" used in the literature. We compare how the choice of a particular language influences syntax clarity, code length and the performance: CPU time and memory usage.

The Python implementation of MPDATA is based on NumPy. Its performance is compared with C++/Blitz++ and FORTRAN implementations. A notable performance gain when switching from the standard CPython to PyPy will be exemplified, and the reasons for it will be briefly explained. Discussion of other selected solutions for improving the NumPys relatively poor performance will be also presented.

This talk will describe and extend on the key findings presented in http://arxiv.org/abs/1301.1334.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dorota Jarecka</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2733/object-oriented-programming-with-numpy-using-cpyt</guid><enclosure url="http://www.youtube.com/watch?v=i7rO2qPiesc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/i7rO2qPiesc/hqdefault.jpg"></media:thumbnail></item><item><title>Practical Experience in Teaching Numerical Methods with IPython Notebooks</title><link>http://www.pyvideo.org/video/2726/practical-experience-in-teaching-numerical-method</link><description>&lt;p&gt;Abstract&lt;/p&gt;
New tools like the IPython notebook can enhance both lectures and textbooks, by making class time and individual study more interactive through the inclusion of executable code and animations.  I will demo some materials and activities I've provided for students using the IPython notebook.  I will focus on practical issues I've faced that are particular to this teaching approach.

&lt;p&gt;Description&lt;/p&gt;
Traditional university teaching is based on the use of lectures in class and textbooks out of class.  The medium (lecture or book) discourages the natural curiosity that can lead to deeper understanding by investigating the result of changing a parameter, or looking at the full results of a time-dependent simulation.  

The IPython notebook provides a single medium in which mathematics, explanations, executable code, and animated or interactive visualization can be combined.  Notebooks that combine all of these components can enable new modes of student-led inquiry: the student can experiment with modifications to the code and see the results, all without stepping away from the mathematical explanations themselves.  When notebooks are used by students in the classroom, students can quickly share and discuss results with the instructor or other class members.  The instructor can facilitate deeper learning by posing questions that students may answer through writing appropriate code, during class time.

For the past four years, I have taught a graduate numerical analysis course using SAGE worksheets and IPython notebooks.  I will show examples of the notebooks I've developed and successfully used in this course.  I will describe some practical aspects of my experience, such as:

 - Tradeoffs between using IPython and SAGE
 - Experiences with use of cloud computing platforms
 - Dealing with students' installation issues
 - Quickly getting students up to speed with the Python language and packages
 - Testing and evaluating homework in a math course that is programming-intensive</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David I. Ketcheson</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2726/practical-experience-in-teaching-numerical-method</guid><enclosure url="http://www.youtube.com/watch?v=OaP6LiZuaFM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/OaP6LiZuaFM/hqdefault.jpg"></media:thumbnail></item><item><title>Project based introduction to scientific computing for physics majors</title><link>http://www.pyvideo.org/video/2727/project-based-introduction-to-scientific-computin</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This talk will present an overview of a project-based introductory course in scientific computing using python for physics majors at Cal Poly San Luis Obispo.
&lt;p&gt;Description&lt;/p&gt;
Computational tools and skills are as critical to the training of physics majors as calculus and math, yet they receive much less emphasis in the undergraduate curriculum.  One-off courses that introduce programming and basic numerical problem-solving techniques with commercial software packages for topics that appear in the traditional physics curriculum are insufficient to prepare students for the computing demands of modern technical careers.  Yet tight budgets and rigid degree requirements constrain the ability to expand computational course offerings for physics majors.  

This talk will present an overview of a recently revamped course at Cal Poly San Luis Obispo that uses Python and associated scientific computing libraries to introduce the fundamentals of open-source tools, version control systems, programming, numerical problem solving and algorithmic thinking to undergraduate physics majors.  The spirit of the course is similar to the bootcamps organized by [Software Carpentry](http://software-carpentry.org "Software Carpentry") for researchers in science but is offered as a ten-week for-credit course.  In addition to having a traditional in-class component, students learn the basics of Python by completing tutorials on [Codecademy]( http://www.codecademy.com "Codecademy")'s Python track and practice their algorithmic thinking by tackling [Project Euler]( http://projecteuler.net "Project Euler") problems.  This approach of incorporating online training may provide a different way of thinking about the role of MOOCs in higher education.  The early part of the course focuses on skill-building, while the second half is devoted to application of these skills to an independent research-level computational physics project.    Examples of recent projects and their results will be presented.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jennifer Klay</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2727/project-based-introduction-to-scientific-computin</guid><enclosure url="http://www.youtube.com/watch?v=eJhmMf6bHDU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/eJhmMf6bHDU/hqdefault.jpg"></media:thumbnail></item><item><title>PyMC: Markov Chain Monte Carlo in Python</title><link>http://www.pyvideo.org/video/2734/pymc-markov-chain-monte-carlo-in-python</link><description>&lt;p&gt;Abstract&lt;/p&gt;
PyMC is a Python module that implements Bayesian statistical models and fitting algorithms, including Markov chain Monte Carlo (MCMC). Its flexibility, extensibility, and clean interface make it applicable to a large suite of statistical modeling applications. The upcoming release of PyMC 3 features an expanded set of MCMC samplers, including Hamiltonian Monte Carlo.
&lt;p&gt;Description&lt;/p&gt;
[PyMC](http://pymc-devs.github.io/pymc/) is a Python module that implements Bayesian statistical models and fitting algorithms, including Markov chain Monte Carlo (MCMC). Its flexibility and extensibility make it applicable to a large suite of problems. Along with core sampling functionality, PyMC includes methods for summarizing output, plotting, goodness-of-fit and convergence diagnostics. PyMC seeks to make Bayesian analysis as painless as possible, so that it may be used by a range of data analysts. Its key features include:

* Fits Bayesian statistical models with Markov chain Monte Carlo and other algorithms.
* Includes a large suite of well-documented statistical distributions.
* Uses NumPy for numerics wherever possible.
* Includes a module for modeling Gaussian processes.
* Sampling loops can be paused and tuned manually, or saved and restarted later.
* Creates summaries including tables and plots.
* Traces can be saved to the disk as plain text, Python pickles, SQLite or MySQL database, or hdf5 archives.
* Several convergence diagnostics are available.
* Extensible: easily incorporates custom step methods and unusual probability distributions.
* MCMC loops can be embedded in larger programs, and results can be analyzed with the full power of Python.

The upcoming release of [PyMC 3](https://github.com/pymc-devs/pymc) features an expanded set of MCMC samplers, including Hamiltonian Monte Carlo. For this, we tap into the power of Theano to provide automatic evaluation of mathematical expressions, including gradients used by modern MCMC samplers.

The [source](https://github.com/pymc-devs/pymc) and [documentation](http://pymc-devs.github.io/pymc/) for PyMC can be found on GitHub.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Chris Fonnesbeck</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2734/pymc-markov-chain-monte-carlo-in-python</guid><enclosure url="http://www.youtube.com/watch?v=XbxIo7ScVzc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/XbxIo7ScVzc/hqdefault.jpg"></media:thumbnail></item><item><title>Python Backends for Climate Science Web Apps</title><link>http://www.pyvideo.org/video/2722/python-backends-for-climate-science-web-apps</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We present two web applications:  (PICT: Past Interpretation of Climate Tool), a paleo-climates reconstruction tool and CLIDESC, a climate services layer built on top of the Clide database, a database system used widely in the National Meteorological services across the Pacific. Both these tools have been developed at NIWA in New Zealand. 
&lt;p&gt;Description&lt;/p&gt;
NIWA has developed two tools dedicated respectively to the reconstruction of the climates of the past and to the rapid and flexible development of climate services connected to a widely used meteorological database. 

**PICT (Past Interpretation of Climate Tool** allows the user, given a climate proxy or set of proxies, to reconstruct likely anomalies associated with specific proxy epochs. The tool implements the concept of climate analogs, and reconstruct paleo-climate anomalies in terms of mean atmospheric circulation and sea-surface-temperatures, as well as in terms of the possible changes in the probabilities of synoptic weather regimes (or 'attractors' in the climate system). The whole backend of this application has been exclusively developed using Python with Numpy, scipy, pandas and matplotlib scientific libraries. We present a brief overview of the underlying science before exposing the choices made in designing the python-based compute and data visualisation layer. 

**Clidesc** is an application layer, running in the browser, built on top of **CLIDE**, an open-source database specialized in handling meteorological data in real-time and facilitating its long-term storage. It has been developed using open standards, and facilitate the rapid development of climate services (data analysis and visualisations developed to increase climate intelligence and early warning systems). Clidesc is currently being deployed in several Pacific Islands National Meteorological services. Services can be developed using either R or Python. Development in Python is based on Anaconda and psycopg2, which provides the interface with the postgresql-based Clide database. We present the context and rationale for using open-standards, and give examples of how a user with minimum python knowledge can use templates to rapidly implement a new service tailored to her needs. </description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicolas Fauchereau</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2722/python-backends-for-climate-science-web-apps</guid><enclosure url="http://www.youtube.com/watch?v=yqMqTr3LB7o" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/yqMqTr3LB7o/hqdefault.jpg"></media:thumbnail></item><item><title>PyViennaCL: Very Easy GPGPU Linear Algebra Part 1</title><link>http://www.pyvideo.org/video/2730/pyviennacl-very-easy-gpgpu-linear-algebra-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
PyViennaCL aims to make powerful GPGPU scientific computing really transparently easy, especially for users already using NumPy for representing matrices, by harnessing the ViennaCL linear algebra and numerical computation library for GPGPU and heterogeneous systems. In this talk, I will discuss PyViennaCL's mathematical features, computational architecture, and current developments.
&lt;p&gt;Description&lt;/p&gt;
ViennaCL provides a BLAS-like interface to a set of OpenCL, CUDA and OpenMP compute kernels for linear algebra operations, such as dense and sparse matrix products, direct and iterative solvers, preconditioners. At the C++ API level, ViennaCL uses templates to represent a mathematical expression graph, for which it then generates an appropriate compute kernel.

Interfacing with a C++ templating API from Python, for which users' expressions are expected to be set at compile time, poses a number of problems for the dynamic creation of objects and execution of arbitrary expressions.  For the Python interface, we have a scheduler which takes an expression tree object constructed in Python (using Boost.Python), and then generates and dispatches the relevant kernel, using the relevant data types for the operands. Furthermore, so that users do not regularly incur expensive copying of matrices across slow system buses, PyViennaCL implements various caching mechanisms. Work is currently in progress to support multiple, heterogeneous and distributed platforms, and custom, user-supplied expression nodes, using PyOpenCL and PyCUDA.

To make these features approachable to users familiar with NumPy and SciPy, the PyViennaCL API attempts to be as similar to the NumPy API as possible, providing recognisable classes, methods, and attributes, and transparently converting operand and result types where these things are defined.

This talk will introduce PyViennaCL, covering in more detail the computational architecture described above, as well as these Python API features, and the power of upcoming work to extend the PyViennaCL scheduler and API to custom compute operations, by integrating with PyOpenCL and PyCUDA. In the process, I will provide some comparative benchmark results, to demonstrate the utility of this new work.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Toby St Clere Smithe</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2730/pyviennacl-very-easy-gpgpu-linear-algebra-part-1</guid><enclosure url="http://www.youtube.com/watch?v=Qym3Onh-Z20" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Qym3Onh-Z20/hqdefault.jpg"></media:thumbnail></item><item><title>Reproducible, Relocatable, Customizable Builds and Packaging with HashDist Part1</title><link>http://www.pyvideo.org/video/2724/reproducible-relocatable-customizable-builds-an</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This talk introduces HashDist, a critical component of the scientific software development workflow.  HashDist enables highly customizable, source-driven, and reproducible builds for scientific software stacks.  HashDist builds can be made relocatable, allowing the easy redistribution of binaries on all three major operating systems as well as cloud and supercomputing platforms. 
&lt;p&gt;Description&lt;/p&gt;
Developing scientific software is a continuous balance between not reinventing the wheel and getting fragile codes to interoperate with one another.  Binary software distributions such as Anaconda provide a robust starting point for many scientific software packages, but this solution alone is insufficient for many scientific software developers.  HashDist provides a critical component of the development workflow, enabling highly customizable, source-driven, and reproducible builds for scientific software stacks, available from both the IPython Notebook and the command line.

To address these issues, the Coastal and Hydraulics Laboratory at the US Army Engineer Research and Development Center has funded the development of HashDist in collaboration with Simula Research Laboratories and the University of Texas at Austin.  HashDist is motivated by a functional approach to package build management, and features intelligent caching of sources and builds, parametrized build specifications, and the ability to interoperate with system compilers and packages. HashDist enables the easy specification of "software stacks", which allow both the novice user to install a default environment and the advanced user to configure every aspect of their build in a modular fashion.  As an advanced feature, HashDist builds can be made relocatable, allowing the easy redistribution of binaries on all three major operating systems as well as cloud, and supercomputing platforms.  As a final benefit, all HashDist builds are reproducible, with a build hash specifying exactly how each component of the software stack was installed.

This talk will feature an introduction to the problem of packaging Python-based scientific software, a discussion of the basic tools available to scientific Python developers, and a detailed discussion and demonstration of the HashDist package build manager.

The HashDist documentation is available from: http://hashdist.readthedocs.org/en/latest/
HashDist is currently hosted at: https://github.com/hashdist/hashdist</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel,Aron Ahmadia,Chris Kees,Dag Sverre Seljebotn,Ondrej Certik</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2724/reproducible-relocatable-customizable-builds-an</guid><enclosure url="http://www.youtube.com/watch?v=wviHkzk0AkY" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/wviHkzk0AkY/hqdefault.jpg"></media:thumbnail></item><item><title>Reproducible Science: Walking the Walk Part 1</title><link>http://www.pyvideo.org/video/2742/reproducible-science-walking-the-walk-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will train reproducible research warriors on the practices and tools that make experimental verification possible with an end-to-end data analysis workflow. The tutorial will expose attendees to open science methods during data gathering, storage, analysis up to publication into a reproducible article. Attendees are expected to have basic familiarity with scientific Python and Git.
&lt;p&gt;Description&lt;/p&gt;
The tutorial will cover four hours with the following topics


* Introduction (10min)
   * History of scientific societies and publications
      * Leeuwenhoek was the Man !
      * The Invisible College 
      * Nullius in Verba
* Replication of the early microscope experiments by Leeuwenhoek[a][b]
   * Image Acquisition (15 min)
      * Hands on: Cell camera phone microscope 
         * With drop of water
      * Hands on: Each pair acquires images
   * Data Sharing (45min)
      * Image gathering, storage, and sharing (15min)
         * GitHub (www.github.com)
         * Figshare (www.figshare.com)
         * Midas (www.midasplatform.com) 
         * Hands on: Upload the images
      * Metadata Identifiers (15 min)
         * Citable
         * Machine Readable
         * Hands on: Create data citation and machine readable metadata
      * Hands on: Download data via RESTful API (15min)
         * Provenance and 
         * Python scripts
         * Hands on: Download the data via HTTP
   * Break (10min)
   * Local processing (60min)
      * Replication Enablement (20min)
         * Package versioning
         * Virtual Machines
         * Docker
         * Cloud services
         * Hands on: 
            * Create a virtualenv
            * Run our tutorial package verification script
      * Revision Control with Git (20min)
         * Keeping track of changes
         * Unique hashes
         * Hands on:
            * Forking a repository in GitHub
            * Cloning a repository
            * Creating a branch
            * Making a commit
            * Pushing a branch
            * Diffing
            * Merging 
            * Pushing again        
            * Create pull request
      * Python scripts (20min)
         * Data analysis, particle counting.
         * Hands on:
            * Run scripts on new data
            * Generate histogram for the data
   * Testing (30min)
      * Unit testing with known data
      * Regression testing with known data
      * Hands on: 
         * Run tests
         * Add coverage for another method to the unit tests
   * Break (10min)
   * Publication Tools (30min)
      * Article generation 
      * RST to HTML
      * GitHub replication and sharing
      * Hands on: 
         * Run dexy to generate the document
   * Reproducibility Verification (30min)
      * Reproducing Works
      * Publication of Positive and Negative results
      * Hands on:
         * Create Open Science Framework (OSF) project
         * Connect Figshare and Github to OSF project
         * Fork or link another groups project in the OSF to run dexy on their work


Infrastructure:

Attendees will use software installed in their laptops to gather and  process data, then publish and share a reproducible report.

They will access repositories in GitHub, upload data to a repository and publish materials necessary to replicate their data analysis.

We expect that wireless network will be have moderate bandwidth to allow all attendees to move data, source code and publications between their laptops and hosting servers.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aashish Chaudhary,Ana Nelson,Arfon Smith,Jean-Christophe Fillion-Robin,Luis Ibanez,Matthew McCormick</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2742/reproducible-science-walking-the-walk-part-1</guid><enclosure url="http://www.youtube.com/watch?v=EzX7MN_bzqg" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/EzX7MN_bzqg/hqdefault.jpg"></media:thumbnail></item><item><title>Reproducible Science: Walking the Walk Part 2</title><link>http://www.pyvideo.org/video/2737/reproducible-science-walking-the-walk-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will train reproducible research warriors on the practices and tools that make experimental verification possible with an end-to-end data analysis workflow. The tutorial will expose attendees to open science methods during data gathering, storage, analysis up to publication into a reproducible article. Attendees are expected to have basic familiarity with scientific Python and Git.
&lt;p&gt;Description&lt;/p&gt;
The tutorial will cover four hours with the following topics


* Introduction (10min)
   * History of scientific societies and publications
      * Leeuwenhoek was the Man !
      * The Invisible College 
      * Nullius in Verba
* Replication of the early microscope experiments by Leeuwenhoek[a][b]
   * Image Acquisition (15 min)
      * Hands on: Cell camera phone microscope 
         * With drop of water
      * Hands on: Each pair acquires images
   * Data Sharing (45min)
      * Image gathering, storage, and sharing (15min)
         * GitHub (www.github.com)
         * Figshare (www.figshare.com)
         * Midas (www.midasplatform.com) 
         * Hands on: Upload the images
      * Metadata Identifiers (15 min)
         * Citable
         * Machine Readable
         * Hands on: Create data citation and machine readable metadata
      * Hands on: Download data via RESTful API (15min)
         * Provenance and 
         * Python scripts
         * Hands on: Download the data via HTTP
   * Break (10min)
   * Local processing (60min)
      * Replication Enablement (20min)
         * Package versioning
         * Virtual Machines
         * Docker
         * Cloud services
         * Hands on: 
            * Create a virtualenv
            * Run our tutorial package verification script
      * Revision Control with Git (20min)
         * Keeping track of changes
         * Unique hashes
         * Hands on:
            * Forking a repository in GitHub
            * Cloning a repository
            * Creating a branch
            * Making a commit
            * Pushing a branch
            * Diffing
            * Merging 
            * Pushing again        
            * Create pull request
      * Python scripts (20min)
         * Data analysis, particle counting.
         * Hands on:
            * Run scripts on new data
            * Generate histogram for the data
   * Testing (30min)
      * Unit testing with known data
      * Regression testing with known data
      * Hands on: 
         * Run tests
         * Add coverage for another method to the unit tests
   * Break (10min)
   * Publication Tools (30min)
      * Article generation 
      * RST to HTML
      * GitHub replication and sharing
      * Hands on: 
         * Run dexy to generate the document
   * Reproducibility Verification (30min)
      * Reproducing Works
      * Publication of Positive and Negative results
      * Hands on:
         * Create Open Science Framework (OSF) project
         * Connect Figshare and Github to OSF project
         * Fork or link another groups project in the OSF to run dexy on their work


Infrastructure:

Attendees will use software installed in their laptops to gather and  process data, then publish and share a reproducible report.

They will access repositories in GitHub, upload data to a repository and publish materials necessary to replicate their data analysis.

We expect that wireless network will be have moderate bandwidth to allow all attendees to move data, source code and publications between their laptops and hosting servers.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aashish Chaudhary,Ana Nelson,Arfon Smith,Jean-Christophe Fillion-Robin,Luis Ibanez,Matthew McCormick</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2737/reproducible-science-walking-the-walk-part-2</guid><enclosure url="http://www.youtube.com/watch?v=HCyHn_by3N0" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/HCyHn_by3N0/hqdefault.jpg"></media:thumbnail></item><item><title>RM204 BOF MatPlotLib Discussion</title><link>http://www.pyvideo.org/video/2717/rm204-bof-matplotlib-discussion</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We will have an open discussion about current matplotlib enhancement proposals and take calls for new ones.  Anyone interested in matplotlib's future development efforts is more than welcome to attend.  There will be no presentation.  Current MEPs exist here: https://github.com/matplotlib/matplotlib/wiki#matplotlib-enhancement-proposals-meps</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Damon McDougall</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2717/rm204-bof-matplotlib-discussion</guid><enclosure url="http://www.youtube.com/watch?v=adPZFH7nVLE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/adPZFH7nVLE/hqdefault.jpg"></media:thumbnail></item><item><title>Scientific Computing in the Undergraduate Meteorology Curriculum at Millersville Univ</title><link>http://www.pyvideo.org/video/2725/scientific-computing-in-the-undergraduate-meteoro</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Scientific computing is an important aspect of the undergraduate meteorology curriculum at Millersville University.  All students take a course in Fortran, and many take additional courses in Python and atmospheric numerical modeling.  This presentation discusses how scientific computing is incorporated into the curriculum, and why Fortran and Python were chosen as the languages to be taught.
&lt;p&gt;Description&lt;/p&gt;
Scientific computing is integrated into the undergraduate meteorology curriculum at Millersville University.  The curriculum guidelines published by the American Meteorological Society specifically address scientific computing in undergraduate atmospheric sciences curricula, stating that students should gain experience using a high-level structured programming language (e.g., C, C++, Python, MATLAB,  IDL, or Fortran).   This is addressed at Millersville via the programming-specific courses are ESCI 282  _Fortran Programming for the Earth Sciences_, and ESCI 386  _Scientific Programming, Analysis and Visualization with Python_.  There are also additional courses in which the students are required to use scientific computing as part of their assignments.  Examples of these courses are ESCI 445  _Numerical Modeling of the Atmosphere and Oceans_, and ESCI 390  _Remote Sensing_.

Although the universitys computer science department teaches programming courses in Java, this is not very applicable to our students needs for scientific programming.  We therefore teach our own programming courses in-house.  The required programming course taken by all students is the Fortran course, which teaches them the elements of programming.  This is many students first exposure to programming.  Most then follow-on by taking the Python course.  In the Python course, scientific data analysis and visualization are stressed, using the Scientific Python, Numerical Python, and Matplotlib libraries.

In the elective numerical modeling course, students are required to write programs for finite-difference solutions to various 1-D and 2-D partial differential equations relevant to modeling the fluid dynamics of the atmosphere.  They may program in any language of their choosing, but the majority of students choose Python, even if they have no prior experience with it.  This is because of Pythons  intuitive syntax and ease of use.  In the elective remote sensing course students are introduced to and use IDL/ENVI for display and analysis of remote sensing imagery.

Prior to 2012 the current Python course was instead taught as a course in IDL.  The transition was made for several reasons.  The primary reason was the limited market and usage of IDL compared to more pervasive languages such as MATLAB and Python.  Many students would not have access to IDL once they graduate.  Also, Python is gaining traction in usage in the atmospheric and oceanic sciences, and is not proprietary like IDL and MATLAB, so students will have access to it no matter where they find employment or graduate school opportunities.  The high cost of maintaining an institutional IDL license is also an issue that the university must address annually, and it times of lean budgets it becomes an attractive target for elimination.  The IDL course is still on the books, but there are no immediate plans for upcoming offerings.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex DeCaria</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2725/scientific-computing-in-the-undergraduate-meteoro</guid><enclosure url="http://www.youtube.com/watch?v=igBEH7H8Ahk" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/igBEH7H8Ahk/hqdefault.jpg"></media:thumbnail></item><item><title>SociaLite: Python intergrated query Language for Data Analysis</title><link>http://www.pyvideo.org/video/2729/socialite-python-intergrated-query-language-for</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SociaLite is a Python-integrated query language for data analysis. It makes scientific data analysis simple, yet achieves fast performance with its compiler optimizations. We support relational tables and operations in SociaLite as well as Python integration, which makes it easy to implement various analysis algorithms, including Blast algorithm and genome assembly algorithm in bioinformatics.
&lt;p&gt;Description&lt;/p&gt;
SociaLite is a Python-integrated query language for distributed data analysis.  
It makes scientific data analysis simple, yet achieves fast performance with its compiler optimizations. The performance of SociaLite is often more than three orders of magnitude faster than Hadoop programs, and close to optimized C programs.  For example, PageRank algorithm can be implemented in just 2 lines of SociaLite query, which runs nearly as fast as an optimal parallelized C code. 

SociaLite supports well-known high-level concepts to make data analysis easy for non-expert programmers.
We support relational tables for storing data, and relational operations, such as join, selection, and projection, for processing the data. Moreover, SociaLite queries are fully integrated with Python, so both SociaLite and Python code can be used to implement data analysis logic.  For the integration with Python, we support embedding and extending SociaLite, where embedding supports using SociaLite queries directly in Python code, and extending supports using Python functions in SociaLite queries. 

The Python integration makes it easy to implement various analysis algorithms in SociaLite and Python. For example, the BLAST algorithm in bioinformatics can be implemented in just a few lines of SociaLite queries and Python code. Also genome assembly algorithm -- generating a De Bruijn graph and applying Eulerian cycle algorithm -- can be simply implemented. In the talk, I will demonstrate these algorithms in SociaLite as well as more general algorithms such as K-means clustering and logistic regression.

The SociaLite queries are compiled to highly optimized parallel/distributed code; we apply optimizations such as pipelined evaluation and prioritization. The runtime system also speeds up the performance; for example, the customized memory allocator reduces memory allocation time and footprint.  In short, SociaLite makes high-performance data analysis easy with its high-level abstractions and compiler/runtime optimizations.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jiwon Seo</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2729/socialite-python-intergrated-query-language-for</guid><enclosure url="http://www.youtube.com/watch?v=IjrK4cESxJI" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/IjrK4cESxJI/hqdefault.jpg"></media:thumbnail></item><item><title>SymPy Tutorial Part 1</title><link>http://www.pyvideo.org/video/2706/sympy-tutorial-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SymPy is a pure Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python and does not require any external libraries.

&lt;p&gt;Description&lt;/p&gt;
In this tutorial we will introduce attendees to SymPy. We will show basics of constructing and manipulating mathematical expressions in SymPy, the most common issues and differences from other computer algebra systems, and how to deal with them. In the last part of this tutorial we will show how to solve some practical problems with SymPy. This will include showing how to interface SymPy with popular numeric libraries like NumPy.

This knowledge should be enough for attendees to start using SymPy for solving mathematical problems and hacking SymPy's internals (though hacking core modules may require additional expertise).</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer,Jason K. Moore,Matthew Rocklin</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2706/sympy-tutorial-part-1</guid><enclosure url="http://www.youtube.com/watch?v=Lgp442bibDM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/Lgp442bibDM/hqdefault.jpg"></media:thumbnail></item><item><title>SymPy Tutorial Part 2</title><link>http://www.pyvideo.org/video/2713/sympy-tutorial-part-2</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SymPy is a pure Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python and does not require any external libraries.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer,Jason K. Moore,Matthew Rocklin</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2713/sympy-tutorial-part-2</guid><enclosure url="http://www.youtube.com/watch?v=_PTe10whFKo" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/_PTe10whFKo/hqdefault.jpg"></media:thumbnail></item><item><title>SymPy Tutorial Part 3</title><link>http://www.pyvideo.org/video/2707/sympy-tutorial-part-3</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SymPy is a pure Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python and does not require any external libraries.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer,Jason K. Moore,Matthew Rocklin</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2707/sympy-tutorial-part-3</guid><enclosure url="http://www.youtube.com/watch?v=qleGSnrnxgc" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/qleGSnrnxgc/hqdefault.jpg"></media:thumbnail></item><item><title>SymPy Tutorial Part 4</title><link>http://www.pyvideo.org/video/2711/sympy-tutorial-part-4</link><description>&lt;p&gt;Abstract&lt;/p&gt;
SymPy is a pure Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python and does not require any external libraries.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aaron Meurer,Jason K. Moore,Matthew Rocklin</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2711/sympy-tutorial-part-4</guid><enclosure url="http://www.youtube.com/watch?v=04AGOl1P2U4" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/04AGOl1P2U4/hqdefault.jpg"></media:thumbnail></item><item><title>Taking Control: Enabling Mathematicians and Scientists</title><link>http://www.pyvideo.org/video/2712/taking-control-enabling-mathematicians-and-scien</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Good solutions to hard problems require both domain and algorithmic expertise.
Domain experts know what to do and computer scientists know how to do it
well. This talk discusses challenges and experiences trying to reconcile these
two groups, particularly within SymPy. It proposes concrete approaches
including multiple dispatch, pattern matching, and programmatic strategies.

&lt;p&gt;Description&lt;/p&gt;
Good solutions to hard problems require both domain and algorithmic expertise.
Domain experts know *what* to do and computer scientists know *how* to do it
well.  Coordination between the algorithmic and domain programmer is 
challenging to do well and difficult to scale.  It is also arguably one of the
most relevant blocks to scientific progress today.

This talk draws from experience supporting mathematical programmers in the
SymPy project.  SymPy is a computer algebra system, a complex problem that
requires the graph manipulation algorithms of a modern compiler alongside the           mathematics of several PhD theses.  SymPy draws from a broad developer base
with experienced and novice developers alike and so struggles to maintain a
cohesive organized codebase.

We approach this development problem by separating software engineering
into a collection of small functions, written by domain experts, alongside an
abstract control system, written by algorithmic programmers.  We facilitate
this division with techniques taken from other languages and compiler 
technologies.  Notably we motivate the use of a few general purpose libraries
for multiple dispatch, pattern matching, and programmatic control.
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Matthew Rocklin</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2712/taking-control-enabling-mathematicians-and-scien</guid><enclosure url="http://www.youtube.com/watch?v=QldxygVVj-s" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/QldxygVVj-s/hqdefault.jpg"></media:thumbnail></item><item><title>Teaching Numerical Methods with IPython Notebooks 2</title><link>http://www.pyvideo.org/video/2746/teaching-numerical-methods-with-ipython-notebooks</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will give participants an introduction to the use of IPython notebooks in teaching numerical methods or scientific computing, at the level of an undergraduate or graduate university course.  Prior familiarity with notebooks is not necessary.  Participants will create an interactive notebook that explains and helps students to implement and explore a numerical algorithm.
&lt;p&gt;Description&lt;/p&gt;
This tutorial is targeted to those who are or soon will be teaching numerical methods or scientific computing and are interested in using Python as the programming language for their course.  The tutorial will be useful both to academics teaching university courses and those in industry who run training sessions.  No prior knowledge of the IPython notebook is necessary, but participants should have some familiarity with Python, Numpy, and Matplotlib.

IPython notebooks are an excellent medium for teaching nuemrical methods since they can include both mathematical explanations and executable code in a single document.  The tutorial will begin with an introduction to the IPython notebook, emphasizing how to overcome aspects that can be confusing to students.  Next we will go over available free resources for 

- ensuring that students have a suitable computing environment, using either a cloud platform or a packaged distribution
- distributing and collecting notebooks
- converting notebooks to other formats that may be useful in a course

We will also review a number of excellent existing resources containing IPython notebooks for numerical methods courses.  Using these notebooks as examples, we will discuss how to design effective notebooks for teaching, including

- typesetting mathematical equations and expressions using LaTeX
- Formatting, referencing, and layout using Markdown
- inserting complete or partial code snippets
- embedding figures and other media
- embedding interactive widgets

We will briefly discuss different approaches to using IPython notebooks in a course, including their use as the basis for

- homework assignments
- short activities during a class session
- longer laboratory sessions

Finally, participants will be asked to develop, individually or in small groups, a notebook of their own that could be used as an assignment, classroom exercise, or lecture.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia,David I. Ketcheson</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2746/teaching-numerical-methods-with-ipython-notebooks</guid><enclosure url="http://www.youtube.com/watch?v=cm0oVqbxssU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/cm0oVqbxssU/hqdefault.jpg"></media:thumbnail></item><item><title>Teaching Numerical Methods with IPython Notebooks, Part 1</title><link>http://www.pyvideo.org/video/2860/teaching-numerical-methods-part-1</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will give participants an introduction to the use of IPython notebooks in teaching numerical methods or scientific computing, at the level of an undergraduate or graduate university course.  Prior familiarity with notebooks is not necessary.  Participants will create an interactive notebook that explains and helps students to implement and explore a numerical algorithm.
&lt;p&gt;Description&lt;/p&gt;
This tutorial is targeted to those who are or soon will be teaching numerical methods or scientific computing and are interested in using Python as the programming language for their course.  The tutorial will be useful both to academics teaching university courses and those in industry who run training sessions.  No prior knowledge of the IPython notebook is necessary, but participants should have some familiarity with Python, Numpy, and Matplotlib.

IPython notebooks are an excellent medium for teaching nuemrical methods since they can include both mathematical explanations and executable code in a single document.  The tutorial will begin with an introduction to the IPython notebook, emphasizing how to overcome aspects that can be confusing to students.  Next we will go over available free resources for 

- ensuring that students have a suitable computing environment, using either a cloud platform or a packaged distribution
- distributing and collecting notebooks
- converting notebooks to other formats that may be useful in a course

We will also review a number of excellent existing resources containing IPython notebooks for numerical methods courses.  Using these notebooks as examples, we will discuss how to design effective notebooks for teaching, including

- typesetting mathematical equations and expressions using LaTeX
- Formatting, referencing, and layout using Markdown
- inserting complete or partial code snippets
- embedding figures and other media
- embedding interactive widgets

We will briefly discuss different approaches to using IPython notebooks in a course, including their use as the basis for

- homework assignments
- short activities during a class session
- longer laboratory sessions

Finally, participants will be asked to develop, individually or in small groups, a notebook of their own that could be used as an assignment, classroom exercise, or lecture.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia,David I. Ketcheson</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2860/teaching-numerical-methods-part-1</guid><enclosure url="http://www.youtube.com/watch?v=L-caFdJMR9E" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/L-caFdJMR9E/hqdefault.jpg"></media:thumbnail></item><item><title>Teaching Numerical Methods with IPython Notebooks, Part 3</title><link>http://www.pyvideo.org/video/2859/teaching-numerical-methods-part-3</link><description>&lt;p&gt;Abstract&lt;/p&gt;
This tutorial will give participants an introduction to the use of IPython notebooks in teaching numerical methods or scientific computing, at the level of an undergraduate or graduate university course.  Prior familiarity with notebooks is not necessary.  Participants will create an interactive notebook that explains and helps students to implement and explore a numerical algorithm.
&lt;p&gt;Description&lt;/p&gt;
This tutorial is targeted to those who are or soon will be teaching numerical methods or scientific computing and are interested in using Python as the programming language for their course.  The tutorial will be useful both to academics teaching university courses and those in industry who run training sessions.  No prior knowledge of the IPython notebook is necessary, but participants should have some familiarity with Python, Numpy, and Matplotlib.

IPython notebooks are an excellent medium for teaching nuemrical methods since they can include both mathematical explanations and executable code in a single document.  The tutorial will begin with an introduction to the IPython notebook, emphasizing how to overcome aspects that can be confusing to students.  Next we will go over available free resources for 

- ensuring that students have a suitable computing environment, using either a cloud platform or a packaged distribution
- distributing and collecting notebooks
- converting notebooks to other formats that may be useful in a course

We will also review a number of excellent existing resources containing IPython notebooks for numerical methods courses.  Using these notebooks as examples, we will discuss how to design effective notebooks for teaching, including

- typesetting mathematical equations and expressions using LaTeX
- Formatting, referencing, and layout using Markdown
- inserting complete or partial code snippets
- embedding figures and other media
- embedding interactive widgets

We will briefly discuss different approaches to using IPython notebooks in a course, including their use as the basis for

- homework assignments
- short activities during a class session
- longer laboratory sessions

Finally, participants will be asked to develop, individually or in small groups, a notebook of their own that could be used as an assignment, classroom exercise, or lecture.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia,David I. Ketcheson</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2859/teaching-numerical-methods-part-3</guid><enclosure url="http://www.youtube.com/watch?v=rpGghGDEkZE" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/rpGghGDEkZE/hqdefault.jpg"></media:thumbnail></item><item><title>The Failure of Python Object Serializations: Why HPC in Python is Broken and How to Fix it</title><link>http://www.pyvideo.org/video/2720/the-failure-of-python-object-serializations-why</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Parallel and asynchronous computing in python is crippled by pickle's poor object serialization. Dill, a more robust serialization package, strives to serialize all of python.  Dill has been used to enable state persistence and recovery, global caching, and the coordination of distributed parallel calculations across a network of the world's largest computers.
&lt;p&gt;Description&lt;/p&gt;
Parallel and asynchronous computing in python is crippled by pickle's poor object serialization. However, a more robust serialization package would drastically improve the situation. To leverage the cores found in modern processors we need to communicate functions between different processes -- and that means callables must be serialized without pickle barfing.  Similarly, parallel and distributed computing with MPI, GPUs, sockets, and across other process boundaries all need serialized functions (or other callables).  So why is pickling in python so broken?  Python's ability to leverage these awesome communication technologies is limited by python's own inability to be a fully serializable language.  In actuality, serialization in python is quite limited, and for really no good reason.

Many raise security concerns for full object serialization, however it can be argued that it is not pickle's responsibility to do proper authentication. In fact, one could apply rather insecure serialization of all objects the objects were all sent across RSA-encrypted ssh-tunnels, for example. 

Dill is a serialization package that strives to serialize all of python.  We have forked python's multiprocessing to use dill. Dill can also be leveraged by mpi4py, ipython, and other parallel or distributed python packages. Dill serves as the backbone for a distributed parallel computing framework that is being used to design the next generation of large-scale heterogeneous computing platforms, and has been leveraged in large-scale calculations of risk and uncertainty.  Dill has been used to enable state persistence and recovery, global caching, and the coordination of distributed parallel calculations across a network of the world's largest computers.


[http://pythonhosted.org/dill](http://pythonhosted.org/dill "")

[https://github.com/uqfoundation](https://github.com/uqfoundation "")

[http://matthewrocklin.com/blog/work/2013/12/05/Parallelism-and-Serialization/](http://matthewrocklin.com/blog/work/2013/12/05/Parallelism-and-Serialization/ "")

[http://stackoverflow.com/questions/19984152/what-can-multiprocessing-and-dill-do-together?rq=1](http://stackoverflow.com/questions/19984152/what-can-multiprocessing-and-dill-do-together?rq=1 "")

[https://groups.google.com/forum/#!topic/mpi4py/1fd4FwdgpWY](https://groups.google.com/forum/#!topic/mpi4py/1fd4FwdgpWY "")

[http://nbviewer.ipython.org/gist/anonymous/5241793](http://nbviewer.ipython.org/gist/anonymous/5241793 "")</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael McKerns</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2720/the-failure-of-python-object-serializations-why</guid><enclosure url="http://www.youtube.com/watch?v=oSoT0m8SgrU" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/oSoT0m8SgrU/hqdefault.jpg"></media:thumbnail></item><item><title>The Wonderful World of Scientific Computing with Python</title><link>http://www.pyvideo.org/video/2744/the-wonderful-world-of-scientific-computing-with</link><description>&lt;p&gt;Abstract&lt;/p&gt;
We will give an overview of the basics of the scientific computing ecosystem with Python: what does each of the fundamental packages (numpy, matplotlib, scipy, sympy and pandas) 
do, and how does it work? We will use the IPython Notebook in our quest to enter this wonderful world.
&lt;p&gt;Description&lt;/p&gt;
Starting out with scientific computing in Python can be daunting: Where do I start? What are the basic packages, and what is the use case for each of them? What are the fundamental ideas I need to understand each package and how it works?

In this tutorial, we will use examples of scientific questions and calculations which lead directly to the need for certain computational tools as a gateway to understand the basic structure of the scientific computing ecosystem. The specific packages we will touch on are `numpy`, `matplotlib`, `scipy`, `sympy` and `pandas`, all viewed through the wonderful lens of the IPython Notebook.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">David P. Sanders</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2744/the-wonderful-world-of-scientific-computing-with</guid><enclosure url="http://www.youtube.com/watch?v=A9tv7WBIwyM" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/A9tv7WBIwyM/hqdefault.jpg"></media:thumbnail></item><item><title>WCSAxes: A Framework for Plotting Astronomical and Geospatial Data</title><link>http://www.pyvideo.org/video/2723/wcsaxes-a-framework-for-plotting-astronomical-an</link><description>&lt;p&gt;Abstract&lt;/p&gt;
I will present WCSAxes, a new framework for plotting astronomical data that seamlessly handles the plotting of ticks, tick labels, and grid lines for arbitrary coordinate systems and projections. While originally written for with astronomical data, it can be used for any kind of map provided that the projection and coordinate system can be represented by a pixel-to-world transformation.
&lt;p&gt;Description&lt;/p&gt;
Astronomical data (whether images on the sky, or other data) are typically stored with information about their corresponding projection (Gnomonic, Mercator, Conical, Aitoff, and *many* more) and coordinate system (Equatorial, Galactic, Ecliptic, and so on).

I will present [WCSAxes](https://github.com/astrofrog/wcsaxes "WCSAxes"), a new framework for plotting such astronomical data, developed as part of the [Astropy](http://www.astropy.org) project. WCSAxes consists primarily of a [Matplotlib](http://www.matplotlib.org "Matplotlib") Axes sub-class that seamlessly handles the plotting of ticks, tick labels, and grid lines for arbitrary coordinate systems and projections.

As an example, the following plot was produced with WCSAxes:

![The Galactic Center as seen by Chandra](http://www.mpia.de/~robitaille/chandra_avm_small.png "The Galactic center as seen by the Chandra X-ray observatory")

(Image Credit: NASA/CXC/UMass/D. Wang et al. - [http://chandra.harvard.edu/photo/2009/gcenter/](http://chandra.harvard.edu/photo/2009/gcenter/))

Since it is a sub-class of the Matplotlib Axes class, all the default Matplotlib methods such as plot, scatter, imshow, contour, as well as patches, lines, collections, and so on are supported, and WCSAxes - in combination with Matplotlib's ability to accept arbitrary transformations - makes it very easy to define whether the plotting should apply to pixel coordinates, or a world coordinate system related to the data.

WCSAxes has been designed as a framework that can be easily used in other Python tools, and it is planned for inclusion in [Glue](http://www.glueviz.org), [APLpy](http://aplpy.github.io), and other astronomical tools. While originally written for Astronomical images, it should be easily extendable to any kind of map (such as Earth-based geospatial data) provided that the projection and coordinate system can be represented by a pixel-to-world transformation.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Robitaille</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2723/wcsaxes-a-framework-for-plotting-astronomical-an</guid><enclosure url="http://www.youtube.com/watch?v=ukR5a2TJJt4" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/ukR5a2TJJt4/hqdefault.jpg"></media:thumbnail></item><item><title>You Win or You SciPy</title><link>http://www.pyvideo.org/video/2715/you-win-or-you-scipy</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Reflections on the State of Scientific Python 
&lt;p&gt;Description&lt;/p&gt;
Python is currently one of the most popular programming languages and it seems that that Scientific Python has truly hit its stride in recent years. With fame comes a deluge of users, but not necessarily any more developers. Scientific Python is often held up as one of the core strengths of the Python language. Why is this so? And how much does it actually help us? This talk intends to be a frank discussion on the great parts of the SciPy community and the parts that need work.

As a confederation of packages and projects, there are several issues that affect everyone. Sometimes these issues fall through the cracks and other times they are vigorously tackled head on. In either case, I posit that greater communication about these global topics is necessary to support and scale to the next wave of SciPy users and developers.

Points of discussion in this talk may include:

* Packaging,
* Education,
* Matplotlib - aged or awesome,
* Competition from other languages,
* Diversity,
* Employing our own,
* Interfacing with the broader Python community,
* The legal status of projects, and
* Maintaining critical packages in the ecosystem (when devs have moved on).

Historically, the SciPy conference has not had many overview talks, talks about the community itself, what we are doing right, and what we are doing wrong.

They were often relegated to keynotes if they were present at all. This talk is a boots-on-the-ground attempt to rectify that.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel,Anthony Scopatz,Katy Huff</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2715/you-win-or-you-scipy</guid><enclosure url="http://www.youtube.com/watch?v=e0Z9-EuZJac" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/e0Z9-EuZJac/hqdefault.jpg"></media:thumbnail></item><item><title>Zeke: A Python Platform for Teaching Mathematical Modeling of Infectious Diseases</title><link>http://www.pyvideo.org/video/2728/zeke-a-python-platform-for-teaching-mathematical</link><description>&lt;p&gt;Abstract&lt;/p&gt;
Zeke is an educational platform implemented using Python, SciPy and Django which allows students and researchers interested in the modeling of infectious diseases to learn both modeling and scientific computing skills in an interactive environment, without forcing theoretical and computational skills to be developed simultaneously.
&lt;p&gt;Description&lt;/p&gt;
Computational and mathematical models can yield profound insights in the study of the spread of infectious diseases, illustrating difficult concepts such as herd immunity, suggesting new avenues for empirical research, and obtaining repeatable, quantifiable evidence in situations where other study designs are difficult if not impossible.

Teaching infectious disease modeling presents a challenge however, as it requires the development of three unrelated skill sets: the theory of infectious disease models, subject-matter expertise about the diseases themselves, and the programming skills needed to implement all but the simplest models.

Rather than forcing these skill sets to be developed in parallel, *Zeke* is an educational platform meant to allow students to develop these skills in sequence. It uses a zombie epidemic to remove the need for specific expertise regarding disease systems, relying instead on a familiar cultural reference. Students are allowed to first explore the theory of modeling, a Django front-end enabling interaction with models without the need for scientific computing skills. All the models are however implemented to be stand-alone models that can be run using SciPy or other packages, and as students grow in sophistication the open-source nature of *Zeke* allows them to develop both computational and subject-specific expertise in a stepwise fashion.</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eric Lofgren</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 -0500</pubDate><guid>http://www.pyvideo.org/video/2728/zeke-a-python-platform-for-teaching-mathematical</guid><enclosure url="http://www.youtube.com/watch?v=bZv9Rw6Nh8M" length="None" type="video/flv"></enclosure><media:thumbnail url="http://i1.ytimg.com/vi/bZv9Rw6Nh8M/hqdefault.jpg"></media:thumbnail></item></channel></rss>